{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ideas and useful sources\n",
    "## 1. Similar competitions\n",
    "### - Smoker status:\n",
    "### - https://www.kaggle.com/code/cv13j0/efficient-prediction-of-smoker-status\n",
    "### - https://www.kaggle.com/code/arunklenin/ps3e24-eda-feature-engineering-ensemble\n",
    "### -  https://www.kaggle.com/code/ravi20076/playgrounds3e24-eda-baseline\n",
    "### - https://www.kaggle.com/code/oscarm524/ps-s3-ep24-eda-modeling-submission\n",
    "### -  https://www.kaggle.com/code/ashishkumarak/binary-classification-smoker-or-not-eda-xgboost\n",
    "\n",
    "## 2. Ideas and Tasks\n",
    "### - Logging\n",
    "### - Turning into classes and functions (config classes)\n",
    "### - Optuna + model ensemble (use this notebook: https://www.kaggle.com/code/rzatemizel/ensemble-pipeline)\n",
    "### - SHAP\n",
    "### - Do the rest of feature engineering -> clustering, pca, count, aggregation etc.\n",
    "### - Calculate baseline catboost with all provided columns, with selected from all provided, with all feature engineered, with selected feature engineering then finetune \n",
    "### -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import f1_score, roc_auc_score, make_scorer\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.ensemble import VotingClassifier, HistGradientBoostingClassifier\n",
    "\n",
    "import optuna\n",
    "from optuna import Trial\n",
    "from optuna import create_study\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "from utils.eda import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.options.display.float_format = '{:,.2f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('test.csv')\n",
    "train_df = pd.read_csv('train.csv')\n",
    "submission_df = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape: (110023, 13)\n",
      "Number of duplicates found and removed: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data type</th>\n",
       "      <th>#missing</th>\n",
       "      <th>%missing</th>\n",
       "      <th>#unique</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>average</th>\n",
       "      <th>standard_deviation</th>\n",
       "      <th>first value</th>\n",
       "      <th>second value</th>\n",
       "      <th>third value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>110023</td>\n",
       "      <td>165,034.00</td>\n",
       "      <td>275,056.00</td>\n",
       "      <td>220,045.00</td>\n",
       "      <td>31,761.05</td>\n",
       "      <td>165034</td>\n",
       "      <td>165035</td>\n",
       "      <td>165036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CustomerId</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>19698</td>\n",
       "      <td>15,565,701.00</td>\n",
       "      <td>15,815,690.00</td>\n",
       "      <td>15,692,096.61</td>\n",
       "      <td>71,684.99</td>\n",
       "      <td>15773898</td>\n",
       "      <td>15782418</td>\n",
       "      <td>15807120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Surname</th>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2708</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lucchese</td>\n",
       "      <td>Nott</td>\n",
       "      <td>K?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CreditScore</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>454</td>\n",
       "      <td>350.00</td>\n",
       "      <td>850.00</td>\n",
       "      <td>656.53</td>\n",
       "      <td>80.32</td>\n",
       "      <td>586</td>\n",
       "      <td>683</td>\n",
       "      <td>656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Geography</th>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>France</td>\n",
       "      <td>France</td>\n",
       "      <td>France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gender</th>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Female</td>\n",
       "      <td>Female</td>\n",
       "      <td>Female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Age</th>\n",
       "      <td>float64</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>74</td>\n",
       "      <td>18.00</td>\n",
       "      <td>92.00</td>\n",
       "      <td>38.12</td>\n",
       "      <td>8.86</td>\n",
       "      <td>23.00</td>\n",
       "      <td>46.00</td>\n",
       "      <td>34.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tenure</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>2.81</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Balance</th>\n",
       "      <td>float64</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>22513</td>\n",
       "      <td>0.00</td>\n",
       "      <td>250,898.09</td>\n",
       "      <td>55,333.61</td>\n",
       "      <td>62,788.52</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NumOfProducts</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4</td>\n",
       "      <td>1.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1.55</td>\n",
       "      <td>0.54</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HasCrCard</th>\n",
       "      <td>float64</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IsActiveMember</th>\n",
       "      <td>float64</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <td>float64</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>41670</td>\n",
       "      <td>11.58</td>\n",
       "      <td>199,992.48</td>\n",
       "      <td>112,315.15</td>\n",
       "      <td>50,277.05</td>\n",
       "      <td>160,976.75</td>\n",
       "      <td>72,549.27</td>\n",
       "      <td>138,882.09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                data type  #missing  %missing  #unique           min  \\\n",
       "id                  int64         0      0.00   110023    165,034.00   \n",
       "CustomerId          int64         0      0.00    19698 15,565,701.00   \n",
       "Surname            object         0      0.00     2708           NaN   \n",
       "CreditScore         int64         0      0.00      454        350.00   \n",
       "Geography          object         0      0.00        3           NaN   \n",
       "Gender             object         0      0.00        2           NaN   \n",
       "Age               float64         0      0.00       74         18.00   \n",
       "Tenure              int64         0      0.00       11          0.00   \n",
       "Balance           float64         0      0.00    22513          0.00   \n",
       "NumOfProducts       int64         0      0.00        4          1.00   \n",
       "HasCrCard         float64         0      0.00        2          0.00   \n",
       "IsActiveMember    float64         0      0.00        2          0.00   \n",
       "EstimatedSalary   float64         0      0.00    41670         11.58   \n",
       "\n",
       "                          max       average standard_deviation first value  \\\n",
       "id                 275,056.00    220,045.00          31,761.05      165034   \n",
       "CustomerId      15,815,690.00 15,692,096.61          71,684.99    15773898   \n",
       "Surname                   NaN           NaN                NaN    Lucchese   \n",
       "CreditScore            850.00        656.53              80.32         586   \n",
       "Geography                 NaN           NaN                NaN      France   \n",
       "Gender                    NaN           NaN                NaN      Female   \n",
       "Age                     92.00         38.12               8.86       23.00   \n",
       "Tenure                  10.00          5.00               2.81           2   \n",
       "Balance            250,898.09     55,333.61          62,788.52        0.00   \n",
       "NumOfProducts            4.00          1.55               0.54           2   \n",
       "HasCrCard                1.00          0.75               0.43        0.00   \n",
       "IsActiveMember           1.00          0.50               0.50        1.00   \n",
       "EstimatedSalary    199,992.48    112,315.15          50,277.05  160,976.75   \n",
       "\n",
       "                second value third value  \n",
       "id                    165035      165036  \n",
       "CustomerId          15782418    15807120  \n",
       "Surname                 Nott          K?  \n",
       "CreditScore              683         656  \n",
       "Geography             France      France  \n",
       "Gender                Female      Female  \n",
       "Age                    46.00       34.00  \n",
       "Tenure                     2           7  \n",
       "Balance                 0.00        0.00  \n",
       "NumOfProducts              1           2  \n",
       "HasCrCard               1.00        1.00  \n",
       "IsActiveMember          0.00        0.00  \n",
       "EstimatedSalary    72,549.27  138,882.09  "
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_df = summary(test_df)\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape: (165034, 14)\n",
      "Number of duplicates found and removed: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data type</th>\n",
       "      <th>#missing</th>\n",
       "      <th>%missing</th>\n",
       "      <th>#unique</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>average</th>\n",
       "      <th>standard_deviation</th>\n",
       "      <th>first value</th>\n",
       "      <th>second value</th>\n",
       "      <th>third value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>165034</td>\n",
       "      <td>0.00</td>\n",
       "      <td>165,033.00</td>\n",
       "      <td>82,516.50</td>\n",
       "      <td>47,641.36</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CustomerId</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>23221</td>\n",
       "      <td>15,565,701.00</td>\n",
       "      <td>15,815,690.00</td>\n",
       "      <td>15,692,005.02</td>\n",
       "      <td>71,397.82</td>\n",
       "      <td>15674932</td>\n",
       "      <td>15749177</td>\n",
       "      <td>15694510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Surname</th>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2797</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Okwudilichukwu</td>\n",
       "      <td>Okwudiliolisa</td>\n",
       "      <td>Hsueh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CreditScore</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>457</td>\n",
       "      <td>350.00</td>\n",
       "      <td>850.00</td>\n",
       "      <td>656.45</td>\n",
       "      <td>80.10</td>\n",
       "      <td>668</td>\n",
       "      <td>627</td>\n",
       "      <td>678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Geography</th>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>France</td>\n",
       "      <td>France</td>\n",
       "      <td>France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gender</th>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Age</th>\n",
       "      <td>float64</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>71</td>\n",
       "      <td>18.00</td>\n",
       "      <td>92.00</td>\n",
       "      <td>38.13</td>\n",
       "      <td>8.87</td>\n",
       "      <td>33.00</td>\n",
       "      <td>33.00</td>\n",
       "      <td>40.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tenure</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>11</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>5.02</td>\n",
       "      <td>2.81</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Balance</th>\n",
       "      <td>float64</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>30075</td>\n",
       "      <td>0.00</td>\n",
       "      <td>250,898.09</td>\n",
       "      <td>55,478.09</td>\n",
       "      <td>62,817.66</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NumOfProducts</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4</td>\n",
       "      <td>1.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1.55</td>\n",
       "      <td>0.55</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HasCrCard</th>\n",
       "      <td>float64</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IsActiveMember</th>\n",
       "      <td>float64</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <td>float64</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>55298</td>\n",
       "      <td>11.58</td>\n",
       "      <td>199,992.48</td>\n",
       "      <td>112,574.82</td>\n",
       "      <td>50,292.87</td>\n",
       "      <td>181,449.97</td>\n",
       "      <td>49,503.50</td>\n",
       "      <td>184,866.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Exited</th>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                data type  #missing  %missing  #unique           min  \\\n",
       "id                  int64         0      0.00   165034          0.00   \n",
       "CustomerId          int64         0      0.00    23221 15,565,701.00   \n",
       "Surname            object         0      0.00     2797           NaN   \n",
       "CreditScore         int64         0      0.00      457        350.00   \n",
       "Geography          object         0      0.00        3           NaN   \n",
       "Gender             object         0      0.00        2           NaN   \n",
       "Age               float64         0      0.00       71         18.00   \n",
       "Tenure              int64         0      0.00       11          0.00   \n",
       "Balance           float64         0      0.00    30075          0.00   \n",
       "NumOfProducts       int64         0      0.00        4          1.00   \n",
       "HasCrCard         float64         0      0.00        2          0.00   \n",
       "IsActiveMember    float64         0      0.00        2          0.00   \n",
       "EstimatedSalary   float64         0      0.00    55298         11.58   \n",
       "Exited              int64         0      0.00        2          0.00   \n",
       "\n",
       "                          max       average standard_deviation  \\\n",
       "id                 165,033.00     82,516.50          47,641.36   \n",
       "CustomerId      15,815,690.00 15,692,005.02          71,397.82   \n",
       "Surname                   NaN           NaN                NaN   \n",
       "CreditScore            850.00        656.45              80.10   \n",
       "Geography                 NaN           NaN                NaN   \n",
       "Gender                    NaN           NaN                NaN   \n",
       "Age                     92.00         38.13               8.87   \n",
       "Tenure                  10.00          5.02               2.81   \n",
       "Balance            250,898.09     55,478.09          62,817.66   \n",
       "NumOfProducts            4.00          1.55               0.55   \n",
       "HasCrCard                1.00          0.75               0.43   \n",
       "IsActiveMember           1.00          0.50               0.50   \n",
       "EstimatedSalary    199,992.48    112,574.82          50,292.87   \n",
       "Exited                   1.00          0.21               0.41   \n",
       "\n",
       "                    first value   second value third value  \n",
       "id                            0              1           2  \n",
       "CustomerId             15674932       15749177    15694510  \n",
       "Surname          Okwudilichukwu  Okwudiliolisa       Hsueh  \n",
       "CreditScore                 668            627         678  \n",
       "Geography                France         France      France  \n",
       "Gender                     Male           Male        Male  \n",
       "Age                       33.00          33.00       40.00  \n",
       "Tenure                        3              1          10  \n",
       "Balance                    0.00           0.00        0.00  \n",
       "NumOfProducts                 2              2           2  \n",
       "HasCrCard                  1.00           1.00        1.00  \n",
       "IsActiveMember             0.00           1.00        0.00  \n",
       "EstimatedSalary      181,449.97      49,503.50  184,866.69  \n",
       "Exited                        0              0           0  "
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_df = summary(train_df)\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Numerical/Categorical Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = [column for column in train_df.columns if train_df[column].dtype == 'object']\n",
    "num_cols = train_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "num_cols.remove('id')\n",
    "num_cols.remove('Exited')\n",
    "num_cols.remove('CustomerId')\n",
    "cat_cols.remove('Surname')\n",
    "\n",
    "num_cols.remove('IsActiveMember')\n",
    "cat_cols.append('IsActiveMember')\n",
    "\n",
    "num_cols.remove('Tenure')\n",
    "cat_cols.append('Tenure')\n",
    "\n",
    "num_cols.remove('NumOfProducts')\n",
    "cat_cols.append('NumOfProducts')\n",
    "\n",
    "num_cols.remove('HasCrCard')\n",
    "cat_cols.append('HasCrCard')\n",
    "\n",
    "target_col = 'Exited'\n",
    "\n",
    "train_df['HasCrCard'].astype('category')\n",
    "train_df['IsActiveMember'].astype('category');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category Columns: ['Geography', 'Gender', 'IsActiveMember', 'Tenure', 'NumOfProducts', 'HasCrCard']\n",
      "Numerical Columns: ['CreditScore', 'Age', 'Balance', 'EstimatedSalary']\n",
      "Target Column: Exited\n"
     ]
    }
   ],
   "source": [
    "print(f'Category Columns: {cat_cols}')\n",
    "print(f'Numerical Columns: {num_cols}')\n",
    "print(f'Target Column: {target_col}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Target Distribution\n",
    "Source: https://www.kaggle.com/code/arunklenin/ps4e1-advanced-feature-engineering-ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_pie_chart(data, title, ax):\n",
    "    data_counts = data[target_col].value_counts()\n",
    "    labels = data_counts.index\n",
    "    sizes = data_counts.values\n",
    "    colors = [ (0.4, 0.7, 0.6), 'crimson']  \n",
    "    explode = (0.1, 0)  \n",
    "\n",
    "    ax.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.1f%%', shadow=True, startangle=140)\n",
    "    ax.axis('equal') \n",
    "    ax.set_title(title)\n",
    "\n",
    "fig, axes = plt.subplots(1, 1, figsize=(18, 6))  # Create three subplots in a row\n",
    "\n",
    "plot_pie_chart(train_df, \"Train Churn Distribution\", axes)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. EDA Numerical Data Analysis \n",
    "Source: https://www.kaggle.com/code/arunklenin/ps4e1-advanced-feature-engineering-ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_rows = len(num_cols)\n",
    "n_cols = 2\n",
    "fig, axs = plt.subplots(n_rows // n_cols, n_cols, figsize=(12, 4 * (n_rows // n_cols)))\n",
    "sns.set_palette(\"Set3\")\n",
    "\n",
    "for i, col in enumerate(num_cols):\n",
    "    row_idx = i // n_cols\n",
    "    col_idx = i % n_cols\n",
    "\n",
    "    sns.violinplot(x=target_col, y=col, data=train_df, ax=axs[row_idx, col_idx])\n",
    "    axs[row_idx, col_idx].set_title(f'{col.title()} Distribution by Target (Train)', fontsize=14)\n",
    "    axs[row_idx, col_idx].set_xlabel('Churn', fontsize=12)\n",
    "    axs[row_idx, col_idx].set_ylabel(col.title(), fontsize=12)\n",
    "    sns.despine()\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. EDA Categorical Data Analysis\n",
    "Source: https://www.kaggle.com/code/arunklenin/ps4e1-advanced-feature-engineering-ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABlkAAAGxCAYAAADh3tC3AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAABgVElEQVR4nO3dd3yN9///8efJTgSxY5UYlSYSohKjglKltKqqraq9a+9RVbNCjRgxS4zaq/OjgyraqlmrNVrl42MlUREpiczz+8Mv5+tIVFzCSeRxv93cbjnv631d1+s65yDXeZ73+20ym81mAQAAAAAAAAAA4IHY2boAAAAAAAAAAACAnIiQBQAAAAAAAAAAwABCFgAAAAAAAAAAAAMIWQAAAAAAAAAAAAwgZAEAAAAAAAAAADCAkAUAAAAAAAAAAMAAQhYAAAAAAAAAAAADCFkAAAAAAAAAAAAMIGQBAAAA7sNsNtu6hMcuO1xzdqght+C5BgAAAIwhZAEAAECWadeunSpVqmT1p3Llyqpfv77GjRun69ev27rEBzZ//nwtWbLE1mVYufs59vHxUY0aNdSpUyft3LnTqu+FCxdUqVIlbd68OdPHz+w1N2jQQCNGjDB8nnuJiIhQjx49dPHixQzPlR3cuHFD7777rqpUqaLAwED997//tdo+Z86cdK9TRn+ygw0bNmjKlCm2LsOmvvnmG3Xv3l3BwcGqXLmy6tSpo379+unw4cOPtY7NmzerUqVKunDhwmM9LwAAAIxzsHUBAAAAeLL4+PhozJgxlsdJSUn6/fffNWPGDJ04cUJr1qyRyWSyYYUPZubMmerTp4+ty0inVatWeuONNyTdfo6vXLmijRs3qnv37ho9erTatm0rSSpatKjWrVunp556KtPHzuw1h4WFyd3d3dgF/Ivdu3drx44dGj169CM/l1GfffaZtm/frg8++EAVK1ZUqVKlrLa/8cYbCg4OtjzesGGDNm7cqHXr1j3uUu9r/vz5CgoKsnUZNpGcnKzBgwdr69atat68uUaPHq0CBQro0qVLWr9+vd5++21NnTpVL7/8sq1LBQAAQDZFyAIAAIAs5e7urqpVq1q1BQYG6ubNm5o9e7aOHDmSbjsenKenZ7rn8aWXXlLv3r01efJk1a9fX6VKlZKTk9Mje759fHweyXFtfa7MiImJkSS1adMmw9DQ09NTnp6elsc//vijJPHez2YWLFigb775RqGhoWratKnVtldeeUW9e/fWuHHj1LBhQ7m6utqoSgAAAGRnTBcGAACAx6Jy5cqSpEuXLlnatm3bppYtW8rPz0/PPfecJk6cqLi4OMv2OXPmqFGjRgoLC1ONGjX0wgsv6Nq1azKbzVq1apWaNWsmf39/NWrUSB9//LHVuhIHDhxQ27ZtVaVKFQUFBWn48OGKjo62bN+8ebN8fHx05MgRvfXWW/Lz81P9+vX18ccfW/qkTecUFhZmNbXTtm3b1KZNGwUEBKhy5cpq0qSJVq5caXW9f/31l7p166Zq1aqpdu3aCg0N1ciRI9WuXTtLn9TUVC1atEiNGjVS5cqV1bhxY33yySeGn2OTyaTBgwcrKSlJGzdulJR+Gq/U1FTNmjVLDRo0UOXKldWgQQPNmDFDSUlJ97zme70OGU3hFRkZqR49esjf31/16tXT7NmzlZKSYtme0T53TpG0efNmjRw5UpLUsGFDS9+79/vnn38UEhKiF154QX5+fnr55Zct13znuWbPnq0pU6aodu3a8vf3V5cuXXT27Nl/fR4TEhI0d+5cNWnSRH5+fnrxxRe1aNEipaamSro9Ld6cOXMkSd7e3g81jdn+/fvVpUsXBQYGWl6POXPmWM6V9votXbpUL730koKCgiyv5Y4dO9SyZUv5+/urcePG+uqrr9SoUSNLbdLtMOiDDz5Q7dq15efnpzfffFO//PKL1XN08eJFffrpp/86TVVKSooWLVqkl19+Wf7+/qpatapat25tdSxJ+u2339S1a1c9++yzqlmzpgYOHKjLly9Lkvbu3atKlSpp7dq1ev7551W7dm399NNPkqSff/5Zbdq00bPPPqsaNWpo8ODBlv2k+79vJWnLli1q3ry5/P39VbNmTQ0ZMkRRUVH3fO7j4+O1ZMkSNW7cOF3AIkl2dnYaMGCAatWqZfVvx6VLlzRo0CAFBQWpSpUq6tChg44fP27Znvaaff311+rXr58CAgIUGBioUaNG6ebNm1bXNG/ePNWvX19VqlRRr169MpxS8Y8//lCPHj1UrVo1VatWTb1799b58+ct2//teQUAAMCjx0gWAAAAPBZpH2yXLl1akvTll19qyJAheuWVVzRgwABdvHhRoaGhOn36tJYuXWoZHXDp0iVt3bpVM2bM0LVr11SgQAFNnz5dS5YsUceOHfXcc8/p999/V2hoqBITE9W7d2/t379fnTp1Us2aNTVz5kxdv35ds2bNUvv27bVx40a5uLhIuv0h54ABA9SxY0cNGDBAGzdu1LRp0+Tt7a3g4GCtW7dOb731ltXUXDt27FDv3r3Vvn179e3bV7du3dLKlSs1YcIE+fj4qFq1aoqOjlbbtm1VqFAhhYSEKCUlRbNmzdKlS5esRjKMHTtWmzdvVo8ePRQQEKD9+/dr0qRJio2NVe/evQ09z+XLl1fx4sV18ODBDLd//PHHWrVqlYYPH67SpUvryJEjCg0NlaOjo/r27ZvhNd/rdcjInDlz9Oqrr2ru3Lk6dOiQFixYoJSUFA0cODBT9devX1/vvvuu5s+fny7cSnPr1i21adNGf//9t/r27avSpUtr27ZtGjVqlP7++2/17NnT0nfFihV69tlnFRISouvXr+vDDz/UiBEj7jltl9lsVs+ePXX48GH17t1bzzzzjPbu3auZM2fq/PnzmjBhgsaMGaOlS5dapv8qWLBgpq7tbidPnlTHjh3VpEkThYaGymw26/PPP1dYWJjKli2rV155xdI3NDRUH3zwgfLly6fKlStrz5496tWrl55//nn1799f586d05gxY5SQkGDZJyEhQR06dNDff/+tgQMHqmjRotq0aZO6du2qxYsXq1atWgoLC1P37t3l4+OjXr16qWjRohnWOm3aNK1evVpDhgxRpUqVFBERoblz56p///7asWOH3NzcdPLkSb399tvy9/fX5MmTZTabNX36dHXu3FlffPGF1bWMGzdOCQkJqlq1qj7//HMNGzZMTZs2VY8ePXTt2jXNnj1bb731lj799FMVKlTovu/bgwcPasiQIerVq5cCAwMVERGhqVOnavDgwfcMLnfv3q24uDir5/lulSpV0uzZsy2Po6Oj1bp1a7m6umr06NFydXXV8uXL9c4772jjxo0qX768pe+YMWP0+uuva968eTp69KhCQ0NVsGBBDR48WJI0depUrVixQj179lTVqlX1zTffaPr06VbnP3v2rFq3bq1y5cpp8uTJSklJ0fz58/X222/r888/V6FChe75vAIAAODxIGQBAABAljKbzUpOTrY8vn79uvbt26f58+eratWqqly5ssxms6ZNm6bg4GBNmzbN0rds2bLq2LGjdu7cqfr160u6vWbC8OHDVbt2bUlSbGysli5dqnbt2mnYsGGSpOeee07R0dGWYGH69Ony8vLSwoULZW9vL0mqUqWKmjVrpk2bNumdd96x1NqrVy9LmPDss89q69at2rFjh4KDgy0fVN45Ndfp06fVokULjRo1ylJ3QECAatSoof3796tatWr65JNPdPPmTX322WcqVqyY5fyNGze27HP27FmtX79egwYNUvfu3SVJderUkclk0sKFC9WmTZt7Bhn3U7hwYf39998Zbtu3b598fX31+uuvS5KCgoLk6upqWe8ko2uW0r8O91KrVi2FhIRIkoKDg3Xjxg2tWLFCnTt3Vv78+e9be8GCBS3rxzzzzDPp1jqRbo98+eOPP7R69Wo9++yzlnMlJydr3rx5at26tTw8PCRJ+fLl07x58yzvg//973+aM2fOPYOiXbt2affu3Zo6daqaN28u6fb7y8XFRbNmzVKHDh1UoUIFy1RgD/Nh9smTJ1W7dm1NnTpVdnZ2lnPt2LFD+/fvt/rw/8UXX1SrVq0sj4cOHaoKFSooLCzMEkgWKlRIgwYNsvT5/PPPdfLkSa1fv15VqlSRJNWtW1ft2rXTtGnTtGnTJvn4+MjJyUkFCxb812uJiorSwIEDrUZiubi4qG/fvjp16pQCAgI0b9485c+fX+Hh4XJ2dpZ0+300YMAAnTp1yrJf69at1aRJE0m3g86pU6daRnulqVatmpo2barw8HANHTr0vu/bgwcPytnZWd26dbOc28PDQ8eOHZPZbM5wSre00SBly5a1ak9NTbWMJEpjZ2cnOzs7LV++XDExMVqzZo1KlixpeU6bNm2qWbNmWQUy9erV0/DhwyXd/nvx888/a8eOHRo8eLBiY2P1ySefWMJa6fZ7ODIy0jK1nHR7RJmLi4uWLVtmudZatWrphRde0OLFiy3Hv/t5BQAAwOPDdGEAAADIUvv375evr6/lT+3atTVo0CD5+vpqxowZMplMOnPmjCIiItSgQQMlJydb/gQGBsrd3V0///yz1TGffvppy8+HDx9WUlKSGjVqZNVnxIgRCg8PV3x8vI4cOaJ69epZAp/k5GSVLl1a5cuXT3fsgIAAy89pHzbfOWXZ3bp27aopU6YoLi5OJ0+e1Ndff61FixZJkmXqoj179iggIMASsEhSyZIlrc61Z88emc3mdM9BgwYNlJCQcM+RKJmV0YfKklSjRg3t3r1bbdq00dKlS/XXX3+pbdu2atGixX2PeefrcC93T7v04osvKi4uTocPH85M2Zmyb98+lSxZ0hKwpGnevLkSEhJ05MgRS5ufn58lYJFkCUfi4+PveWx7e/t015EWuOzduzdLrkGSWrRooY8//lhJSUn6888/tW3bNs2ZM0cpKSlW02BJ1s99YmKiDh06pMaNG1u9zo0bN5aDw/99j+6XX35RkSJF5Ovra3l/paSk6Pnnn9dvv/2W4dRU9zJ9+nR17NhR0dHROnTokDZv3mwZnZJW68GDB1W3bl1LyCFJ/v7+2r59u2W6QElWo5POnj2rK1eupBtN8tRTTykgIMDyfN/vfRsYGKhbt27plVdeUWhoqA4ePKg6deqoT58+9/y7cHeQkmbWrFlW/4b5+vpq7ty5luf0mWeeUbFixSzPqZ2dnerWravdu3dbHefu0MrT09Pyb0vav2MNGza06vPSSy9ZPd6zZ49q1KghFxcXy/nc3d1VvXr1dOfLaNQXAAAAHj1GsgAAACBL+fr6aty4cZJuf9Dv7Oys4sWLW76FLf3fouHjxo2z9L3T3esoFC5cON2+95qiKTY2Vqmpqfr444+t1ldJc+cHwJIsU4elsbOzs1rb5W7R0dEaM2aMtm3bJpPJpDJlylg+7E/bLzo6Wr6+vun2LVKkiK5cuWJ1Hc2aNcvwPJGRkfes4X4iIyNVsWLFDLd17dpVefLk0aZNmzRlyhRNnjxZTz/9tN577z3VqlXrX4975+uQ2T5pr9ODfKB/P9evX8+wlrS22NhYS9vdi5WnjRi51wfs169fV4ECBazCCun2ayfdXgsmq9y6dUsTJkzQ559/ruTkZJUqVUoBAQFycHBI9x68++9ASkqK1VRRkuTg4GA1OicmJkZXrlzJ8L0oSVeuXMnU6CJJOnbsmMaNG6djx47JxcVFFSpUsIzkSKs1JiYmXU0ZubNP2t+De72eaWud3O99GxAQoEWLFmnZsmVasmSJFixYoCJFiqhbt27q0KFDhnWk1X/x4kWrvy9t2rTRCy+8YHl85wiimJgYnTt37p7P6Z3hXUbvvbTnKu3vw93/jqW9z+4835YtW7Rly5Z057p738w89wAAAMh6hCwAAADIUnny5JGfn9+/9smXL58kadiwYQoKCkq3/d8++E3bNzo6WuXKlbO0X758WefOnVPlypVlMpnUsWPHDAOMuz/4fFBDhgzRX3/9paVLl6patWpycnJSfHy8NmzYYOnj6empq1evptv3zra061i+fLny5MmTrm+JEiUM1ffXX38pKipKbdq0yXC7nZ2d3nnnHb3zzju6evWqdu7cqQULFqhv377avXu3nJycDJ03zZ0BhyTLtGV3fgCckpJi1effRg5lJH/+/Dp37ly69rQAy+g0a2nHvnbtmpKTk62ClrTg72GOfbcPP/xQ3377rWbOnKnatWvLzc1Nku4bdhUqVEiOjo7p3mOpqam6du2a5XHevHlVtmxZqyn57pTRVGwZuXHjhrp27apKlSrpq6++Uvny5WVnZ6edO3fq22+/tTrfnQvEp9m5c6e8vb0zPHbatG4ZTW935coVy/OdmfdtcHCwgoODFR8frz179mjFihWaNGmSqlatapku7U7PPfecXF1d9c0331imJ5SkYsWKWY1Cu1PevHkVFBRkmarwbpn9+5N2XVevXrX6dywtdLrzfLVr11anTp3SHePuIBAAAAC2wXRhAAAAeOzKlSunQoUK6cKFC/Lz87P88fT01PTp0y3fXs+Iv7+/HB0d9f3331u1L1++XP3795eLi4t8fHx05swZq2NXrFhRYWFhDzzdU9rIhzQHDx5U48aNVbNmTcsHqrt27ZL0f6MjAgMDdejQIcuH/tLtD4zvnDIrMDBQknTt2jWrOmNiYjRz5sx0H7Zm1uzZs+Xi4qLXXnstw+2tW7fWxIkTJd3+sL5ly5Z655139M8//+jGjRsZXvODuHM9CUn6z3/+I1dXV8uH3O7u7oqIiLDq8+uvv1o9vt/5AwMDdfHixXRTqn3xxRdydHSUv7+/0fIVFBSklJSUdCMH0qbGunuKsodx8OBB1ahRQy+88IIlYPntt98UHR19z5E2kmRvb69q1app27ZtVu3bt2+3Wg8pKChIly9fVqFChazeY7/88osWL15smUbtfs/3mTNnFBMTo/bt26tixYqW/ne/76tXr64ff/xRiYmJln1PnTql7t2769ixYxke28vLS0WKFNGXX35p1X7+/HkdPnxY1apVk3T/9+2UKVPUqlUrmc1mubq66vnnn7esV3L58uUMz+3u7q7OnTvrs88+0zfffJNhnz///NPqcVBQkM6ePSsvLy+r5/SLL77Qhg0brKam+zcBAQFycXFJd94ffvgh3flOnz6tZ555xnKuypUra9myZdq6dWumzgUAAIBHi6++AAAA4LGzt7fXwIED9cEHH8je3l7PP/+8YmNjNW/ePEVGRt5zKh7p9hQ57du31/Lly+Xk5KSaNWvq2LFjWrlypQYNGiQHBwfLYvKDBw9W8+bNlZKSovDwcB05ckTvvvvuA9WaL18+HTp0SPv371f16tXl7++vL7/8Ur6+vvL09NShQ4e0cOFCmUwmy1RB7du316pVq9SlSxf17t1bkjR37lwlJiZa1od4+umn1bx5c40ePVoXL15U5cqVdfbsWYWGhqpUqVLpFuO+W0REhCW0SU5OVmRkpD799FP99NNPGj9+vGXtkbsFBgYqPDxchQsXVkBAgCIjI7V06VIFBQVZph+6+5ofxHfffadixYqpdu3a+umnn7Ru3Tr179/fMl3c888/r4ULF2rBggWqWrWqduzYoV9++SXdcy5JW7duVd26dVW+fHmr7S1bttTq1avVp08f9evXT6VLl9b27du1adMm9enTx7K/EXXr1lWNGjU0ZswYRUVFycfHR/v27dPHH3+s1157TRUqVDB87Lv5+/vr66+/1po1a1S+fHmdPHlS8+fPt3ov3Uu/fv3Url079evXT61atdKlS5c0a9YsSf+3Hk/Lli21cuVKderUST179lTx4sW1e/duffzxx2rbtq0cHR0l3X6+jx8/rn379snf3z/dFHpeXl5yd3fXggUL5ODgIAcHB3377bfauHGjpP+bIqtXr1566623LFN0JSYmWtY3qVu3rg4dOpTuOuzs7DRo0CCNHDlSAwcOVIsWLXTt2jWFhYUpf/78lhEc93vf1qpVS0uXLtWIESPUvHlzJSUlafHixfLw8FDNmjXv+Tz27t1bly9fVv/+/dW4cWM1btxYRYsW1ZUrV/TDDz/o66+/VrFixSzH6Nixoz7//HN17NhRnTt3VoECBbRlyxatX79eI0eOvO9rniZPnjzq1auXZs6cKVdXV9WsWVM7d+5MF7L06tVLrVu3Vo8ePfT222/L2dlZ69at07Zt2zR79uxMnw8AAACPDiELAAAAbOKNN95Qnjx5tHjxYq1bt05ubm6qVq2apk2bptKlS//rvkOHDlXhwoW1Zs0ahYeHq1SpUnrvvfcsU2TVqVNHS5YsUVhYmPr16ydHR0f5+vpq6dKl6Rajvp+ePXtq3rx56tatm7Zs2aLJkydrwoQJmjBhgiSpbNmyGjdunL744gsdOHBA0u0PrVesWKEPP/xQw4YNU548edSmTRu5ublZRixIUkhIiBYuXKi1a9cqIiJChQoVUtOmTTVgwID7fiN+48aNlg+5HR0dVbRoUVWuXFkrV67812Ckf//+cnJy0qZNmzR37lzlzZtXDRo00ODBg+95zQ9ixIgR+uabb7Rs2TIVKVJEI0eOtFoTo0ePHoqOjlZ4eLiSkpJUv359ffjhh1bhV40aNVS7dm1Nnz5dv/zyixYtWmR1DldXV33yySeaPn26Zs+erRs3bqhcuXL68MMPrdbPMMJkMmnhwoWaPXu2VqxYoejoaJUqVUoDBw7McMqmhzFixAglJSVp5syZSkxMVKlSpfTuu+/q9OnT2r59e7pp1e5UvXp1zZkzR7NmzVKvXr1UsmRJjR49WgMHDrRMP+fm5qZVq1Zp+vTpmjp1qv755x+VLFlSgwcPVufOnS3H6ty5syZNmqQuXbpo6dKl6d4/efPm1bx58/TRRx+pf//+ypMnj5555hmtXLlS3bp104EDB9SgQQP5+PhYXpe0OurVq6chQ4b86zRaLVu2VJ48ebRw4UL17t1b7u7uCg4O1qBBgyxrlNzvfVu3bl1NmzZN4eHhlsXun332Wa1YscIyJVlG7O3tFRISoqZNm2rDhg2aOnWq/v77b8s1jho1Si1atLBMM1isWDGtXbtW06dP19ixY5WQkKCyZcsaeu/16NFDbm5uWr58uZYvX66AgAANHz5cY8eOtfTx9vbWqlWrFBoaqmHDhslsNuvpp5/W3Llz1bBhwwc6HwAAAB4Nk/nfVvUEAAAA8MCOHDmimJgY1atXz9KWnJys+vXrq1mzZg/0jXcgI99//708PT2tRn39+eefevnllzVv3jw+gAcAAAAeE0ayAAAAAFns0qVLGjhwoHr37q2goCDFx8dr7dq1+ueff/Tmm2/aujw8AX766Sdt2bJFQ4YMkZeXlyIiIjR//nyVK1dOderUsXV5AAAAQK7BSBYAAADgEVizZo1Wr16t8+fPy9HRUVWqVFH//v3l5+dn69LwBLh165ZmzZqlb7/9VlFRUfLw8FBwcLAGDx6swoUL27o8AAAAINcgZAEAAAAAAAAAADDAztYFAAAAAAAAAAAA5ESELAAAAAAAAAAAAAYQsgAAAAAAAAAAABjgYOsCbO3QoUMym81ydHS0dSkAAAAAAAAAAMDGkpKSZDKZFBAQcN++uX4ki9lsltlstnUZAAAbM5vNSkxM5P8EAAAAIJfj3gAA8CC5Qa4fyZI2gsXPz8/GlQAAbCkuLk4nTpxQhQoV5ObmZutyAAAAANgI9wYAgGPHjmW6b64fyQIAAAAAAAAAAGAEIQsAAAAAAAAAAIABhCwAAAAAAAAAAAAGELIAAAAAAAAAAAAYQMgCAAAAAAAAAABggIOtCwAAAAAAAAAA4EmVkpKipKQkW5eB/8/R0VH29vZZdjxCFgAAAAAAAAAAspjZbFZERIRiYmJsXQru4uHhIU9PT5lMpoc+FiELAAAAAAAAAABZLC1gKVq0qNzc3LLkA308HLPZrLi4OEVFRUmSihcv/tDHzFYhy7x58/TLL7/ok08+uWefa9euaeLEidq1a5ckqUmTJho5cqTc3NweV5kAAAAAAAAAANxTSkqKJWApVKiQrcvBHVxdXSVJUVFRKlq06ENPHZZtFr5ftmyZZs+efd9+/fr10/nz5y39f/75Z40bN+4xVAgAAAAAAAAAwP2lrcHC4IDsKe11yYq1cmw+kiUyMlKjRo3SwYMH5eXl9a99Dx06pH379mnLli0qX768JGn8+PHq2rWrBg0apGLFij2OkgEAAAAAAAAAuC+mCMuesvJ1sflIlt9//1358+fXF198oSpVqvxr3wMHDqhIkSKWgEWSgoKCZDKZdPDgwUddKgAAAAAAAAAAgIXNR7I0aNBADRo0yFTfyMjIdAvRODk5ycPDQ5cvXzZcQ9piN0BuRqqO3C4xMVGurq5KTEzk7wNyNbPZbOsSAAA2xu9CyO24NwBu497g4SQkJCg1NVUpKSlKSUm5Z7/ffvtNn3zyifbv369r166pSJEiqlmzprp166bSpUtb+nXo0EGStHz58kdeuxFRUVFasWKFfvjhB12+fFl58+aVj4+PunTpourVq1v6hYWFad68eTp+/LgNq729Zk5qaqri4+OVmpqabrvZbM70/wE2D1keRHx8vJycnNK1Ozs7KyEhwfBxk5KSdOLEiYcpDcjRHB0d5fuMj+wdc9Q/CUCWcnV1lYeHh63LAGwqJSlZv584niVz0gIAcibuDQDuDQCJe4Os4uDg8K+fW69bt07Tp09X9erV1bdvXxUpUkTnz5/XihUr9N1332n+/Pl65plnJMkSBNy6deux1P4gDh8+rEGDBsnDw0Nvv/22ypQpo9jYWG3evFkdOnTQ6NGj1aJFC0lScnKyJNtfR0JCgpKTk3XmzJl79skoi8hIjvqtycXFRYmJienaExISHmoBIUdHR1WoUOFhSgNyNJPJJHtHB0X2HK/EP8/ZuhwAgA04VSyjYgs+UMWKFfnGGgDkYtwbAAC4N8gaCQkJunTpkpydneXi4pJu+6+//qpp06apTZs2GjlypNW2Jk2a6PXXX9f48eP16aefSpLs7G6v/JHRsWwpJiZGI0aMUNmyZbVkyRK5urpatjVt2lR9+vTR5MmT1bBhQxUpUkQODrcjiexwHQ4ODnrqqafk7Oycbtvp06czf5ysLOpR8/T01LZt26zaEhMTFRMT81CL3ptMpocKaYAnReKf55R49A9blwEAsKE7fyEGAORe3BsAALg3eDh2dnays7OTvb297O3t021funSp8ubNq8GDB6fbXrhwYY0cOVKnT59WfHy83N3dZTKZZDabFR4erlWrVik6OlrPPPOM3n//ffn5+UmS5syZo7CwMJ06dcrqeJUqVVKfPn3Ut29fXbhwQQ0bNtSIESO0fv16Xb16VSNGjNDFixf1xRdfaNSoUZo+fbrOnj2rkiVLqmfPnnrttdfueZ1ffvmloqKiNHfuXLm7u1tts7e315AhQ7Rp0ybFxcXJ3t7eEhalXXNKSoqWLFmiL774Qv/73/9kZ2cnb29v9e/fX7Vq1ZJ0O7CaPHmyvv/+e0VHR6tUqVJ688031blzZ8u5PvnkE61atUoXL16Uh4eHGjZsqCFDhqSr6c7a7Ozs5OrqmmHg8yDTReaokCUwMFDTpk3TuXPnVKZMGUnS3r17JUnVqlWzZWkAAAAAAAAAANyX2WzWTz/9pAYNGtwzzGrSpEm6toMHDyoxMVGjR49WYmKipkyZop49e2rnzp2WESKZFRoaqg8++ED58uVT5cqVtWnTJl25ckXjx4/Xu+++q5IlS2rJkiUaMWKE/P39Vb58+QyP8+OPP6pQoULy9/fPcHvFihU1YsSIe9Yxbdo0rV69WkOGDFGlSpUUERGhuXPnqn///tqxY4fc3Nz04Ycf6qefftLw4cNVuHBh7dq1S1OmTJGHh4datmyp//znP5oyZYqGDx+uSpUq6cyZM5oyZYpu3bqlyZMnP9DzYkS2DllSUlIUHR2tvHnzysXFRVWqVFG1atU0cOBAjR07VnFxcRozZoxatGjxUCNZAAAAAAAAAAB4HK5du6aEhASVKlXqgfZzcnLSokWLLOtG3bhxQ++//75Onz4tb2/vBzrWiy++qFatWlm1xcfH68MPP7SMIClbtqyef/557dy5854hS2Rk5ANfx52ioqI0cOBAtWvXztLm4uKivn376tSpUwoICNC+fftUu3ZtNWvWTJJUo0YNubm5qUCBApJuD8QoWbKk3nnnHdnZ2SkoKEhubm66du2a4boeRLYOWS5fvqyGDRsqJCRELVu2lMlkUlhYmMaNG6cOHTrI2dlZTZo0STdnHQAAAAAAAAAA2VHalFkpKSkPtF+FChUsAYskS7jxzz//PHANTz/9dIbtVatWtfzs6ekpSYqLi7vncUwm0wNfx52mT58uSYqOjta5c+d09uxZbd++XZKUlJQk6XaosnbtWkVGRur5559XvXr11Lt3b8sxatasqXXr1qlly5Z68cUXVb9+fb3yyisPNOXXw8hWIcvdQ3dKlSqVbv64QoUKafbs2Y+zLAAAAAAAAAAAsoSHh4fy5MmjS5cu3bNPXFycEhMTrUKVu9cVTwtrUlNTH7iGwoULZ9h+5/Rlacc3m833PE7JkiV19OjRfz3X5cuXVbx48Qy3HTt2TOPGjdOxY8fk4uKiChUqqGTJklbnHTVqlDw9PfXFF19o3LhxkqSAgAB98MEH8vHxUdOmTZWamqrVq1crLCxMs2bNUsmSJTV48GDL6JdHye6RnwEAAAAAAAAAAFjUqVNHe/fuVUJCQobbN2/erFq1aunQoUOZPmbayI07R5bcvHnz4Qq9j+DgYF29elXHjh3LcPuff/6p+vXra9GiRem23bhxQ127dpWbm5u++uorHTp0SJs2bdLrr79u1c/JyUnvvvuuvv76a/3www/64IMPdP78eQ0ePNjS5+WXX9bq1au1d+9ezZw5Ux4eHho6dKgiIyOz9oIzQMgCAAAAAAAAAMBj1LlzZ8XExCg0NDTdtqtXr2rx4sUqU6aM1fRd9+Pu7i7p9siRNL/++utD1/pvmjdvriJFimjSpEmKj4+32paamqqpU6fK0dExwxElZ86cUUxMjNq3b6+KFStaRs7s2rXLsv+tW7fUuHFjhYeHS5JKlCihd955R82aNVNERIQkacCAAerTp48kKW/evHrppZfUq1cvpaSkKCoq6pFde5psNV0YAAAAAAAAAABPuqpVq6p///6aOXOm/vrrL7322msqUKCA/vzzT4WHh+vmzZtatGjRA60rUq9ePYWEhGj06NHq1q2bIiIiFBYWpjx58jyy68ibN68mT56sPn366I033lDbtm3l5eWliIgIrVmzRocPH9bkyZMtU4DdycvLS+7u7lqwYIEcHBzk4OCgb7/9Vhs3bpQkxcfHy8XFRb6+vgoLC5Ojo6MqVaqks2fP6tNPP1Xjxo0l3V6TZcyYMZoyZYrq1q2r2NhYhYWFqWzZsvL29n5k156GkAUAAAAAAAAAgMfs3XfflY+Pj1atWqWQkBDFxMTI09NTdevWVc+ePVWiRIkHOp6Xl5emTJmi+fPnq3v37ipfvrwmTJigCRMmPKIruK1OnTrasGGDwsPD9fHHH+vKlSvKnz+/fH19tWbNGgUEBGS4X968eTVv3jx99NFH6t+/v/LkyaNnnnlGK1euVLdu3XTgwAE1aNBA48eP18yZMxUeHq4rV66oUKFCatWqlfr37y9Jat26tZKSkrR27VqtXr1aLi4uqlWrloYOHSpHR8dHeu2SZDL/26o1uUDaXHF+fn42rgSwvfMNuyjx6B+2LgMAYANO/k+r9PdLbF0GACCb4N4AAHIv7g2yxq1bt3T27Fl5eXnJxcXF1uXgLvd7fR4kN2BNFgAAAAAAAAAAAAMIWQAAAAAAAAAAAAwgZAEAAAAAAAAAADCAkAUAAAAAAAAAAMAAQhYAAAAAAAAAAAADCFkAAAAAAAAAAAAMIGQBAAAAAAAAAAAwgJAFAAAAAAAAAADAAEIWAAAAAAAAAAAAAwhZAAAAAAAAAAAADCBkAQAAAAAAAAAgB0s1p+ao86ampmr27NkKDg5WlSpV1LlzZ507dy6Lq3s8HGxdAAAAAAAAAAAAMM7OZKclh7br8j/XHts5i+ctoC4BDQztO2/ePK1du1YhISEqVqyYpk6dqm7duumrr76Sk5NTFlf6aBGyAAAAAAAAAACQw13+55rOx161dRn3lZiYqPDwcA0dOlT16tWTJIWGhio4OFhbt25Vs2bNbFzhg2G6MAAAAAAAAAAA8FicPHlSN2/eVM2aNS1t+fLlk4+Pj/bv32/DyowhZAEAAAAAAAAAAI9FRESEJKl48eJW7UWLFtXly5dtUdJDIWQBAAAAAAAAAACPRXx8vCSlW3vF2dlZCQkJtijpoRCyAAAAAAAAAACAx8LFxUXS7bVZ7pSQkCBXV1dblPRQCFkAAAAAAAAAAMBjkTZNWFRUlFV7VFSUPD09bVHSQyFkAQAAAAAAAAAAj4W3t7fc3d21d+9eS1tsbKyOHz+u6tWr27AyYxxsXQAAAAAAAAAAAMgdnJyc1LZtW02bNk0FCxZUyZIlNXXqVHl6eqpRo0a2Lu+BEbIAAAAAAAAAAJDDFc9bIMecr1+/fkpOTtb777+vW7duKTAwUEuWLJGTk1MWVvh4ELIAAAAAAAAAAJCDpZpT1SWggU3Oa2d68FVJ7O3tNXToUA0dOvQRVPV4sSYLAAAAAAAAAAA5mJGgIyefNzvhGQAAAAAAAAAAADCAkAUAAAAAAAAAAMAAQhYAAAAAAAAAAAADCFkAAAAAAAAAAAAMIGQBAAAAAAAAAAAwgJAFAAAAAAAAAADAAEIWAAAAAAAAAAAAAwhZAAAAAAAAAAAADCBkAQAAAAAAAAAANjFv3jy1a9fO1mUYRsgCAAAAAAAAAEAOZk5JyZHnXbZsmWbPnp1F1diGg60LAAAAAAAAAAAAxpns7RXZc7wS/zz32M7pVLGMii34wNC+kZGRGjVqlA4ePCgvL68sruzxImQBAAAAAABWnCqWsXUJAAAb4f+AnCvxz3NKPPqHrcvIlN9//1358+fXF198oblz5+rixYu2LskwQhYAAAAAAGBhTkkx/K1UAMCTwZySIpO9va3LwBOsQYMGatCgga3LyBKELAAAAAAAwMJkb6/PTu7T33H/2LoUAIANFHbLqxbeQbYuA8gxCFkAAAAAAICV36LO63zsVVuXAQCwgdL5ChGyAA/AztYFAAAAAAAAAAAA5ESELAAAAAAAAAAAAAYQsgAAAAAAAAAAABhg8zVZUlNTFRYWpg0bNig2NlbPPvusxowZozJlymTY/8qVKwoJCdHPP/8sSapZs6ZGjhwpT0/Px1k2AAAAAAAAAADZhlPFjD9Tf1LOl13ZPGSZN2+e1q5dq5CQEBUrVkxTp05Vt27d9NVXX8nJySld/4EDByolJUVLly6VJI0bN069evXS5s2bH3fpAAAAAAAAAADYnDklRcUWfGCT85rs7R/qGJMnT86iamzDptOFJSYmKjw8XH379lW9evXk7e2t0NBQRUZGauvWren6x8bGav/+/erWrZt8fHzk4+Oj7t276/fff9e1a9dscAUAAAAAAAAAANjWwwYdOe282YlNQ5aTJ0/q5s2bqlmzpqUtX7588vHx0f79+9P1d3Z2lpubmz777DPduHFDN27c0Oeff66yZcsqf/78j7N0AAAAAAAAAACQy9l0urCIiAhJUvHixa3aixYtqsuXL6fr7+zsrA8//FDjx49X9erVZTKZVKRIEa1cuVJ2djbNiwAAAAAAAAAAQC5j05AlPj5ektKtveLs7Kzr16+n6282m3Xq1CkFBASoa9euSklJUWhoqHr37q01a9bI3d3dUB1ms1lxcXGG9gWeBCaTSa6urrYuAwCQDcTHx8tsNtu6DACAjXBvAABIw73Bw0lISFBqaqpSUlKUkpJi63Jwl5SUFKWmpio+Pl6pqanptpvNZplMpkwdy6Yhi4uLi6Tba7Ok/SzdfgNm9Evdf/7zH61evVo//PCDJVBZsGCBnn/+eW3atEkdOnQwVEdSUpJOnDhhaF/gSeDq6iofHx9blwEAyAbOnj1r+SIMACD34d4AAJCGe4OH5+DgoISEBFuXgQwkJCQoOTlZZ86cuWefuweH3ItNQ5a0acKioqL01FNPWdqjoqLk7e2drv/Bgwfl5eVlNWIlf/788vLy0n//+1/DdTg6OqpChQqG9wdyusymsgCAJ5+XlxffVgOAXIx7AwBAGu4NHk5CQoIuXbokJycnqwEGyB7MZrMcHBz01FNPydnZOd3206dPZ/pYNg1ZvL295e7urr1791pCltjYWB0/flxt27ZN17948eLasmWLEhISLBceHx+vCxcu6JVXXjFch8lkkpubm+H9AQAAnhRMEQMAAABA4t7gYTk7OysiIkIJCQmGl7nAo5OQkCA7Ozvly5dP9vb26bY/yBdPbBqyODk5qW3btpo2bZoKFiyokiVLaurUqfL09FSjRo2UkpKi6Oho5c2bVy4uLmrRooWWLFmiAQMGqH///pKkmTNnysnJSS1btrTlpQAAAAAAAAAAIEmyt7eXh4eHoqKiJElubm6MGM0G0tZnj4qKkoeHR4YBy4OyacgiSf369VNycrLef/993bp1S4GBgVqyZImcnJx04cIFNWzYUCEhIWrZsqWKFi2q1atXa+rUqerQoYPs7OxUvXp1rVmzRvny5bP1pQAAAAAAAAAAIEny9PSUJEvQguzDw8PD8vo8LJuHLPb29ho6dKiGDh2ablupUqV06tQpq7by5ctrwYIFj6s8AAAAAAAAAAAemMlkUvHixVW0aFElJSXZuhz8f46OjlkygiWNzUMWAAAAAAAAAACeVPb29ln6oT6yFztbFwAAAAAAAAAAAJATEbIAAAAAAAAAAAAYQMgCAAAAAAAAAABgACELAAAAAAAAAACAAYQsAAAAAAAAAAAABhCyAAAAAAAAAAAAGEDIAgAAAAAAAAAAYAAhCwAAAAAAAAAAgAGELAAAAAAAAAAAAAYQsgAAAAAAAAAAABhAyAIAAAAAAAAAAGAAIQsAAAAAAAAAAIABhCwAAAAAAAAAAAAGELIAAAAAAAAAAAAYQMgCAAAAAAAAAABggIOtCwCQfThVLGPrEgAANsL/AQAAAAAAPDhCFgCSJHNKioot+MDWZQAAbMickiKTvb2tywAAAAAAIMcgZAEgSTLZ2+uzk/v0d9w/ti4FAGADhd3yqoV3kK3LAAAAAAAgRyFkAWDxW9R5nY+9ausyAAA2UDpfIUIWAAAAAAAeEAvfAwAAAAAAAAAAGEDIAgAAAAAAAAAAYAAhCwAAAAAAAAAAgAGELAAAAAAAAAAAAAYQsgAAAAAAAAAAABhAyAIAAAAAAAAAAGAAIQsAAAAAAAAAAIABhCwAAAAAAAAAAAAGELIAAAAAAAAAAAAYQMgCAAAAAAAAAABgACELAAAAAAAAAACAAYQsAAAAAAAAAAAABhCyAAAAAAAAAAAAGEDIAgAAAAAAAAAAYAAhCwAAAAAAAAAAgAEOD7Pz9evXdeDAAUVFRalx48aKiYmRl5eXTCZTVtUHAAAAAAAAAACQLRkOWebPn6+FCxfq1q1bMplM8vf3V2hoqGJiYhQeHq58+fJlZZ0AAAAAAAAAAADZiqHpwlauXKk5c+aoU6dOWr9+vcxmsySpQ4cOOn/+vGbNmpWlRQIAAAAAAAAAAGQ3hkKWTz75RN27d1f//v3l6+traQ8ODtaAAQO0ffv2LCsQAAAAAAAAAAAgOzIUsly6dElBQUEZbitXrpz+/vvvhyoKAAAAAAAAAAAguzMUshQvXlyHDh3KcNtvv/2m4sWLP1RRAAAAAAAAAAAA2Z2hhe9btWqlOXPmyMXFRfXr15ckxcXF6dtvv9XChQvVqVOnrKwRAAAAAAAAAAAg2zEUsnTr1k0XLlzQtGnTNG3aNElS+/btJUmvvPKKevTokXUVAgAAAAAAAAAAZEOGQhaTyaTx48erU6dO2rNnj65fv668efMqKChIFStWzOoaAQAAAAAAAAAAsh1DIUsaLy8veXl5ZVUtAAAAAAAAAAAAOYahkKVdu3YymUwZbrOzs5Obm5vKlCmjN954Q+XKlXuoAgEAAAAAAAAAALIjOyM7lS5dWocPH9ahQ4ckSUWKFJGdnZ2OHj2q/fv3Kzo6Wl999ZVef/11HT9+PEsLBgAAAAAAAAAAyA4MhSxFihRRiRIl9O2332rFihWaPn26li1bpq1bt6pChQqqW7euduzYoRo1amjmzJn/eqzU1FTNnj1bwcHBqlKlijp37qxz587ds39SUpKmT5+u4OBgVa1aVW3bttWJEyeMXAYAAAAAAAAAAIBhhkKWTZs2qX///ipRooRVe5EiRdSrVy+tXr1a9vb2at26tY4cOfKvx5o3b57Wrl2riRMnat26dTKZTOrWrZsSExMz7D927Fht3LhREyZM0KZNm+Th4aFu3brpn3/+MXIpAAAAAAAAAAAAhhgKWeLj4+Xo6JjhNpPJpJs3b0qS3Nzc7hmWSFJiYqLCw8PVt29f1atXT97e3goNDVVkZKS2bt2arv/58+e1ceNGhYSEqH79+ipfvrwmTZokJycn/fbbb0YuBQAAAAAAAAAAwBBDIUu1atU0a9YsXblyxar96tWrmjt3rgICAiRJ+/bt01NPPXXP45w8eVI3b95UzZo1LW358uWTj4+P9u/fn67/Tz/9pHz58qlu3bpW/bdv365atWoZuRQAAAAAAAAAAABDHIzsNHLkSL3zzjtq1KiRAgICVLBgQUVHR+vQoUPKkyePZsyYoV27dmnu3LkaO3bsPY8TEREhSSpevLhVe9GiRXX58uV0/f/73/+qdOnS+u6777Ro0SJFRkbKx8dHI0aMUPny5Y1ciiTJbDYrLi7O8P5ATmcymeTq6mrrMgAA2UB8fLzMZrOtywAA2Aj3BgCANNwbIDczm80ymUyZ6msoZClXrpy2bNmiFStWaO/evfr999/l6empbt26qX379sqbN69u3ryp0NBQNWnS5J7HiY+PlyQ5OTlZtTs7O+v69evp+t+4cUP/+9//NG/ePA0bNkz58uXT/Pnz1aZNG23ZskWFChUycjlKSkrSiRMnDO0LPAlcXV3l4+Nj6zIAANnA2bNnLb+jAQByH+4NAABpuDdAbnd3bnEvhkIWSSpQoID69+9/z+3+/v7y9/f/12O4uLhIur02S9rPkpSQkJDhN2ccHR31zz//KDQ01DJyJTQ0VPXq1dOnn36qrl27GrkUOTo6qkKFCob2BZ4EmU1lAQBPPi8vL76tBgC5GPcGAIA03BsgNzt9+nSm+xoOWQ4fPqx9+/YpKSnJ8pctbdqtgwcPav369fc9Rto0YVFRUVZrt0RFRcnb2ztdf09PTzk4OFhNDebi4qLSpUvrwoULRi9FJpNJbm5uhvcHAAB4UjBFDAAAAACJewPkbg/yxRNDIcuqVas0ceLEDJNMOzs71alTJ1PH8fb2lru7u/bu3WsJWWJjY3X8+HG1bds2Xf/q1asrOTlZx44dk5+fnyTp1q1bOn/+vJo1a2bkUgAAAAAAAAAAAAyxM7LTypUrVadOHe3du1ddunTRm2++qcOHD2vWrFlydnZW8+bNM3UcJycntW3bVtOmTdP333+vkydPauDAgfL09FSjRo2UkpKiK1eu6NatW5Juhyy1a9fW8OHDdeDAAZ0+fVrDhg2Tvb29Xn31VSOXAgAAAAAAAAAAYIihkOXChQtq27at8ufPLz8/Px08eFAuLi5q3LixevTooRUrVmT6WP369VOrVq30/vvv6+2335a9vb2WLFkiJycnXb58WXXq1NGWLVss/efMmaOgoCD16dNHrVq10o0bN7RixQoVLFjQyKUAAAAAAAAAAAAYYmi6MEdHR8tC9WXLltW5c+eUlJQkR0dHVatWTeHh4Zk+lr29vYYOHaqhQ4em21aqVCmdOnXKqs3d3V1jx47V2LFjjZQOAAAAAAAAAACQJQyNZHnmmWf0ww8/SJLKlCmj1NRUHT58WJIUERGRZcUBAAAAAAAAAABkV4ZGsnTq1El9+vTR9evXFRISooYNG2rYsGFq3LixvvzySz377LNZXScAAAAAAAAAAEC2YmgkywsvvKAFCxaoQoUKkqTx48fLy8tLa9euVbly5TR69OgsLRIAAAAAAAAAACC7MTSSRZLq16+v+vXrS5IKFChgtQ4LU4YBAAAAAAAAAIAnneE1WY4ePZrhtgMHDuill156qKIAAAAAAAAAAACyu0yPZAkPD1dcXJwkyWw2a8OGDdq1a1e6focOHZKTk1PWVQgAAAAAAAAAAJANZTpkSUxMVFhYmCTJZDJpw4YN6frY2dkpb968evfdd7OuQgAAAAAAAAAAgGwo0yFLz5491bNnT0mSt7e31q9fL39//0dWGAAAAAAAAAAAQHZmaOH7kydPZnUdAAAAAAAAAAAAOYqhkEWSfv75Z/3www+Kj49Xamqq1TaTyaRJkyY9dHEAAAAAAAAAAADZlaGQZfHixZo2bZqcnZ1VsGBBmUwmq+13PwYAAAAAAAAAAHjSGApZVq1apVdeeUUffvihnJycsromAAAAAAAAAACAbM/OyE5Xr15Vq1atCFgAAAAAAAAAAECuZShk8fHx0Z9//pnVtQAAAAAAAAAAAOQYhqYLe++99zRgwAC5ubmpSpUqcnV1TdenRIkSD10cAAAAAAAAAABAdmUoZHn77beVmpqq9957756L3J84ceKhCgMAAAAAAAAAAMjODIUsEydOzOo6AAAAAAAAAAAAchRDIctrr72W1XUAAAAAAAAAAADkKIZCFklKTEzUxo0btXv3bl25ckWTJk3Svn375OvrK39//6ysEQAAAAAAAAAAINuxM7JTdHS0Xn/9dX344Yc6d+6cjh49qlu3bmnnzp1q166dDh06lNV1AgAAAAAAAAAAZCuGQpaPPvpIN2/e1JYtW/Tpp5/KbDZLkmbNmiU/Pz/Nnj07S4sEAAAAAAAAAADIbgyFLD/88IP69++vMmXKyGQyWdqdnZ3VuXNn/f7771lWIAAAAAAAAAAAQHZkKGRJSEiQh4dHhtvs7e2VlJT0MDUBAAAAAAAAAABke4ZCFj8/P61evTrDbV9++aUqV678UEUBAAAAAAAAAABkdw5Gdurfv786duyoV199VfXq1ZPJZNJXX32lOXPm6KefftLixYuzuk4AAAAAAAAAAIBsxdBIlurVq2vp0qVydXXV4sWLZTabtWzZMl25ckULFy5UzZo1s7pOAAAAAAAAAACAbMXQSBZJCgwM1Nq1a3Xr1i1dv35d7u7ucnV1lZ2dodwGAAAAAAAAAAAgRzGciMyfP19dunSRi4uLihUrpt9++03PPfecli1bloXlAQAAAAAAAAAAZE+GQpbFixcrLCxMTz/9tKWtTJkyevXVVzV9+nStW7cuywoEAAAAAAAAAADIjgxNF7Z+/XoNHDhQXbt2tbR5enpqxIgRKliwoFasWKG33nory4oEAAAAAAAAAADIbgyNZImMjJSvr2+G2/z8/HThwoWHKgoAAAAAAAAAACC7MxSylC5dWrt3785w2969e+Xp6flQRQEAAAAAAAAAAGR3hqYLe/vttzVp0iQlJyfrhRdeUKFChRQdHa1t27ZpxYoVGjJkSFbXCQAAAAAAAAAAkK0YClneeecdRUREaOnSpVq2bJml3d7eXh06dFDHjh2zqDwAAAAAAAAAAIDsyVDIcv36dQ0ePFjdu3fX4cOHFRMTo3z58snf318FChTI6hoBAAAAAAAAAACyHUMhyxtvvKEBAwaoadOmCg4OzuqaAAAAAAAAAAAAsj1DC99fv36dESsAAAAAAAAAACBXMxSytG/fXh999JH27Nmj6OjorK4JAAAAAAAAAAAg2zM0Xdjnn3+uS5cuqVOnThluN5lMOn78+EMVBgAAAAAAAAAAkJ0ZClmaN2+e1XUAAAAAAAAAAADkKIZClj59+mR1HQAAAAAAAAAAADmKoZAlzc6dO7V7925duXJFAwcO1IkTJ+Tr66uSJUtmVX0AAAAAAAAAAADZkqGQJT4+Xr1799bu3bvl7u6umzdvqkuXLlqzZo2OHz+ulStXqmLFilldKwAAAAAAAAAAQLZhZ2SnGTNm6Pfff9eyZcu0Z88emc1mSdJHH32kYsWKadasWVlaJAAAAAAAAAAAQHZjKGT5+uuvNWjQINWsWVMmk8nSXqRIEb377rs6ePBglhUIAAAAAAAAAACQHRkKWWJjY++57kr+/PkVFxf3UEUBAAAAAAAAAABkd4ZClooVK+rLL7/McNv27dtZjwUAAAAAAAAAADzxDIUs7777rj7//HP16NFDGzZskMlk0v79+zVhwgStWbNGXbt2zfSxUlNTNXv2bAUHB6tKlSrq3Lmzzp07l6l9v/zyS1WqVEkXLlwwchkAAAAAAAAAAACGGQpZXnjhBU2dOlWnTp3S2LFjZTabNXnyZH3zzTcaO3asmjRpkuljzZs3T2vXrtXEiRO1bt06mUwmdevWTYmJif+638WLFzVu3Dgj5QMAAAAAAAAAADw0hwfd4ejRo7p48aLKlSunHTt26MyZM4qJiVG+fPlUrlw52dllPrdJTExUeHi4hg4dqnr16kmSQkNDFRwcrK1bt6pZs2YZ7peamqqhQ4fK19dXe/bsedBLAAAAAAAAAAAAeGiZDlliY2PVo0cPHT58WGazWSaTSVWrVtWMGTNUrlw5Qyc/efKkbt68qZo1a1ra8uXLJx8fH+3fv/+eIcuCBQuUlJSkPn36ELIAAAAAAAAAAACbyHTIMnPmTB0/flx9+/ZV5cqVdebMGS1YsECjR4/W4sWLDZ08IiJCklS8eHGr9qJFi+ry5csZ7nP06FGFh4dr48aNioyMNHTeu5nNZsXFxWXJsYCcyGQyydXV1dZlAACygfj4eJnNZluXAQCwEe4NAABpuDdAbpY20CQzMh2y/PDDDxo0aJA6dOggSapbt66KFSumIUOGKC4uTm5ubg9caHx8vCTJycnJqt3Z2VnXr19P1z8uLk5DhgzRkCFDVLZs2SwLWZKSknTixIksORaQE7m6usrHx8fWZQAAsoGzZ89afkcDAOQ+3BsAANJwb4Dc7u7c4l4yHbJcuXJFvr6+Vm01atRQSkqKLl++rPLlyz9YhZJcXFwk3V6bJe1nSUpISMjwmzMTJ05U2bJl1bp16wc+179xdHRUhQoVsvSYQE6S2VQWAPDk8/Ly4ttqAJCLcW8AAEjDvQFys9OnT2e6b6ZDluTk5HTJTf78+SXdDkWMSJsmLCoqSk899ZSlPSoqSt7e3un6b9q0SU5OTgoICJAkpaSkSJJefvllNW/eXOPHjzdUh8lkMjQSBwAA4EnDFDEAAAAAJO4NkLs9yBdPMh2y/Bujiaa3t7fc3d21d+9eS8gSGxur48ePq23btun6f/fdd1aPjxw5oqFDh2rRokWGRtIAAAAAAAAAAAAYlSUhi9HhxE5OTmrbtq2mTZumggULqmTJkpo6dao8PT3VqFEjpaSkKDo6Wnnz5pWLi4vKlCljtX9ERIQkqUSJEipUqNBDXwcAAAAAAAAAAEBmPVDIMnbsWLm7u1sep41gGT16tPLkyWNpN5lMWr58eaaO2a9fPyUnJ+v999/XrVu3FBgYqCVLlsjJyUkXLlxQw4YNFRISopYtWz5IqQAAAAAAAAAAAI9UpkOWwMBASemnBsuo/UGmD7O3t9fQoUM1dOjQdNtKlSqlU6dO3XPfGjVq/Ot2AAAAAAAAAACARyXTIcsnn3zyKOsAAAAAAAAAAADIUexsXQAAAAAAAAAAAEBORMgCAAAAAAAAAABgACELAAAAAAAAAACAAYQsAAAAAAAAAAAABhCyAAAAAAAAAAAAGEDIAgAAAAAAAAAAYAAhCwAAAAAAAAAAgAGELAAAAAAAAAAAAAYQsgAAAAAAAAAAABhAyAIAAAAAAAAAAGAAIQsAAAAAAAAAAIABhCwAAAAAAAAAAAAGELIAAAAAAAAAAAAYQMgCAAAAAAAAAABgACELAAAAAAAAAACAAYQsAAAAAAAAAAAABhCyAAAAAAAAAAAAGEDIAgAAAAAAAAAAYAAhCwAAAAAAAAAAgAGELAAAAAAAAAAAAAYQsgAAAAAAAAAAABhAyAIAAAAAAAAAAGAAIQsAAAAAAAAAAIABhCwAAAAAAAAAAAAGELIAAAAAAAAAAAAYQMgCAAAAAAAAAABgACELAAAAAAAAAACAAYQsAAAAAAAAAAAABhCyAAAAAAAAAAAAGEDIAgAAAAAAAAAAYAAhCwAAAAAAAAAAgAGELAAAAAAAAAAAAAYQsgAAAAAAAAAAABhAyAIAAAAAAAAAAGAAIQsAAAAAAAAAAIABhCwAAAAAAAAAAAAGELIAAAAAAAAAAAAYQMgCAAAAAAAAAABgACELAAAAAAAAAACAAYQsAAAAAAAAAAAABhCyAAAAAAAAAAAAGEDIAgAAAAAAAAAAYAAhCwAAAAAAAAAAgAGELAAAAAAAAAAAAAYQsgAAAAAAAAAAABhg85AlNTVVs2fPVnBwsKpUqaLOnTvr3Llz9+z/559/qnv37qpRo4Zq1aqlfv366dKlS4+xYgAAAAAAAAAAgGwQssybN09r167VxIkTtW7dOplMJnXr1k2JiYnp+l67dk2dOnVSnjx5tHLlSn388ce6du2aunbtqoSEBBtUDwAAAAAAAAAAciubhiyJiYkKDw9X3759Va9ePXl7eys0NFSRkZHaunVruv7btm1TfHy8Jk+erIoVK6py5cqaOnWq/vrrL/366682uAIAAAAAAAAAAJBb2TRkOXnypG7evKmaNWta2vLlyycfHx/t378/Xf9atWpp7ty5cnZ2Trft+vXrj7RWAAAAAAAAAACAOznY8uQRERGSpOLFi1u1Fy1aVJcvX07Xv1SpUipVqpRV28KFC+Xs7KzAwMBHVygAAAAAAAAAAMBdbBqyxMfHS5KcnJys2p2dnTM1MmXFihVavXq1Ro4cqUKFChmuw2w2Ky4uzvD+QE5nMpnk6upq6zIAANlAfHy8zGazrcsAANgI9wYAgDTcGyA3M5vNMplMmepr05DFxcVF0u21WdJ+lqSEhIR//aXObDZr1qxZmj9/vnr06KGOHTs+VB1JSUk6ceLEQx0DyMlcXV3l4+Nj6zIAANnA2bNnLV+EAQDkPtwbAADScG+A3O7uwSH3YtOQJW2asKioKD311FOW9qioKHl7e2e4T1JSkkaOHKmvvvpKw4YNU5cuXR66DkdHR1WoUOGhjwPkVJlNZQEATz4vLy++rQYAuRj3BgCANNwbIDc7ffp0pvvaNGTx9vaWu7u79u7dawlZYmNjdfz4cbVt2zbDfYYNG6atW7dq+vTpatasWZbUYTKZ5ObmliXHAgAAyMmYIgYAAACAxL0BcrcH+eKJTUMWJycntW3bVtOmTVPBggVVsmRJTZ06VZ6enmrUqJFSUlIUHR2tvHnzysXFRZs3b9aWLVs0bNgwBQUF6cqVK5ZjpfUBAAAAAAAAAAB4HOxsXUC/fv3UqlUrvf/++3r77bdlb2+vJUuWyMnJSZcvX1adOnW0ZcsWSdJXX30lSfroo49Up04dqz9pfQAAAAAAAAAAAB4Hm45kkSR7e3sNHTpUQ4cOTbetVKlSOnXqlOVxeHj44ywNAAAAAAAAAADgnmw+kgUAAAAAAAAAACAnImQBAAAAAAAAAAAwgJAFAAAAAAAAAADAAEIWAAAAAAAAAAAAAwhZAAAAAAAAAAAADCBkAQAAAAAAAAAAMICQBQAAAAAAAAAAwABCFgAAAAAAAAAAAAMIWQAAAAAAAAAAAAwgZAEAAAAAAAAAADCAkAUAAAAAAAAAAMAAQhYAAAAAAAAAAAADCFkAAAAAAAAAAAAMIGQBAAAAAAAAAAAwgJAFAAAAAAAAAADAAEIWAAAAAAAAAAAAAwhZAAAAAAAAAAAADCBkAQAAAAAAAAAAMICQBQAAAAAAAAAAwABCFgAAAAAAAAAAAAMIWQAAAAAAAAAAAAwgZAEAAAAAAAAAADCAkAUAAAAAAAAAAMAAQhYAAAAAAAAAAAADCFkAAAAAAAAAAAAMIGQBAAAAAAAAAAAwgJAFAAAAAAAAAADAAEIWAAAAAAAAAAAAAwhZAAAAAAAAAAAADCBkAQAAAAAAAAAAMICQBQAAAAAAAAAAwABCFgAAAAAAAAAAAAMIWQAAAAAAAAAAAAwgZAEAAAAAAAAAADCAkAUAAAAAAAAAAMAAQhYAAAAAAAAAAAADCFkAAAAAAAAAAAAMIGQBAAAAAAAAAAAwgJAFAAAAAAAAAADAAEIWAAAAAAAAAAAAAwhZAAAAAAAAAAAADCBkAQAAAAAAAAAAMICQBQAAAAAAAAAAwABCFgAAAAAAAAAAAAMIWQAAAAAAAAAAAAwgZAEAAAAAAAAAADCAkAUAAAAAAAAAAMAAm4csqampmj17toKDg1WlShV17txZ586du2f/a9euafDgwQoMDFRgYKBGjx6tuLi4x1gxAAAAAAAAAABANghZ5s2bp7Vr12rixIlat26dTCaTunXrpsTExAz79+vXT+fPn9eyZcs0e/Zs/fzzzxo3btxjrhoAAAAAAAAAAOR2Ng1ZEhMTFR4err59+6pevXry9vZWaGioIiMjtXXr1nT9Dx06pH379ikkJES+vr6qVauWxo8fr88//1yRkZE2uAIAAAAAAAAAAJBb2TRkOXnypG7evKmaNWta2vLlyycfHx/t378/Xf8DBw6oSJEiKl++vKUtKChIJpNJBw8efCw1AwAAAAAAAAAASJKDLU8eEREhSSpevLhVe9GiRXX58uV0/SMjI9P1dXJykoeHR4b9MyMpKUlms1lHjx41tD/wpDCZTGqQ5ymluJa0dSkAABuwt7PXsWPHZDabbV0KAMDGuDcAgNyNewPgdm5gMpky1demIUt8fLyk20HJnZydnXX9+vUM+9/dN61/QkKCoRrSnqjMPmHAkyyvk4utSwAA2Bi/EwEAJO4NAADcGyB3M5lMOSNkcXG5/UtbYmKi5WdJSkhIkKura4b9ExMT07UnJCTIzc3NUA0BAQGG9gMAAAAAAAAAALmbTddkSZv6Kyoqyqo9KipKnp6e6fp7enqm65uYmKiYmBgVK1bs0RUKAAAAAAAAAABwF5uGLN7e3nJ3d9fevXstbbGxsTp+/LiqV6+ern9gYKAiIiJ07tw5S1vavtWqVXv0BQMAAAAAAAAAAPx/Np0uzMnJSW3bttW0adNUsGBBlSxZUlOnTpWnp6caNWqklJQURUdHK2/evHJxcVGVKlVUrVo1DRw4UGPHjlVcXJzGjBmjFi1aMJIFAAAAAAAAAAA8Viaz2Wy2ZQEpKSmaMWOGNm/erFu3bikwMFAffPCBSpUqpQsXLqhhw4YKCQlRy5YtJUlXr17VuHHj9OOPP8rZ2VlNmjTRyJEj5ezsbMvLAAAAAAAAAAAAuYzNQxYAAAAAAAAAAICcyKZrsgAAAAAAAAAAAORUhCwAAAAAAAAAAAAGELIAAAAAAAAAAAAYQMgCAAAAAAAAAABgACELAAAAAAAAAACAAYQsAAAAAAAAAAAABhCyAAAAAAAAAAAAGEDIAgAAAAAAAAAAYAAhCwAAAAAAAAAAgAGELAAAAAAAAAAAAAYQsgAAcr1Lly7pxx9/1K1bt3T16lVblwMAAADARvbv36+1a9fqxo0bOn36tJKSkmxdEgAgm3OwdQEAANhKYmKihg8frq+//lp2dnb69ttvNWXKFP3zzz8KCwtT3rx5bV0iAAAAgMfgxo0b6tq1qw4fPiyTyaTnnntO06ZN03//+18tW7ZMnp6eti4RAJBNMZIFAJBrzZ8/XydPntTy5cvl7OwsSWrfvr0uXryoqVOn2rg6AAAAAI/LjBkzJElbt26Vi4uLJGnYsGFyc3PTRx99ZMvSAADZHCELACDX+s9//qPRo0erRo0alragoCBNmDBB27dvt2FlAAAAAB6nH374QcOGDVPp0qUtbeXKldOYMWP0yy+/2LAyAEB2R8gCAMi1IiMj9dRTT6VrL168uGJjY21QEQAAAABbiI6OVpEiRdK1u7u7Kz4+3gYVAQByCkIWAECuVb58ee3evTtd+1dffaUKFSrYoCIAAAAAtuDn56ctW7aka1+xYoV8fHxsUBEAIKdg4XsAQK7Vt29fDRgwQH/88YdSUlL06aef6syZM/ruu+8UGhpq6/IAAAAAPCaDBg1Sp06ddOjQISUnJ2v+/Pk6ffq0jh8/riVLlti6PABANmYym81mWxcBAICt7Nq1SwsXLtTx48eVmpqqihUrqlu3bmrcuLGtSwMAAADwGJ08eVLh4eFW9wadO3dWlSpVbF0aACAbI2QBAAAAAAAAAAAwgOnCAAC5ymeffZbpvi1atHhkdQAAAACwrbCwsEz37dOnzyOsBACQkzGSBQCQq3h7e2eqn8lk0okTJx5xNQAAAABspUGDBpnqZzKZ9P333z/iagAAORUhCwAAAAAAAAAAgAF2ti4AAIDs6NKlS7YuAQAAAICNJSYm6sCBA7YuAwCQjbEmCwAg17pw4YKmTJmiU6dOKSUlRZJkNpuVmJio6OhoHT9+3MYVAgAAAHgcjh8/rvfff1+nTp1Sampquu1MJQwAuBdGsgAAcq2JEyfqjz/+0EsvvaTIyEg1a9ZMvr6++vvvvzV27FhblwcAAADgMQkJCZGDg4PGjBkjR0dHjR49Wh06dJCDg4NmzJhh6/IAANkYI1kAALnWgQMHNH/+fAUGBmrXrl164YUX5O/vr9DQUO3cuVNvvvmmrUsEAAAA8Bj89ttvWr58ufz9/bVp0yY9/fTTatOmjTw9PbV+/Xq99NJLti4RAJBNMZIFAJBrJSQkqFSpUpKkcuXK6dSpU5KkFi1a6MiRI7YsDQAAAMBjlJqaqiJFikiSvLy89Mcff0iSGjZsqJMnT9qyNABANkfIAgDItUqXLm25eSpbtqxlnuXU1FTdvHnTlqUBAAAAeIzKlSun/fv3S5LKlCmjY8eOSZL++ecfJSYm2rI0AEA2x3RhAIBcq2XLlho2bJgmT56sevXqqV27dipRooR+/vlnVapUydblAQAAAHhM2rZtq1GjRkmSXnzxRb366qtycXHRr7/+qipVqti4OgBAdkbIAgDItbp27SoHBweZTCb5+/urT58+mj9/vooXL66PPvrI1uUBAAAAeExef/115c+fXx4eHipfvrw++ugjLViwQCVKlNDo0aNtXR4AIBszmc1ms62LAAAAAAAAAB6nsLCwTPft06fPI6wEAJCTEbIAAHK1nTt36o8//lBCQkK6bdxIAQAAAE8ub29v2dnZydPT81/7mUwmff/994+pKgBATsN0YQCAXGvixIlauXKlChcuLCcnJ6ttJpOJkAUAAAB4gr355pvaunWrJKlZs2Zq1qyZvL29bVwVACCnYSQLACDXqlGjhgYNGqS33nrL1qUAAAAAsIGUlBTt2bNHW7Zs0bZt21SwYEG9/PLLatasmcqWLWvr8gAAOQAhCwAg13ruuee0cuVKeXl52boUAAAAADaWlJSkn376SV9//bW+//57PfXUU2ratKmaNWumEiVK2Lo8AEA2RcgCAMi15s+fr7Nnz2rixInppgsDAAAAkHslJiZqw4YNCg0N1c2bN3XixAlblwQAyKZYkwUAkGu99NJLeuutt/Tss8+qSJEiMplMVttZ3BIAAADIXSIjI/X111/rm2++0ZEjR1SmTBm1a9fO1mUBALIxQhYAQK41YsQI5cuXT61atZKrq6utywEAAABgA3cHK6VLl9ZLL72ksWPHytvb29blAQCyOaYLAwDkWv7+/lq/fj03TgAAAEAutGzZMn3zzTc6evSoSpQooZdeeklNmjSRr6+vrUsDAOQghCwAgFyrWbNmCgkJkb+/v61LAQAAAPCYeXt7y9HRUbVr15afn9+/9u3Tp89jqgoAkNMQsgAAcq29e/dqypQp6t+/v7y8vOTgYD2LZokSJWxUGQAAAIBHrUGDBpnqZzKZWK8RAHBPhCwAgFzL19dXKSkpkmS16L3ZbJbJZNKJEydsVRoAAAAAAAByABa+BwDkWkuXLrV1CQAAAAAAAMjBGMkCAICkxMREOTk52boMAAAAAAAA5CB2ti4AAABbWrNmjRo0aKCqVavq/PnzGjNmjMLCwmxdFgAAAAAAAHIAQhYAQK715Zdfavr06Xrttdfk6OgoSSpfvrwWLVqkjz/+2MbVAQAAAAAAILsjZAEA5Frh4eEaNWqU+vbtKzu72/8ltm/fXuPGjdOGDRtsXB0AAAAAAACyO0IWAECudfbsWVWvXj1de/Xq1RUREWGDigAAAAAAAJCTELIAAHKtwoUL68yZM+naf/31VxUtWtQGFQEAAAAAACAnIWQBAORab731lsaNG6fvvvtOknTmzBmtXr1akyZN0uuvv27j6gAAAAAAAJDdmcxms9nWRQAAYCszZszQ8uXLlZCQIElycHBQ69at9d5771nWaQEAAAAAAAAyQsgCAMhVQkJC1L9/f7m5uVna4uPjdfr0aZnNZpUrV07u7u42rBAAAAAAAAA5BV/RBQDkKitWrFB8fLxVW58+fVSsWDH5+/sTsAAAAAAAACDTCFkAALlKRgM4f/31V8t0YQAAAAAAAEBmEbIAAAAAAAAAAAAYQMgCAAAAAAAAAABgACELACDXMZlMti4BAAAAAAAATwAHWxcAAMDjNnHiRDk7O1seJyUlaerUqcqTJ49Vv5CQkMddGgAAAAAAAHIQQhYAQK4SGBioK1euWLUFBATo2rVrunbtmo2qAgAAAAAAQE5kMpvNZlsXAQAAAAAAAAAAkNOwJgsAAAAAAAAAAIABhCwAAAAAAAAAAAAGELIAAAAAAAAAAAAYQMgCAAAAIMf466+/NGHCBDVu3FhVqlTRs88+q7feekurVq1SUlLSY6ujUqVKmjNnzmM7HwAAAIDsycHWBQAAAABAZmzZskUjR45UuXLl1KlTJ3l5eenWrVvauXOnQkJCtGvXLi1YsEAmk8nWpQIAAADIJQhZAAAAAGR7f/31l0aOHKnatWtrzpw5cnD4v1uZevXqqUaNGurXr5/+85//6OWXX7ZhpQAAAAByE6YLAwAAAJDtLV68WHZ2dpo4caJVwJKmcePGatGihezsbt/ipKamatGiRWrUqJEqV66sxo0b65NPPrHap127dho1apQWLVqk+vXry8/PT61bt9aRI0es+u3bt09vvfWWqlSposaNG2v37t3pzp+QkKCPPvpI9erVU+XKlfXKK69oy5YtVn0aNGigSZMmqUOHDqpWrZo++OCDh31aAAAAANgYI1kAAAAAZHvff/+9atasqUKFCt2zz5QpUyw/jx07Vps3b1aPHj0UEBCg/fv3a9KkSYqNjVXv3r0t/b799luVL19e77//vsxms6ZMmaJ+/fpp+/btsre31++//67OnTurRo0amjVrli5duqRBgwZZnddsNqt379769ddf1a9fP5UvX15bt27VwIEDlZiYqBYtWlj6rlq1Su+88466d+8uFxeXrHuCAAAAANgEIQsAAACAbO369eu6fv26ypYtm25bcnKy1WOTyaT//e9/Wr9+vQYNGqTu3btLkurUqSOTyaSFCxeqTZs2KlCggGX/JUuWyN3dXZJ08+ZNDR8+XCdOnFDlypW1cOFCFSxYUPPnz5eTk5MkycPDQwMHDrScc/fu3frxxx8VGhqqpk2bSpKCg4MVHx+vadOm6eWXX7aMvilatKhGjBhhGXEDAAAAIGfjN3sAAAAA2VpqamqG7efOnZOvr6/Vn0aNGmnPnj0ym81q0KCBkpOTLX8aNGighIQEHTx40HKMChUqWAIWSSpWrJgkKT4+XpJ08OBBBQcHWwIWSXrxxRdlb29vefzLL7/IZDKpXr166c535coV/fnnn5a+5cuXJ2ABAAAAniCMZAEAAACQrRUoUEBubm66ePGiVXvx4sW1ceNGy+O5c+fqjz/+UExMjCSpWbNmGR4vMjLS8rOrq6vVtjvXdJFuj6IpWLCgVR8HBwfLSBhJiomJkdlsVrVq1TI8X1RUlJ555hlJUuHChe95nQAAAAByHkIWAAAAANlew4YNtX37dt24ccMy8sTJyUl+fn6WPh4eHpKkfPnySZKWL1+uPHnypDtWiRIlMn1eDw8P/f3331ZtZrNZ169ftzzOmzev3NzctGLFigyPUaZMmUyfDwAAAEDOwjh1AAAAANlejx49lJKSovfee0+JiYnptt+6dUvnz5+XJAUGBkqSrl27Jj8/P8ufmJgYzZw50zLSJTNq1aqlXbt2WaYPk6Qff/xRSUlJlsdBQUGKi4uT2Wy2Ot+ff/6puXPnpls3BgAAAMCTg5EsAAAAALK9ihUravr06Ro+fLhatGihN998U5UqVVJycrIOHTqkjRs36u+//1bXrl319NNPq3nz5ho9erQuXryoypUr6+zZswoNDVWpUqVUtmzZTJ+3d+/e2rZtm7p06aKuXbvq2rVrCg0NlaOjo6VPvXr1FBgYqF69eqlXr14qX768jh49qjlz5qhOnTrpphsDAAAA8OQgZAEAAACQI7zwwgv64osvtGbNGm3cuFEXL16U2WxW6dKl1bRpU7Vu3doSoISEhGjhwoVau3atIiIiVKhQITVt2lQDBgywWrT+fsqWLauVK1dq8uTJGjhwoAoVKqThw4dr8uTJlj52dnZatGiRZs2apYULF+rq1asqVqyYOnbsqN69e2f10wAAAAAgGzGZzWazrYsAAAAAAAAAAADIaViTBQAAAAAAAAAAwABCFgAAAAAAAAAAAAMIWQAAAAAAAAAAAAwgZAEAAAAAAAAAADCAkAUAAAAAAAAAAMAAQhYAAAAAAAAAAAADCFkAAAAAAAAAAAAMIGQBAAAAAAAAAAAwgJAFAAAAAAAAAADAAEIWAAAAAAAAAAAAAwhZAAAAAAAAAAAADCBkAQAAAAAAAAAAMOD/AQgaAsezbY2XAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 2000x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "custom_palette =(0.4, 0.7, 0.6), 'crimson'\n",
    "for col in cat_cols:\n",
    "    contingency_table = pd.crosstab(train_df[col], train_df[target_col], normalize='index')\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    contingency_table.plot(kind=\"bar\", stacked=True, color=custom_palette,figsize=(20, 4))\n",
    "    plt.title(f\"Percentage Distribution of Target across {col}\")\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel(\"Percentage\")\n",
    "    plt.legend(title=\"Churn Class\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_correlation_heatmap(df: pd.core.frame.DataFrame, title_name: str='Train correlation') -> None:\n",
    "\n",
    "    corr = df.corr()\n",
    "    fig, axes = plt.subplots(figsize=(12, 8))\n",
    "    mask = np.zeros_like(corr)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "    sns.heatmap(corr, mask=mask, linewidths=.5, cmap='RdBu_r', annot=True,annot_kws={\"size\": 8})\n",
    "    plt.title(title_name)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_correlation_heatmap(train_df[num_cols+[target_col]], 'Train Dataset Correlation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering\n",
    "### 5.1 Arithmetical Operations betweeen columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArithmeticFeatureCombiner(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, num_cols):\n",
    "        self.num_cols = num_cols\n",
    "        self.new_num_cols = []\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # No fitting necessary for this transformer\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Perform the arithmetic operations without creating redundant features\n",
    "        for i, col1 in enumerate(self.num_cols):\n",
    "            for col2 in self.num_cols[i+1:]:  # Start from i+1 to avoid redundancy\n",
    "                X[f'{col1}+{col2}'] = X[col1] + X[col2]\n",
    "                X[f'{col1}*{col2}'] = X[col1] * X[col2]\n",
    "                self.new_num_cols.extend([f'{col1}+{col2}', f'{col1}*{col2}'])\n",
    "        return X\n",
    "\n",
    "    def get_feature_names_out(self):\n",
    "        # Return the new column names after transformation\n",
    "        return self.num_cols + self.new_num_cols\n",
    "\n",
    "arithmetic_combiner = ArithmeticFeatureCombiner(num_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Binning(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, col, n_bins):\n",
    "        self.n_bins = n_bins\n",
    "        self.col = col\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        transformed = pd.qcut(X[self.col], self.n_bins, labels=False)\n",
    "        transformed[transformed.isna()] = 0\n",
    "        transformed_series = pd.Series(transformed, \n",
    "                                       name=f'QCut{self.n_bins}{self.col}',\n",
    "                                       index=X.index)\n",
    "        X_copy = X.copy()\n",
    "        return pd.concat([X_copy, transformed_series], axis=1)\n",
    "    \n",
    "age_binner = Binning('Age', 5)\n",
    "balance_binner = Binning('Balance',5)\n",
    "salary_binner = Binning('Salary',5)\n",
    "credit_score_binner = Binning('CreditScore',5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupByMergeTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, group_col, agg_col, agg_func):\n",
    "        self.group_col = group_col\n",
    "        self.agg_col = agg_col\n",
    "        self.agg_func = agg_func\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Create the aggregated DataFrame\n",
    "        self.agg_df_ = X.groupby(by=self.group_col).agg({self.agg_col: self.agg_func}).reset_index().rename(columns={self.agg_col: f'{self.agg_col}_agg'})\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Merge the original DataFrame with the aggregated DataFrame\n",
    "        return pd.merge(X, self.agg_df_, on=self.group_col, how='left')\n",
    "    \n",
    "agg_balance_sum = GroupByMergeTransformer(group_col='CustomerId', agg_col='Balance', agg_func='sum')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZeroTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, column):\n",
    "        self.column = column\n",
    "        self.new_column_name = column+'_is_zero'\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # No fitting necessary for this transformer\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Create a new column with the transformed values\n",
    "        X[self.new_column_name] = X[self.column].apply(lambda x: 1 if x == 0 else 0)\n",
    "        return X\n",
    "    \n",
    "# Create an instance of your custom transformer\n",
    "balance_zero_transformer = ZeroTransformer(column='Balance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCAClusteringTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns, n_components, n_clusters):\n",
    "        self.columns = columns\n",
    "        self.n_components = n_components\n",
    "        self.n_clusters = n_clusters\n",
    "        self.pca = PCA(n_components=self.n_components)\n",
    "        self.kmeans = KMeans(n_clusters=self.n_clusters, random_state=42)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X_subset = X[self.columns]\n",
    "        self.pca.fit(X_subset)\n",
    "        X_pca = self.pca.transform(X_subset)\n",
    "        self.kmeans.fit(X_pca)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_subset = X[self.columns]\n",
    "        X_pca = self.pca.transform(X_subset)\n",
    "        clusters = self.kmeans.predict(X_pca)\n",
    "        X_transformed = X.copy()\n",
    "        X_transformed['Cluster'] = clusters\n",
    "        for i in range(self.n_components):\n",
    "            X_transformed[f'PCA_{i}'] = X_pca[:, i]\n",
    "        return X_transformed\n",
    "    \n",
    "pca_clustering_transformer = PCAClusteringTransformer(\n",
    "    columns=[\"CustomerId\",\"EstimatedSalary\",\"Balance\"], n_components=2, n_clusters=3)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', MinMaxScaler().set_output(transform='pandas')),\n",
    "    ('pca_clustering', pca_clustering_transformer),\n",
    "    # ... other steps (e.g., preprocessing, classifiers, etc.)\n",
    "])\n",
    "\n",
    "pca_clustering_transformer.fit_transform(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = ColumnTransformer([\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore',sparse_output=False), cat_cols)\n",
    "], remainder='passthrough').set_output(transform='pandas')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7 Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler().set_output(transform='pandas')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_selector = SelectKBest(chi2, k = 15).set_output(transform='pandas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropColumn(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, cols=[]):\n",
    "        self.cols = cols\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        \n",
    "        return X.drop(self.cols, axis=1)\n",
    "    \n",
    "column_drop = DropColumn('CustomerId')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Correlation table \n",
    "https://www.kaggle.com/code/ashishkumarak/binary-classification-smoker-or-not-eda-xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+----------+-------------+\n",
      "|           Column 1          | Column 2 | Correlation |\n",
      "+-----------------------------+----------+-------------+\n",
      "|         CreditScore         |  Exited  |    -0.03    |\n",
      "|       Age+CreditScore       |  Exited  |     0.01    |\n",
      "| CreditScore*EstimatedSalary |  Exited  |     0.01    |\n",
      "|       CreditScore+Age       |  Exited  |     0.01    |\n",
      "| EstimatedSalary*CreditScore |  Exited  |     0.01    |\n",
      "|     Age+EstimatedSalary     |  Exited  |     0.02    |\n",
      "| CreditScore+EstimatedSalary |  Exited  |     0.02    |\n",
      "|       EstimatedSalary       |  Exited  |     0.02    |\n",
      "|     EstimatedSalary+Age     |  Exited  |     0.02    |\n",
      "| EstimatedSalary+CreditScore |  Exited  |     0.02    |\n",
      "|   Balance+EstimatedSalary   |  Exited  |     0.11    |\n",
      "|   EstimatedSalary+Balance   |  Exited  |     0.11    |\n",
      "|   Balance*EstimatedSalary   |  Exited  |     0.12    |\n",
      "|   EstimatedSalary*Balance   |  Exited  |     0.12    |\n",
      "|         Age+Balance         |  Exited  |     0.13    |\n",
      "|           Balance           |  Exited  |     0.13    |\n",
      "|     Balance*CreditScore     |  Exited  |     0.13    |\n",
      "|         Balance+Age         |  Exited  |     0.13    |\n",
      "|     Balance+CreditScore     |  Exited  |     0.13    |\n",
      "|     CreditScore*Balance     |  Exited  |     0.13    |\n",
      "|     CreditScore+Balance     |  Exited  |     0.13    |\n",
      "|     Age*EstimatedSalary     |  Exited  |     0.18    |\n",
      "|     EstimatedSalary*Age     |  Exited  |     0.18    |\n",
      "|         Age*Balance         |  Exited  |     0.20    |\n",
      "|         Balance*Age         |  Exited  |     0.20    |\n",
      "|       Age*CreditScore       |  Exited  |     0.29    |\n",
      "|       CreditScore*Age       |  Exited  |     0.29    |\n",
      "|             Age             |  Exited  |     0.34    |\n",
      "+-----------------------------+----------+-------------+\n"
     ]
    }
   ],
   "source": [
    "correlation_matrix = train_df[num_cols+[target_col]].corr()\n",
    "\n",
    "table = PrettyTable()\n",
    "table.field_names = [\"Column 1\", \"Column 2\", \"Correlation\"]\n",
    "\n",
    "for col1 in num_cols:\n",
    "    correlation = correlation_matrix.at[col1, target_col]\n",
    "    table.add_row([col1, target_col, f\"{correlation:.2f}\"])\n",
    "table.sortby = \"Correlation\"\n",
    "\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model setup with pre-defined params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_cat = {'learning_rate': 0.01505677476702907, \n",
    "               'depth': 10, \n",
    "               'subsample': 0.2716781821240621, \n",
    "               'random_strength': 4.83060093374094e-05, \n",
    "               'l2_leaf_reg': 0.00020833337972882073, \n",
    "               'model_size_reg': 4.043233260462883}\n",
    "\n",
    "best_params_xgb = {'learning_rate': 0.0995227238765534, \n",
    "                   'max_depth': 6, \n",
    "                   'subsample': 0.8974781754027489, \n",
    "                   'colsample_bytree': 0.41327513349222667, \n",
    "                   'min_child_weight': 11}\n",
    "\n",
    "best_params_lgbm = {'learning_rate': 0.0998967279943594, \n",
    "                    'max_depth': 7, \n",
    "                    'subsample': 0.5231345460679563, \n",
    "                    'colsample_bytree': 0.7963988397680257, \n",
    "                    'min_child_weight': 9}\n",
    "\n",
    "best_params_hist = {'learning_rate': 0.02914386061886033, \n",
    "                    'max_iter': 998, \n",
    "                    'max_leaf_nodes': 48, \n",
    "                    'min_samples_leaf': 25, \n",
    "                    'l2_regularization': 2.45430963795829e-09}\n",
    "\n",
    "catboost_model = CatBoostClassifier(**best_params_cat, silent=True)\n",
    "xgboost_model = XGBClassifier(**best_params_xgb)\n",
    "lgbm_model = LGBMClassifier(**best_params_lgbm)\n",
    "hist_model = HistGradientBoostingClassifier(**best_params_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Pipeline setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-82 {color: black;}#sk-container-id-82 pre{padding: 0;}#sk-container-id-82 div.sk-toggleable {background-color: white;}#sk-container-id-82 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-82 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-82 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-82 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-82 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-82 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-82 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-82 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-82 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-82 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-82 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-82 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-82 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-82 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-82 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-82 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-82 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-82 div.sk-item {position: relative;z-index: 1;}#sk-container-id-82 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-82 div.sk-item::before, #sk-container-id-82 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-82 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-82 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-82 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-82 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-82 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-82 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-82 div.sk-label-container {text-align: center;}#sk-container-id-82 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-82 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-82\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;agg_balance_sum&#x27;,\n",
       "                 GroupByMergeTransformer(agg_col=&#x27;Balance&#x27;, agg_func=&#x27;sum&#x27;,\n",
       "                                         group_col=&#x27;CustomerId&#x27;)),\n",
       "                (&#x27;balance_zero_transformer&#x27;, ZeroTransformer(column=&#x27;Balance&#x27;)),\n",
       "                (&#x27;age_binning&#x27;, AgeBinning(n_bins=5)),\n",
       "                (&#x27;encoder&#x27;,\n",
       "                 ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                                   transformers=[(&#x27;encoder&#x27;,\n",
       "                                                  OneHotEncoder(handle_unknown=&#x27;ignore&#x27;,\n",
       "                                                                sparse_output=False),\n",
       "                                                  [&#x27;Geography&#x27;, &#x27;Gender&#x27;,\n",
       "                                                   &#x27;IsActiveMember&#x27;, &#x27;Tenure&#x27;,\n",
       "                                                   &#x27;NumOfProducts&#x27;,\n",
       "                                                   &#x27;HasCrCard&#x27;])])),\n",
       "                (&#x27;skb&#x27;,\n",
       "                 SelectKBest(k=15, score_func=&lt;function chi2 at 0x15cbcbc40&gt;))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-766\" type=\"checkbox\" ><label for=\"sk-estimator-id-766\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;agg_balance_sum&#x27;,\n",
       "                 GroupByMergeTransformer(agg_col=&#x27;Balance&#x27;, agg_func=&#x27;sum&#x27;,\n",
       "                                         group_col=&#x27;CustomerId&#x27;)),\n",
       "                (&#x27;balance_zero_transformer&#x27;, ZeroTransformer(column=&#x27;Balance&#x27;)),\n",
       "                (&#x27;age_binning&#x27;, AgeBinning(n_bins=5)),\n",
       "                (&#x27;encoder&#x27;,\n",
       "                 ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                                   transformers=[(&#x27;encoder&#x27;,\n",
       "                                                  OneHotEncoder(handle_unknown=&#x27;ignore&#x27;,\n",
       "                                                                sparse_output=False),\n",
       "                                                  [&#x27;Geography&#x27;, &#x27;Gender&#x27;,\n",
       "                                                   &#x27;IsActiveMember&#x27;, &#x27;Tenure&#x27;,\n",
       "                                                   &#x27;NumOfProducts&#x27;,\n",
       "                                                   &#x27;HasCrCard&#x27;])])),\n",
       "                (&#x27;skb&#x27;,\n",
       "                 SelectKBest(k=15, score_func=&lt;function chi2 at 0x15cbcbc40&gt;))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-767\" type=\"checkbox\" ><label for=\"sk-estimator-id-767\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GroupByMergeTransformer</label><div class=\"sk-toggleable__content\"><pre>GroupByMergeTransformer(agg_col=&#x27;Balance&#x27;, agg_func=&#x27;sum&#x27;,\n",
       "                        group_col=&#x27;CustomerId&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-768\" type=\"checkbox\" ><label for=\"sk-estimator-id-768\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ZeroTransformer</label><div class=\"sk-toggleable__content\"><pre>ZeroTransformer(column=&#x27;Balance&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-769\" type=\"checkbox\" ><label for=\"sk-estimator-id-769\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">AgeBinning</label><div class=\"sk-toggleable__content\"><pre>AgeBinning(n_bins=5)</pre></div></div></div><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-770\" type=\"checkbox\" ><label for=\"sk-estimator-id-770\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">encoder: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                  transformers=[(&#x27;encoder&#x27;,\n",
       "                                 OneHotEncoder(handle_unknown=&#x27;ignore&#x27;,\n",
       "                                               sparse_output=False),\n",
       "                                 [&#x27;Geography&#x27;, &#x27;Gender&#x27;, &#x27;IsActiveMember&#x27;,\n",
       "                                  &#x27;Tenure&#x27;, &#x27;NumOfProducts&#x27;, &#x27;HasCrCard&#x27;])])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-771\" type=\"checkbox\" ><label for=\"sk-estimator-id-771\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">encoder</label><div class=\"sk-toggleable__content\"><pre>[&#x27;Geography&#x27;, &#x27;Gender&#x27;, &#x27;IsActiveMember&#x27;, &#x27;Tenure&#x27;, &#x27;NumOfProducts&#x27;, &#x27;HasCrCard&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-772\" type=\"checkbox\" ><label for=\"sk-estimator-id-772\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder(handle_unknown=&#x27;ignore&#x27;, sparse_output=False)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-773\" type=\"checkbox\" ><label for=\"sk-estimator-id-773\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">remainder</label><div class=\"sk-toggleable__content\"><pre>[&#x27;CreditScore&#x27;, &#x27;Age&#x27;, &#x27;Balance&#x27;, &#x27;EstimatedSalary&#x27;, &#x27;CustomerId&#x27;, &#x27;Balance_agg&#x27;, &#x27;Balance_is_zero&#x27;, &#x27;QCut5_Age&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-774\" type=\"checkbox\" ><label for=\"sk-estimator-id-774\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">passthrough</label><div class=\"sk-toggleable__content\"><pre>passthrough</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-775\" type=\"checkbox\" ><label for=\"sk-estimator-id-775\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SelectKBest</label><div class=\"sk-toggleable__content\"><pre>SelectKBest(k=15, score_func=&lt;function chi2 at 0x15cbcbc40&gt;)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('agg_balance_sum',\n",
       "                 GroupByMergeTransformer(agg_col='Balance', agg_func='sum',\n",
       "                                         group_col='CustomerId')),\n",
       "                ('balance_zero_transformer', ZeroTransformer(column='Balance')),\n",
       "                ('age_binning', AgeBinning(n_bins=5)),\n",
       "                ('encoder',\n",
       "                 ColumnTransformer(remainder='passthrough',\n",
       "                                   transformers=[('encoder',\n",
       "                                                  OneHotEncoder(handle_unknown='ignore',\n",
       "                                                                sparse_output=False),\n",
       "                                                  ['Geography', 'Gender',\n",
       "                                                   'IsActiveMember', 'Tenure',\n",
       "                                                   'NumOfProducts',\n",
       "                                                   'HasCrCard'])])),\n",
       "                ('skb',\n",
       "                 SelectKBest(k=15, score_func=<function chi2 at 0x15cbcbc40>))])"
      ]
     },
     "execution_count": 556,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_baseline = Pipeline([\n",
    "    ('encoder', encoding),\n",
    "    ])\n",
    "\n",
    "pipeline_baseline_binning = Pipeline([\n",
    "    ('age_binning', age_binner),\n",
    "    ('encoder', encoding),\n",
    "    ])\n",
    "\n",
    "pipeline_baseline_binning_sel = Pipeline([\n",
    "    ('age_binning', age_binner),\n",
    "    ('encoder', encoding),\n",
    "    ('skb', feature_selector)\n",
    "    ])\n",
    "\n",
    "pipeline_baseline_binning_sel_arth = Pipeline([\n",
    "    ('arithmetic_combiner', arithmetic_combiner),\n",
    "    ('age_binning', age_binner),\n",
    "    ('encoder', encoding),\n",
    "    ('skb', feature_selector)\n",
    "    ])\n",
    "\n",
    "pipeline_baseline_plus = Pipeline([\n",
    "    ('balance_zero_transformer', balance_zero_transformer),\n",
    "    ('age_binning', age_binner),\n",
    "    ('encoder', encoding),\n",
    "    ])\n",
    "\n",
    "pipeline_baseline_plus_sel = Pipeline([\n",
    "    ('agg_balance_sum', agg_balance_sum),\n",
    "    ('balance_zero_transformer', balance_zero_transformer),\n",
    "    ('age_binning', age_binner),\n",
    "    ('encoder', encoding),\n",
    "    ('skb', feature_selector)\n",
    "    ])\n",
    "\n",
    "pipeline_baseline_plus_sel_pca = Pipeline([\n",
    "    ('agg_balance_sum', agg_balance_sum),\n",
    "    ('balance_zero_transformer', balance_zero_transformer),\n",
    "    ('age_binning', age_binner),\n",
    "    ('encoder', encoding),\n",
    "    ('scaler', scaler),\n",
    "    ('pca_cluster', pca_clustering_transformer),\n",
    "    ('skb', feature_selector)\n",
    "    ])\n",
    "\n",
    "pipeline_all = Pipeline([\n",
    "    ('arithmetic_combiner', arithmetic_combiner),\n",
    "    ('agg_balance_sum', agg_balance_sum),\n",
    "    ('balance_zero_transformer', balance_zero_transformer),\n",
    "    ('age_binning', age_binner),\n",
    "    ('encoder', encoding),\n",
    "    ('scaler', scaler),\n",
    "    ('pca_cluster', pca_clustering_transformer),\n",
    "    ('skb', feature_selector)\n",
    "    ])\n",
    "\n",
    "\n",
    "selected_pipeline = pipeline_baseline_plus_sel\n",
    "selected_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [],
   "source": [
    "catboost_pipeline = make_pipeline(selected_pipeline, catboost_model)\n",
    "\n",
    "xgboost_pipeline = make_pipeline(selected_pipeline, xgboost_model)\n",
    "\n",
    "lgbm_pipeline = make_pipeline(selected_pipeline, lgbm_model)\n",
    "\n",
    "hist_pipeline = make_pipeline(selected_pipeline, hist_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_results(pipeline, params, cv, model):\n",
    "    steps = [step[0] for step in pipeline.named_steps['pipeline'].steps]\n",
    "    steps = ', '.join(steps)\n",
    "    log_df = pd.DataFrame()\n",
    "    log_df['Time'] = [datetime.datetime.now().strftime('%Y-%m-%d-%H-%M')]\n",
    "    log_df['Pipeline'] = [steps]\n",
    "    log_df['Params'] = [params]\n",
    "    log_df['ROC AUC'] = [cv]\n",
    "    log_df['Model'] = [model]\n",
    "\n",
    "    if os.path.exists('log.csv'):\n",
    "        log_df_ = pd.read_csv('log.csv')\n",
    "        log_df = pd.concat([log_df_, log_df])\n",
    "        log_df.to_csv('log.csv', index=False)\n",
    "    else:\n",
    "        log_df.to_csv('log.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [],
   "source": [
    "def instantiate_catboost(trial : Trial) -> CatBoostClassifier:\n",
    "  params = {\n",
    "        \"iterations\": 100,\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 0.1, log=True),\n",
    "        \"depth\": trial.suggest_int(\"depth\", 1, 10),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.05, 1.0),\n",
    "        'random_strength': trial.suggest_float('random_strength',1e-6,10,log=True),\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg',1e-8,100,log=True),\n",
    "        'model_size_reg': trial.suggest_float('model_size_reg',1e-8,100,log=True),\n",
    "    }\n",
    "  return CatBoostClassifier(**params, silent=True)\n",
    "\n",
    "def instantiate_xgboost(trial : Trial) -> XGBClassifier:\n",
    "  params = {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 0.1, log=True),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 2, 10),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.05, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.05, 1.0),\n",
    "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 20),\n",
    "  }\n",
    "  return XGBClassifier(**params)\n",
    "\n",
    "def instantiate_lgbm(trial : Trial) -> LGBMClassifier:\n",
    "  params = {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 0.1, log=True),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 2, 10),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.05, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.05, 1.0),\n",
    "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 20),\n",
    "  }\n",
    "  return LGBMClassifier(**params)\n",
    "\n",
    "def instantiate_hist(trial: Trial) -> HistGradientBoostingClassifier:\n",
    "    params = {\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "        'max_iter': trial.suggest_int('max_iter', 100, 1000),\n",
    "        'max_leaf_nodes': trial.suggest_int('max_leaf_nodes', 20, 64),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 20, 60),\n",
    "        'l2_regularization': trial.suggest_float('l2_regularization', 1e-10, 1.0, log=True)\n",
    "    }\n",
    "    return HistGradientBoostingClassifier(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial : Trial, pipeline, algo):\n",
    "    X = train_df[cat_cols+num_cols+['CustomerId']]\n",
    "    y = train_df['Exited']\n",
    "\n",
    "    if algo == 'cat':\n",
    "        # CATBOOST\n",
    "        pipeline = make_pipeline(pipeline, instantiate_catboost(trial))\n",
    "    elif algo == 'xgb':\n",
    "        # XGBOOST\n",
    "        pipeline = make_pipeline(pipeline, instantiate_xgboost(trial))\n",
    "    elif algo == 'lgbm':\n",
    "        # LGBM\n",
    "        pipeline = make_pipeline(pipeline, instantiate_lgbm(trial))\n",
    "    elif algo == 'hist':\n",
    "        pipeline = make_pipeline(pipeline, instantiate_hist(trial))\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    roc_auc_scorer = make_scorer(roc_auc_score, needs_proba=True)\n",
    "    scores = cross_val_score(pipeline, X, y, scoring=roc_auc_scorer, cv=skf)\n",
    "  \n",
    "    return np.min([np.mean(scores), np.median([scores])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:12:25,976] A new study created in memory with name: optimization\n",
      "[I 2024-01-18 20:12:32,107] Trial 0 finished with value: 0.8866345788738279 and parameters: {'learning_rate': 0.0640292715063446, 'depth': 7, 'subsample': 0.08168640160070685, 'random_strength': 6.915262738736656e-06, 'l2_leaf_reg': 8.789984245833185, 'model_size_reg': 1.0108037877678784}. Best is trial 0 with value: 0.8866345788738279.\n",
      "[I 2024-01-18 20:12:36,725] Trial 1 finished with value: 0.8838884978241062 and parameters: {'learning_rate': 0.022900173332229126, 'depth': 5, 'subsample': 0.14737748484993246, 'random_strength': 0.5846534244541821, 'l2_leaf_reg': 0.0003383587138462115, 'model_size_reg': 0.00022566173527363627}. Best is trial 0 with value: 0.8866345788738279.\n",
      "[I 2024-01-18 20:12:51,199] Trial 2 finished with value: 0.8845692681195791 and parameters: {'learning_rate': 0.002149803009607883, 'depth': 10, 'subsample': 0.06562796557070492, 'random_strength': 0.886536808854381, 'l2_leaf_reg': 0.06170032956748291, 'model_size_reg': 3.9287718499676604e-08}. Best is trial 0 with value: 0.8866345788738279.\n",
      "[I 2024-01-18 20:12:56,459] Trial 3 finished with value: 0.8591645691460854 and parameters: {'learning_rate': 0.0014820347642895526, 'depth': 4, 'subsample': 0.8358216290405047, 'random_strength': 0.03583366612188934, 'l2_leaf_reg': 3.0458438539944405e-05, 'model_size_reg': 8.997264853105233e-06}. Best is trial 0 with value: 0.8866345788738279.\n",
      "[I 2024-01-18 20:13:03,691] Trial 4 finished with value: 0.8856427205440669 and parameters: {'learning_rate': 0.014598414541724364, 'depth': 8, 'subsample': 0.1403749514102257, 'random_strength': 0.0004468397540279354, 'l2_leaf_reg': 2.6739973890044722e-06, 'model_size_reg': 0.0020389269209293297}. Best is trial 0 with value: 0.8866345788738279.\n",
      "[I 2024-01-18 20:13:15,281] Trial 5 finished with value: 0.8861610941503986 and parameters: {'learning_rate': 0.01599565361421617, 'depth': 9, 'subsample': 0.8129888182363106, 'random_strength': 0.003943467603870479, 'l2_leaf_reg': 0.0009057652189645091, 'model_size_reg': 0.023004751133134223}. Best is trial 0 with value: 0.8866345788738279.\n",
      "[I 2024-01-18 20:13:23,416] Trial 6 finished with value: 0.8812758569057314 and parameters: {'learning_rate': 0.019420589867559554, 'depth': 8, 'subsample': 0.7381489174234878, 'random_strength': 7.00351782422995, 'l2_leaf_reg': 6.708174837717495e-08, 'model_size_reg': 77.6592612736205}. Best is trial 0 with value: 0.8866345788738279.\n",
      "[I 2024-01-18 20:13:28,720] Trial 7 finished with value: 0.8861636252715233 and parameters: {'learning_rate': 0.03758014223152227, 'depth': 6, 'subsample': 0.30150438606884783, 'random_strength': 0.0007592785039786878, 'l2_leaf_reg': 0.00017290876838536685, 'model_size_reg': 0.00032977172268590263}. Best is trial 0 with value: 0.8866345788738279.\n",
      "[I 2024-01-18 20:13:33,515] Trial 8 finished with value: 0.8512513619338475 and parameters: {'learning_rate': 0.005536003096868564, 'depth': 3, 'subsample': 0.9637615731740102, 'random_strength': 0.0014379571811227382, 'l2_leaf_reg': 1.6555180418219518e-07, 'model_size_reg': 3.827340249693567e-06}. Best is trial 0 with value: 0.8866345788738279.\n",
      "[I 2024-01-18 20:13:37,558] Trial 9 finished with value: 0.8591756168527955 and parameters: {'learning_rate': 0.011606922092604603, 'depth': 2, 'subsample': 0.6587110986083687, 'random_strength': 9.023381237809386e-05, 'l2_leaf_reg': 0.005160821923573532, 'model_size_reg': 0.017351363844055598}. Best is trial 0 with value: 0.8866345788738279.\n",
      "[I 2024-01-18 20:13:42,966] Trial 10 finished with value: 0.8864615272230221 and parameters: {'learning_rate': 0.05148661168580443, 'depth': 6, 'subsample': 0.3449743491903412, 'random_strength': 1.2857486899963324e-06, 'l2_leaf_reg': 16.298030431103623, 'model_size_reg': 7.277058364743609}. Best is trial 0 with value: 0.8866345788738279.\n",
      "[I 2024-01-18 20:13:48,474] Trial 11 finished with value: 0.8868009780780259 and parameters: {'learning_rate': 0.0788126187012447, 'depth': 6, 'subsample': 0.41608500794302394, 'random_strength': 1.0828335506039025e-06, 'l2_leaf_reg': 72.30694365159188, 'model_size_reg': 10.979472286142347}. Best is trial 11 with value: 0.8868009780780259.\n",
      "[I 2024-01-18 20:13:54,934] Trial 12 finished with value: 0.8872386089204809 and parameters: {'learning_rate': 0.09652230939427657, 'depth': 7, 'subsample': 0.5136433754472601, 'random_strength': 1.3629333287050142e-06, 'l2_leaf_reg': 41.95200475320008, 'model_size_reg': 0.9034777336699488}. Best is trial 12 with value: 0.8872386089204809.\n",
      "[I 2024-01-18 20:13:58,612] Trial 13 finished with value: 0.8737343007567152 and parameters: {'learning_rate': 0.09113662329349778, 'depth': 1, 'subsample': 0.5336163986280018, 'random_strength': 2.1391551812420575e-05, 'l2_leaf_reg': 0.48118030304513865, 'model_size_reg': 1.156586445785089}. Best is trial 12 with value: 0.8872386089204809.\n",
      "[I 2024-01-18 20:14:03,508] Trial 14 finished with value: 0.8866829287897464 and parameters: {'learning_rate': 0.09708089114386088, 'depth': 5, 'subsample': 0.4228386820817288, 'random_strength': 1.1636582970895276e-06, 'l2_leaf_reg': 92.17525967991314, 'model_size_reg': 88.6771797758426}. Best is trial 12 with value: 0.8872386089204809.\n",
      "[I 2024-01-18 20:14:10,099] Trial 15 finished with value: 0.8831608077155263 and parameters: {'learning_rate': 0.005722466467810224, 'depth': 7, 'subsample': 0.5474067128784152, 'random_strength': 1.7246562299848052e-05, 'l2_leaf_reg': 0.7197020982014452, 'model_size_reg': 0.23604427598776762}. Best is trial 12 with value: 0.8872386089204809.\n",
      "[I 2024-01-18 20:14:14,505] Trial 16 finished with value: 0.8843618590065567 and parameters: {'learning_rate': 0.03495936517831216, 'depth': 4, 'subsample': 0.4045759936143529, 'random_strength': 0.0001155558374186974, 'l2_leaf_reg': 0.034223526129872235, 'model_size_reg': 6.8897453536125965}. Best is trial 12 with value: 0.8872386089204809.\n",
      "[I 2024-01-18 20:14:21,271] Trial 17 finished with value: 0.8853886675148344 and parameters: {'learning_rate': 0.03283489323853743, 'depth': 7, 'subsample': 0.6403207992881328, 'random_strength': 4.558093807104365e-06, 'l2_leaf_reg': 85.31447543328264, 'model_size_reg': 0.07874353583199757}. Best is trial 12 with value: 0.8872386089204809.\n",
      "[I 2024-01-18 20:14:36,017] Trial 18 finished with value: 0.887321101459396 and parameters: {'learning_rate': 0.06564060410261023, 'depth': 10, 'subsample': 0.2583263818222128, 'random_strength': 0.008675166494175011, 'l2_leaf_reg': 2.3930072764605907, 'model_size_reg': 11.808909554860776}. Best is trial 18 with value: 0.887321101459396.\n",
      "[I 2024-01-18 20:14:50,807] Trial 19 finished with value: 0.885516802770552 and parameters: {'learning_rate': 0.007672309417217349, 'depth': 10, 'subsample': 0.2695830411069287, 'random_strength': 0.013217326868925227, 'l2_leaf_reg': 2.422499160463419, 'model_size_reg': 0.0047273086858042055}. Best is trial 18 with value: 0.887321101459396.\n",
      "[I 2024-01-18 20:15:00,994] Trial 20 finished with value: 0.8872239999496417 and parameters: {'learning_rate': 0.04979593719806406, 'depth': 9, 'subsample': 0.2405130690237013, 'random_strength': 0.0967109078158157, 'l2_leaf_reg': 0.06257732009331905, 'model_size_reg': 0.44917264684224456}. Best is trial 18 with value: 0.887321101459396.\n",
      "[I 2024-01-18 20:15:11,134] Trial 21 finished with value: 0.8873355288498066 and parameters: {'learning_rate': 0.05410924833574149, 'depth': 9, 'subsample': 0.22782912408410483, 'random_strength': 0.03985751428413683, 'l2_leaf_reg': 0.1002187366487778, 'model_size_reg': 0.19277259450835516}. Best is trial 21 with value: 0.8873355288498066.\n",
      "[I 2024-01-18 20:15:21,133] Trial 22 finished with value: 0.8873147186322121 and parameters: {'learning_rate': 0.05748395389176948, 'depth': 9, 'subsample': 0.2015715435221747, 'random_strength': 0.006948945574565614, 'l2_leaf_reg': 0.47551970958037354, 'model_size_reg': 4.260000072538249}. Best is trial 21 with value: 0.8873355288498066.\n",
      "[I 2024-01-18 20:15:31,135] Trial 23 finished with value: 0.8866118097885806 and parameters: {'learning_rate': 0.027440396528421528, 'depth': 9, 'subsample': 0.2131696846056836, 'random_strength': 0.005932657386522572, 'l2_leaf_reg': 0.008973095363099825, 'model_size_reg': 5.789067521971723}. Best is trial 21 with value: 0.8873355288498066.\n",
      "[I 2024-01-18 20:15:45,663] Trial 24 finished with value: 0.886999181369399 and parameters: {'learning_rate': 0.05261849035028076, 'depth': 10, 'subsample': 0.17947224493859287, 'random_strength': 0.09857649612898099, 'l2_leaf_reg': 0.4706492740916997, 'model_size_reg': 0.05426753596591285}. Best is trial 21 with value: 0.8873355288498066.\n",
      "[I 2024-01-18 20:15:53,397] Trial 25 finished with value: 0.8867994043809789 and parameters: {'learning_rate': 0.041144614813027655, 'depth': 8, 'subsample': 0.3504007930658427, 'random_strength': 0.016198163483635262, 'l2_leaf_reg': 3.0721573433722753, 'model_size_reg': 29.559118848142187}. Best is trial 21 with value: 0.8873355288498066.\n",
      "[I 2024-01-18 20:16:03,735] Trial 26 finished with value: 0.8849698164906371 and parameters: {'learning_rate': 0.0032598053921824867, 'depth': 9, 'subsample': 0.29247120666185983, 'random_strength': 0.16766201927415383, 'l2_leaf_reg': 0.1730611491640609, 'model_size_reg': 1.8356403028238355}. Best is trial 21 with value: 0.8873355288498066.\n",
      "[I 2024-01-18 20:16:18,140] Trial 27 finished with value: 0.8869985375842435 and parameters: {'learning_rate': 0.06524338882210025, 'depth': 10, 'subsample': 0.13396423874102634, 'random_strength': 0.022266243371446785, 'l2_leaf_reg': 0.007211557037886599, 'model_size_reg': 0.14872902835396076}. Best is trial 21 with value: 0.8873355288498066.\n",
      "[I 2024-01-18 20:16:25,650] Trial 28 finished with value: 0.8871207192028809 and parameters: {'learning_rate': 0.057656876697713015, 'depth': 8, 'subsample': 0.21600098651260013, 'random_strength': 0.0023606181947887187, 'l2_leaf_reg': 2.449343866958471, 'model_size_reg': 0.009058735345183467}. Best is trial 21 with value: 0.8873355288498066.\n",
      "[I 2024-01-18 20:16:35,310] Trial 29 finished with value: 0.8854592527795893 and parameters: {'learning_rate': 0.025205795601271508, 'depth': 9, 'subsample': 0.053616622036712414, 'random_strength': 0.7734380161478964, 'l2_leaf_reg': 5.587411674562446, 'model_size_reg': 2.873441406265946}. Best is trial 21 with value: 0.8873355288498066.\n",
      "[I 2024-01-18 20:16:51,033] Trial 30 finished with value: 0.8872326387760889 and parameters: {'learning_rate': 0.06960666395341669, 'depth': 10, 'subsample': 0.4726873390191279, 'random_strength': 0.00019431684358202242, 'l2_leaf_reg': 0.0247988347170993, 'model_size_reg': 37.76595634262868}. Best is trial 21 with value: 0.8873355288498066.\n",
      "[I 2024-01-18 20:16:57,516] Trial 31 finished with value: 0.8873478763189451 and parameters: {'learning_rate': 0.0926681235540985, 'depth': 7, 'subsample': 0.32669875241702706, 'random_strength': 0.006181345165083915, 'l2_leaf_reg': 5.787262922121833, 'model_size_reg': 0.6234966088950679}. Best is trial 31 with value: 0.8873478763189451.\n",
      "[I 2024-01-18 20:17:05,338] Trial 32 finished with value: 0.8871411167376834 and parameters: {'learning_rate': 0.06731390978181391, 'depth': 8, 'subsample': 0.3521215145344653, 'random_strength': 0.00796932500192002, 'l2_leaf_reg': 12.911286156001413, 'model_size_reg': 0.44514784928096324}. Best is trial 31 with value: 0.8873478763189451.\n",
      "[I 2024-01-18 20:17:15,135] Trial 33 finished with value: 0.8868132925325413 and parameters: {'learning_rate': 0.043334496767707, 'depth': 9, 'subsample': 0.10574953817119229, 'random_strength': 0.06009868257902643, 'l2_leaf_reg': 0.1857493515082619, 'model_size_reg': 11.15099747960089}. Best is trial 31 with value: 0.8873478763189451.\n",
      "[I 2024-01-18 20:17:20,937] Trial 34 finished with value: 0.8871801785395617 and parameters: {'learning_rate': 0.0746265745791167, 'depth': 7, 'subsample': 0.18254535110106027, 'random_strength': 0.25911558785620215, 'l2_leaf_reg': 0.7925400315706274, 'model_size_reg': 4.644382405846907e-05}. Best is trial 31 with value: 0.8873478763189451.\n",
      "[I 2024-01-18 20:17:35,497] Trial 35 finished with value: 0.8867503886701559 and parameters: {'learning_rate': 0.027627221448614653, 'depth': 10, 'subsample': 0.26388162622531264, 'random_strength': 0.0009214780724816721, 'l2_leaf_reg': 0.1450620555448589, 'model_size_reg': 0.000989474819815833}. Best is trial 31 with value: 0.8873478763189451.\n",
      "[I 2024-01-18 20:17:42,603] Trial 36 finished with value: 0.8841967285717574 and parameters: {'learning_rate': 0.020416724232007678, 'depth': 8, 'subsample': 0.16609463651205597, 'random_strength': 2.2312115197943654, 'l2_leaf_reg': 15.133553142246503, 'model_size_reg': 8.633321654053287e-08}. Best is trial 31 with value: 0.8873478763189451.\n",
      "[I 2024-01-18 20:17:53,260] Trial 37 finished with value: 0.8849369559355142 and parameters: {'learning_rate': 0.001046319442686205, 'depth': 9, 'subsample': 0.3180876773226694, 'random_strength': 0.02874481363437645, 'l2_leaf_reg': 0.001334434127599491, 'model_size_reg': 0.06877645197061785}. Best is trial 31 with value: 0.8873478763189451.\n",
      "[I 2024-01-18 20:18:07,956] Trial 38 finished with value: 0.8859576525387849 and parameters: {'learning_rate': 0.016025269123666003, 'depth': 10, 'subsample': 0.0963123325396111, 'random_strength': 0.002641802943907288, 'l2_leaf_reg': 4.968197847833494e-05, 'model_size_reg': 1.6223820986188515}. Best is trial 31 with value: 0.8873478763189451.\n",
      "[I 2024-01-18 20:18:15,043] Trial 39 finished with value: 0.8867817305526041 and parameters: {'learning_rate': 0.04455858877774566, 'depth': 7, 'subsample': 0.3815551078150043, 'random_strength': 0.0003374372055423452, 'l2_leaf_reg': 1.52246457620065, 'model_size_reg': 20.468812904319694}. Best is trial 31 with value: 0.8873478763189451.\n",
      "[I 2024-01-18 20:18:23,464] Trial 40 finished with value: 0.8864269389026098 and parameters: {'learning_rate': 0.03216063155358395, 'depth': 8, 'subsample': 0.46487118285810053, 'random_strength': 0.3853781542645176, 'l2_leaf_reg': 0.0019706376361513477, 'model_size_reg': 0.5103834413186107}. Best is trial 31 with value: 0.8873478763189451.\n",
      "[I 2024-01-18 20:18:30,560] Trial 41 finished with value: 0.8872081749401752 and parameters: {'learning_rate': 0.09960276991876262, 'depth': 7, 'subsample': 0.6052464145693772, 'random_strength': 0.008232239745025917, 'l2_leaf_reg': 27.031094133847944, 'model_size_reg': 1.0484369107340952}. Best is trial 31 with value: 0.8873478763189451.\n",
      "[I 2024-01-18 20:18:36,586] Trial 42 finished with value: 0.8870581950086645 and parameters: {'learning_rate': 0.07385267272828301, 'depth': 6, 'subsample': 0.4856112992249585, 'random_strength': 0.03863193092236939, 'l2_leaf_reg': 5.613616769390131, 'model_size_reg': 3.4113211967895296}. Best is trial 31 with value: 0.8873478763189451.\n",
      "[I 2024-01-18 20:18:41,674] Trial 43 finished with value: 0.8868726033034168 and parameters: {'learning_rate': 0.08080059356146387, 'depth': 5, 'subsample': 0.230302403148752, 'random_strength': 0.0048384888899775345, 'l2_leaf_reg': 3.4266374152203804e-06, 'model_size_reg': 0.03847383710916533}. Best is trial 31 with value: 0.8873478763189451.\n",
      "[I 2024-01-18 20:18:47,439] Trial 44 finished with value: 0.8856085473071664 and parameters: {'learning_rate': 0.058563881365678166, 'depth': 4, 'subsample': 0.7470413701585503, 'random_strength': 0.0013132508355181288, 'l2_leaf_reg': 30.40560755270994, 'model_size_reg': 0.17857204558940634}. Best is trial 31 with value: 0.8873478763189451.\n",
      "[I 2024-01-18 20:19:00,282] Trial 45 finished with value: 0.887769109898462 and parameters: {'learning_rate': 0.08984614537243998, 'depth': 9, 'subsample': 0.8920454231792831, 'random_strength': 0.0025669741381674537, 'l2_leaf_reg': 1.3091379303049552e-08, 'model_size_reg': 39.39005861220538}. Best is trial 45 with value: 0.887769109898462.\n",
      "[I 2024-01-18 20:19:14,442] Trial 46 finished with value: 0.8874708998104786 and parameters: {'learning_rate': 0.058257660528315776, 'depth': 9, 'subsample': 0.9991059005679003, 'random_strength': 0.0004195263904376436, 'l2_leaf_reg': 0.000326433222522659, 'model_size_reg': 59.39768857727536}. Best is trial 45 with value: 0.887769109898462.\n",
      "[I 2024-01-18 20:19:28,904] Trial 47 finished with value: 0.8873511667764072 and parameters: {'learning_rate': 0.08334206685015215, 'depth': 9, 'subsample': 0.9978458693779518, 'random_strength': 0.0005121599197133222, 'l2_leaf_reg': 6.193934603822038e-07, 'model_size_reg': 65.0889142387181}. Best is trial 45 with value: 0.887769109898462.\n",
      "[I 2024-01-18 20:19:39,698] Trial 48 finished with value: 0.8875065115842151 and parameters: {'learning_rate': 0.08560099591054704, 'depth': 8, 'subsample': 0.9264569939005944, 'random_strength': 0.0004899070242521912, 'l2_leaf_reg': 1.5166632964025854e-08, 'model_size_reg': 67.44433001208459}. Best is trial 45 with value: 0.887769109898462.\n",
      "[I 2024-01-18 20:19:50,742] Trial 49 finished with value: 0.8876149095975976 and parameters: {'learning_rate': 0.08443943145770276, 'depth': 8, 'subsample': 0.991506017184951, 'random_strength': 3.997798461109494e-05, 'l2_leaf_reg': 1.4667863119279883e-08, 'model_size_reg': 85.24907981325666}. Best is trial 45 with value: 0.887769109898462.\n",
      "[I 2024-01-18 20:20:01,660] Trial 50 finished with value: 0.8876302393877133 and parameters: {'learning_rate': 0.0868172151484312, 'depth': 8, 'subsample': 0.9979548772585498, 'random_strength': 3.510923393779546e-05, 'l2_leaf_reg': 1.0572273827017674e-08, 'model_size_reg': 94.6451750193339}. Best is trial 45 with value: 0.887769109898462.\n",
      "[I 2024-01-18 20:20:11,857] Trial 51 finished with value: 0.8876679971119689 and parameters: {'learning_rate': 0.08018587237809896, 'depth': 8, 'subsample': 0.9905803926685092, 'random_strength': 3.29945256942119e-05, 'l2_leaf_reg': 1.3224288916709068e-08, 'model_size_reg': 92.26419908674019}. Best is trial 45 with value: 0.887769109898462.\n",
      "[I 2024-01-18 20:20:21,558] Trial 52 finished with value: 0.8875321309319465 and parameters: {'learning_rate': 0.08124842796019803, 'depth': 8, 'subsample': 0.9070257780548131, 'random_strength': 5.18425231117728e-05, 'l2_leaf_reg': 1.0016515205138488e-08, 'model_size_reg': 92.50293230773619}. Best is trial 45 with value: 0.887769109898462.\n",
      "[I 2024-01-18 20:20:31,083] Trial 53 finished with value: 0.8874820807629249 and parameters: {'learning_rate': 0.08411171387923995, 'depth': 8, 'subsample': 0.8897077903999117, 'random_strength': 3.8333061066045463e-05, 'l2_leaf_reg': 1.4408306212450854e-08, 'model_size_reg': 97.65831487301382}. Best is trial 45 with value: 0.887769109898462.\n",
      "[I 2024-01-18 20:20:41,294] Trial 54 finished with value: 0.8836982252571024 and parameters: {'learning_rate': 0.0031911188151335142, 'depth': 8, 'subsample': 0.9147092081502979, 'random_strength': 5.509034788390029e-06, 'l2_leaf_reg': 2.2515171884171072e-08, 'model_size_reg': 22.194146826989154}. Best is trial 45 with value: 0.887769109898462.\n",
      "[I 2024-01-18 20:20:51,245] Trial 55 finished with value: 0.8872454264402057 and parameters: {'learning_rate': 0.046510074727648676, 'depth': 8, 'subsample': 0.9309428852316103, 'random_strength': 4.2310953344858555e-05, 'l2_leaf_reg': 7.589819558533066e-08, 'model_size_reg': 22.612458697668625}. Best is trial 45 with value: 0.887769109898462.\n",
      "[I 2024-01-18 20:20:58,040] Trial 56 finished with value: 0.8873092547120451 and parameters: {'learning_rate': 0.08062511800947157, 'depth': 6, 'subsample': 0.8462732073162926, 'random_strength': 8.150218554793449e-05, 'l2_leaf_reg': 3.739294081610691e-07, 'model_size_reg': 40.55617117684458}. Best is trial 45 with value: 0.887769109898462.\n",
      "[I 2024-01-18 20:21:05,828] Trial 57 finished with value: 0.8874899712579091 and parameters: {'learning_rate': 0.09994097762758064, 'depth': 7, 'subsample': 0.8562010496171317, 'random_strength': 1.0088435182878812e-05, 'l2_leaf_reg': 3.6440526562329255e-08, 'model_size_reg': 89.88376242284671}. Best is trial 45 with value: 0.887769109898462.\n",
      "[I 2024-01-18 20:21:16,483] Trial 58 finished with value: 0.8869947188928076 and parameters: {'learning_rate': 0.037584789152331446, 'depth': 8, 'subsample': 0.795510753076451, 'random_strength': 2.3711616909493007e-06, 'l2_leaf_reg': 1.0599125369424345e-07, 'model_size_reg': 12.245111272810528}. Best is trial 45 with value: 0.887769109898462.\n",
      "[I 2024-01-18 20:21:25,448] Trial 59 finished with value: 0.8872311035961025 and parameters: {'learning_rate': 0.0699366928676756, 'depth': 6, 'subsample': 0.9578415640755635, 'random_strength': 1.770118910999242e-05, 'l2_leaf_reg': 1.1908713304215659e-08, 'model_size_reg': 7.572523884295046}. Best is trial 45 with value: 0.887769109898462.\n",
      "[I 2024-01-18 20:21:30,386] Trial 60 finished with value: 0.8132399575233237 and parameters: {'learning_rate': 0.00783144886245757, 'depth': 1, 'subsample': 0.8750922058553002, 'random_strength': 3.960146939042659e-05, 'l2_leaf_reg': 2.635080990986184e-07, 'model_size_reg': 1.0501736186712395e-08}. Best is trial 45 with value: 0.887769109898462.\n",
      "[I 2024-01-18 20:21:39,309] Trial 61 finished with value: 0.887609599745673 and parameters: {'learning_rate': 0.09633246828294292, 'depth': 7, 'subsample': 0.9473130830696098, 'random_strength': 1.111482108950539e-05, 'l2_leaf_reg': 2.8787852089105638e-08, 'model_size_reg': 96.60430474367377}. Best is trial 45 with value: 0.887769109898462.\n",
      "[I 2024-01-18 20:21:50,616] Trial 62 finished with value: 0.8876091540482575 and parameters: {'learning_rate': 0.08549847656972283, 'depth': 8, 'subsample': 0.9511485730616712, 'random_strength': 1.0836447506573699e-05, 'l2_leaf_reg': 3.168187949324558e-08, 'model_size_reg': 34.05677666855026}. Best is trial 45 with value: 0.887769109898462.\n",
      "[I 2024-01-18 20:21:59,617] Trial 63 finished with value: 0.8873470729631099 and parameters: {'learning_rate': 0.06198492241864235, 'depth': 7, 'subsample': 0.9544227089884876, 'random_strength': 9.656575883430261e-06, 'l2_leaf_reg': 3.64454551726457e-08, 'model_size_reg': 17.50720939911148}. Best is trial 45 with value: 0.887769109898462.\n",
      "[I 2024-01-18 20:22:10,312] Trial 64 finished with value: 0.8874558396397867 and parameters: {'learning_rate': 0.09928835082931142, 'depth': 8, 'subsample': 0.7993487621939751, 'random_strength': 2.9320973616668677e-06, 'l2_leaf_reg': 4.322062042453792e-08, 'model_size_reg': 35.0303463921635}. Best is trial 45 with value: 0.887769109898462.\n",
      "[I 2024-01-18 20:22:25,150] Trial 65 finished with value: 0.8874703385618812 and parameters: {'learning_rate': 0.050619947037318126, 'depth': 9, 'subsample': 0.8955686288138542, 'random_strength': 0.00015471100287781922, 'l2_leaf_reg': 1.5599219329408251e-06, 'model_size_reg': 1.496596998828226e-06}. Best is trial 45 with value: 0.887769109898462.\n",
      "[I 2024-01-18 20:22:34,112] Trial 66 finished with value: 0.8874415112932461 and parameters: {'learning_rate': 0.07956167480490123, 'depth': 7, 'subsample': 0.9709327131570779, 'random_strength': 7.16489934013156e-05, 'l2_leaf_reg': 1.2125325996185644e-07, 'model_size_reg': 99.3331176356397}. Best is trial 45 with value: 0.887769109898462.\n",
      "[I 2024-01-18 20:22:45,385] Trial 67 finished with value: 0.8875099285977335 and parameters: {'learning_rate': 0.06608160244008271, 'depth': 8, 'subsample': 0.9760006694573731, 'random_strength': 2.519002759584191e-05, 'l2_leaf_reg': 7.663462436897974e-07, 'model_size_reg': 5.143594713589489}. Best is trial 45 with value: 0.887769109898462.\n",
      "[I 2024-01-18 20:22:51,209] Trial 68 finished with value: 0.8855834097731914 and parameters: {'learning_rate': 0.08736296066407725, 'depth': 3, 'subsample': 0.8229550983529554, 'random_strength': 8.164202277209006e-06, 'l2_leaf_reg': 1.0888193812951827e-08, 'model_size_reg': 36.851133307512384}. Best is trial 45 with value: 0.887769109898462.\n",
      "[I 2024-01-18 20:23:07,082] Trial 69 finished with value: 0.8872812803211803 and parameters: {'learning_rate': 0.037915628134229946, 'depth': 9, 'subsample': 0.9424804201281856, 'random_strength': 1.2596096198136863e-05, 'l2_leaf_reg': 1.7953335151709604e-07, 'model_size_reg': 2.3283243586156237}. Best is trial 45 with value: 0.887769109898462.\n",
      "[I 2024-01-18 20:23:18,035] Trial 70 finished with value: 0.8876020669091084 and parameters: {'learning_rate': 0.07309545617346991, 'depth': 8, 'subsample': 0.8941328194209448, 'random_strength': 5.4425172515840793e-05, 'l2_leaf_reg': 2.785305238041886e-08, 'model_size_reg': 6.423738787773271}. Best is trial 45 with value: 0.887769109898462.\n",
      "[I 2024-01-18 20:23:28,975] Trial 71 finished with value: 0.8875028744732076 and parameters: {'learning_rate': 0.07363311295402951, 'depth': 8, 'subsample': 0.9077343456342857, 'random_strength': 2.830443989956071e-05, 'l2_leaf_reg': 2.4498324667494265e-08, 'model_size_reg': 11.508011756991683}. Best is trial 45 with value: 0.887769109898462.\n",
      "[I 2024-01-18 20:23:40,126] Trial 72 finished with value: 0.887319384698981 and parameters: {'learning_rate': 0.05236072533556223, 'depth': 8, 'subsample': 0.8671450033444646, 'random_strength': 0.00019839362970527438, 'l2_leaf_reg': 5.59000396809781e-08, 'model_size_reg': 40.2874980415941}. Best is trial 45 with value: 0.887769109898462.\n",
      "[I 2024-01-18 20:23:49,450] Trial 73 finished with value: 0.8872516056772122 and parameters: {'learning_rate': 0.06320177410852947, 'depth': 7, 'subsample': 0.7372793908176546, 'random_strength': 3.5559352755436756e-06, 'l2_leaf_reg': 2.2998236067530973e-08, 'model_size_reg': 7.246465168857304}. Best is trial 45 with value: 0.887769109898462.\n",
      "[I 2024-01-18 20:24:00,854] Trial 74 finished with value: 0.88766418392297 and parameters: {'learning_rate': 0.08850590596259235, 'depth': 8, 'subsample': 0.9770280266960596, 'random_strength': 5.5682726307169586e-05, 'l2_leaf_reg': 1.0167100264018186e-08, 'model_size_reg': 15.365981882861151}. Best is trial 45 with value: 0.887769109898462.\n",
      "[I 2024-01-18 20:24:09,927] Trial 75 finished with value: 0.8874025925547359 and parameters: {'learning_rate': 0.07144154700190325, 'depth': 7, 'subsample': 0.9826147823541196, 'random_strength': 1.536934489420115e-05, 'l2_leaf_reg': 7.104287400238952e-08, 'model_size_reg': 17.579008636668554}. Best is trial 45 with value: 0.887769109898462.\n",
      "[I 2024-01-18 20:24:24,301] Trial 76 finished with value: 0.8875810200867131 and parameters: {'learning_rate': 0.0910867084567142, 'depth': 9, 'subsample': 0.9514888078069176, 'random_strength': 1.807652698904547e-06, 'l2_leaf_reg': 2.0702975190297007e-07, 'model_size_reg': 36.95189965074337}. Best is trial 45 with value: 0.887769109898462.\n",
      "[I 2024-01-18 20:24:39,106] Trial 77 finished with value: 0.8874778824033203 and parameters: {'learning_rate': 0.05705170441100874, 'depth': 9, 'subsample': 0.764261638270577, 'random_strength': 5.919548006513794e-06, 'l2_leaf_reg': 2.737865996043245e-08, 'model_size_reg': 4.023323852939511}. Best is trial 45 with value: 0.887769109898462.\n",
      "[I 2024-01-18 20:24:50,897] Trial 78 finished with value: 0.8853600438363768 and parameters: {'learning_rate': 0.01180730883825336, 'depth': 8, 'subsample': 0.9980775322753748, 'random_strength': 6.202194195554517e-05, 'l2_leaf_reg': 8.75564277796766e-08, 'model_size_reg': 13.395829948128942}. Best is trial 45 with value: 0.887769109898462.\n",
      "[I 2024-01-18 20:25:00,090] Trial 79 finished with value: 0.8869948289415521 and parameters: {'learning_rate': 0.04812744490861313, 'depth': 7, 'subsample': 0.929178477461801, 'random_strength': 0.00011818849181816486, 'l2_leaf_reg': 3.759442195941864e-07, 'model_size_reg': 7.538428530737001}. Best is trial 45 with value: 0.887769109898462.\n",
      "[I 2024-01-18 20:25:11,078] Trial 80 finished with value: 0.8876142327978185 and parameters: {'learning_rate': 0.07426396944452546, 'depth': 8, 'subsample': 0.8844173376050433, 'random_strength': 2.9876542472092433e-05, 'l2_leaf_reg': 5.1777878344338294e-06, 'model_size_reg': 47.447167389265616}. Best is trial 45 with value: 0.887769109898462.\n",
      "[I 2024-01-18 20:25:21,862] Trial 81 finished with value: 0.8875280701348478 and parameters: {'learning_rate': 0.08773618709118815, 'depth': 8, 'subsample': 0.9686510899225792, 'random_strength': 2.35565815311569e-05, 'l2_leaf_reg': 1.6215531669201015e-05, 'model_size_reg': 51.213388804611455}. Best is trial 45 with value: 0.887769109898462.\n",
      "[I 2024-01-18 20:25:35,551] Trial 82 finished with value: 0.8876113550231486 and parameters: {'learning_rate': 0.07530786843947382, 'depth': 9, 'subsample': 0.8320311481191768, 'random_strength': 0.0002500735218258638, 'l2_leaf_reg': 4.892734371834485e-08, 'model_size_reg': 25.02697412240566}. Best is trial 45 with value: 0.887769109898462.\n",
      "[I 2024-01-18 20:25:49,500] Trial 83 finished with value: 0.8875262378216757 and parameters: {'learning_rate': 0.09962029696330922, 'depth': 9, 'subsample': 0.689243868939888, 'random_strength': 3.069272628191702e-05, 'l2_leaf_reg': 6.216486186733378e-06, 'model_size_reg': 24.461578161789216}. Best is trial 45 with value: 0.887769109898462.\n",
      "[I 2024-01-18 20:26:03,865] Trial 84 finished with value: 0.8875668182962289 and parameters: {'learning_rate': 0.06398885705520414, 'depth': 9, 'subsample': 0.836678282374368, 'random_strength': 0.00023808200880345867, 'l2_leaf_reg': 4.313511618916656e-08, 'model_size_reg': 45.2081094908714}. Best is trial 45 with value: 0.887769109898462.\n",
      "[I 2024-01-18 20:26:24,241] Trial 85 finished with value: 0.8874167228135361 and parameters: {'learning_rate': 0.07539411978006749, 'depth': 10, 'subsample': 0.9334691693972875, 'random_strength': 0.00011127816720463706, 'l2_leaf_reg': 1.9294841332568337e-08, 'model_size_reg': 0.00012199823523212614}. Best is trial 45 with value: 0.887769109898462.\n",
      "[I 2024-01-18 20:26:37,922] Trial 86 finished with value: 0.8874319370524704 and parameters: {'learning_rate': 0.08943590875125346, 'depth': 9, 'subsample': 0.8701046395633759, 'random_strength': 1.6153245844659415e-05, 'l2_leaf_reg': 1.49779720627922e-07, 'model_size_reg': 21.803958870772192}. Best is trial 45 with value: 0.887769109898462.\n",
      "[I 2024-01-18 20:26:48,408] Trial 87 finished with value: 0.8872988441008106 and parameters: {'learning_rate': 0.05390520633842088, 'depth': 8, 'subsample': 0.9588098467517518, 'random_strength': 0.00029749697241539393, 'l2_leaf_reg': 8.627383584900379e-05, 'model_size_reg': 1.9515610121543268}. Best is trial 45 with value: 0.887769109898462.\n",
      "[I 2024-01-18 20:26:56,658] Trial 88 finished with value: 0.8868487172234121 and parameters: {'learning_rate': 0.04229098385057681, 'depth': 7, 'subsample': 0.9999429509248361, 'random_strength': 4.240168128489375e-06, 'l2_leaf_reg': 1.0015477819462326e-08, 'model_size_reg': 58.52247358169194}. Best is trial 45 with value: 0.887769109898462.\n",
      "[I 2024-01-18 20:27:09,374] Trial 89 finished with value: 0.8875309314006309 and parameters: {'learning_rate': 0.06250109612219713, 'depth': 9, 'subsample': 0.8861865095196444, 'random_strength': 0.0007010610706469533, 'l2_leaf_reg': 7.436374040698811e-08, 'model_size_reg': 12.378504067241295}. Best is trial 45 with value: 0.887769109898462.\n",
      "[I 2024-01-18 20:27:19,215] Trial 90 finished with value: 0.8875461896590628 and parameters: {'learning_rate': 0.08826670996613252, 'depth': 8, 'subsample': 0.9153134337904574, 'random_strength': 7.47666039357934e-06, 'l2_leaf_reg': 8.835518065285753e-07, 'model_size_reg': 27.582605190740523}. Best is trial 45 with value: 0.887769109898462.\n",
      "[I 2024-01-18 20:27:29,555] Trial 91 finished with value: 0.887428833677874 and parameters: {'learning_rate': 0.07586686911138035, 'depth': 8, 'subsample': 0.944075766037838, 'random_strength': 5.1638403585447664e-05, 'l2_leaf_reg': 3.813396179034634e-08, 'model_size_reg': 98.27026875851317}. Best is trial 45 with value: 0.887769109898462.\n",
      "[I 2024-01-18 20:27:39,801] Trial 92 finished with value: 0.8875026928927792 and parameters: {'learning_rate': 0.07433758585519856, 'depth': 8, 'subsample': 0.9817394573271848, 'random_strength': 9.289302326391063e-05, 'l2_leaf_reg': 1.9885588278198126e-08, 'model_size_reg': 4.562890629029068}. Best is trial 45 with value: 0.887769109898462.\n",
      "[I 2024-01-18 20:27:50,544] Trial 93 finished with value: 0.8833045588880957 and parameters: {'learning_rate': 0.002043402474562126, 'depth': 8, 'subsample': 0.8971065177276533, 'random_strength': 0.00013808880879913012, 'l2_leaf_reg': 5.489422966520322e-08, 'model_size_reg': 59.36078645292532}. Best is trial 45 with value: 0.887769109898462.\n",
      "[I 2024-01-18 20:28:02,777] Trial 94 finished with value: 0.8875784064290301 and parameters: {'learning_rate': 0.0685623750249734, 'depth': 9, 'subsample': 0.855941365322054, 'random_strength': 1.3335116504973946e-05, 'l2_leaf_reg': 1.6115902694894936e-08, 'model_size_reg': 9.62996683228504}. Best is trial 45 with value: 0.887769109898462.\n",
      "[I 2024-01-18 20:28:09,539] Trial 95 finished with value: 0.8873747557248017 and parameters: {'learning_rate': 0.0916817058448798, 'depth': 6, 'subsample': 0.806721990106418, 'random_strength': 6.978057317551311e-05, 'l2_leaf_reg': 3.1673229511568836e-07, 'model_size_reg': 17.24333303962326}. Best is trial 45 with value: 0.887769109898462.\n",
      "[I 2024-01-18 20:28:17,781] Trial 96 finished with value: 0.8874747185019145 and parameters: {'learning_rate': 0.08037697271464296, 'depth': 7, 'subsample': 0.7784561210790285, 'random_strength': 3.591303661184966e-05, 'l2_leaf_reg': 2.8801237399259634e-08, 'model_size_reg': 27.698943986400202}. Best is trial 45 with value: 0.887769109898462.\n",
      "[I 2024-01-18 20:28:27,648] Trial 97 finished with value: 0.8873110154919579 and parameters: {'learning_rate': 0.059197647952717224, 'depth': 8, 'subsample': 0.9237382366520068, 'random_strength': 2.085578209597709e-05, 'l2_leaf_reg': 1.1698347669770406e-07, 'model_size_reg': 1.2464478194515634}. Best is trial 45 with value: 0.887769109898462.\n",
      "[I 2024-01-18 20:28:40,532] Trial 98 finished with value: 0.8877180747931765 and parameters: {'learning_rate': 0.0670744396209816, 'depth': 9, 'subsample': 0.9734857694681516, 'random_strength': 0.0016389091637509917, 'l2_leaf_reg': 1.6808155451054802e-08, 'model_size_reg': 70.21975713154988}. Best is trial 45 with value: 0.887769109898462.\n",
      "[I 2024-01-18 20:28:59,064] Trial 99 finished with value: 0.8872853686320403 and parameters: {'learning_rate': 0.06738595132442475, 'depth': 10, 'subsample': 0.965606958960331, 'random_strength': 0.0013975045142015227, 'l2_leaf_reg': 1.502276421355015e-08, 'model_size_reg': 58.27581727759718}. Best is trial 45 with value: 0.887769109898462.\n"
     ]
    }
   ],
   "source": [
    "study = create_study(study_name='optimization', direction='maximize', pruner=optuna.pruners.MedianPruner())\n",
    "\n",
    "study.optimize(lambda trial: objective(trial, selected_pipeline), n_trials=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:30:08,546] A new study created in memory with name: optimization\n",
      "[I 2024-01-18 20:30:15,461] Trial 0 finished with value: 0.8871009668457775 and parameters: {'learning_rate': 0.04517440998912212, 'max_depth': 9, 'subsample': 0.7999733666430413, 'colsample_bytree': 0.7124971916100009, 'min_child_weight': 5}. Best is trial 0 with value: 0.8871009668457775.\n",
      "[I 2024-01-18 20:30:22,168] Trial 1 finished with value: 0.8858596046098275 and parameters: {'learning_rate': 0.0054483651690426455, 'max_depth': 9, 'subsample': 0.7609990796586651, 'colsample_bytree': 0.9476358536424576, 'min_child_weight': 19}. Best is trial 0 with value: 0.8871009668457775.\n",
      "[I 2024-01-18 20:30:25,961] Trial 2 finished with value: 0.8739623298731651 and parameters: {'learning_rate': 0.012687186717719196, 'max_depth': 3, 'subsample': 0.8398316373102301, 'colsample_bytree': 0.6672220356974892, 'min_child_weight': 20}. Best is trial 0 with value: 0.8871009668457775.\n",
      "[I 2024-01-18 20:30:30,157] Trial 3 finished with value: 0.886489403206428 and parameters: {'learning_rate': 0.052672559602246476, 'max_depth': 4, 'subsample': 0.3152115610324938, 'colsample_bytree': 0.81187218743083, 'min_child_weight': 1}. Best is trial 0 with value: 0.8871009668457775.\n",
      "[I 2024-01-18 20:30:34,321] Trial 4 finished with value: 0.8832989443463892 and parameters: {'learning_rate': 0.042130341457747904, 'max_depth': 4, 'subsample': 0.7599018318532736, 'colsample_bytree': 0.28618721665520747, 'min_child_weight': 15}. Best is trial 0 with value: 0.8871009668457775.\n",
      "[I 2024-01-18 20:30:41,013] Trial 5 finished with value: 0.8856676381317794 and parameters: {'learning_rate': 0.006874486130089832, 'max_depth': 10, 'subsample': 0.43476540897041893, 'colsample_bytree': 0.5164098946943438, 'min_child_weight': 8}. Best is trial 0 with value: 0.8871009668457775.\n",
      "[I 2024-01-18 20:30:47,571] Trial 6 finished with value: 0.886139877694464 and parameters: {'learning_rate': 0.03358224976553689, 'max_depth': 9, 'subsample': 0.9195935654442158, 'colsample_bytree': 0.3355950256544925, 'min_child_weight': 15}. Best is trial 0 with value: 0.8871009668457775.\n",
      "[I 2024-01-18 20:30:51,271] Trial 7 finished with value: 0.8383219120774973 and parameters: {'learning_rate': 0.0034719047131249584, 'max_depth': 2, 'subsample': 0.6920683532570447, 'colsample_bytree': 0.9748558182819326, 'min_child_weight': 8}. Best is trial 0 with value: 0.8871009668457775.\n",
      "[I 2024-01-18 20:30:56,354] Trial 8 finished with value: 0.8850068086184104 and parameters: {'learning_rate': 0.006359763565133317, 'max_depth': 6, 'subsample': 0.7006062925321366, 'colsample_bytree': 0.7259952060764727, 'min_child_weight': 15}. Best is trial 0 with value: 0.8871009668457775.\n",
      "[I 2024-01-18 20:31:00,236] Trial 9 finished with value: 0.8653188027641939 and parameters: {'learning_rate': 0.0010796307605805645, 'max_depth': 3, 'subsample': 0.7463373976062506, 'colsample_bytree': 0.7945558072039305, 'min_child_weight': 15}. Best is trial 0 with value: 0.8871009668457775.\n",
      "[I 2024-01-18 20:31:05,974] Trial 10 finished with value: 0.8840283147792624 and parameters: {'learning_rate': 0.09295436679063761, 'max_depth': 7, 'subsample': 0.08792371464468102, 'colsample_bytree': 0.5088692492935976, 'min_child_weight': 2}. Best is trial 0 with value: 0.8871009668457775.\n",
      "[I 2024-01-18 20:31:10,280] Trial 11 finished with value: 0.8742770546993454 and parameters: {'learning_rate': 0.08957636260342361, 'max_depth': 6, 'subsample': 0.37766702418357473, 'colsample_bytree': 0.06022014527311542, 'min_child_weight': 2}. Best is trial 0 with value: 0.8871009668457775.\n",
      "[I 2024-01-18 20:31:15,149] Trial 12 finished with value: 0.8858081403144382 and parameters: {'learning_rate': 0.023809061321914626, 'max_depth': 5, 'subsample': 0.21840926194346585, 'colsample_bytree': 0.8264178815861575, 'min_child_weight': 5}. Best is trial 0 with value: 0.8871009668457775.\n",
      "[I 2024-01-18 20:31:21,360] Trial 13 finished with value: 0.8872647624000175 and parameters: {'learning_rate': 0.049284097206741095, 'max_depth': 8, 'subsample': 0.538292105392048, 'colsample_bytree': 0.5970576622423586, 'min_child_weight': 1}. Best is trial 13 with value: 0.8872647624000175.\n",
      "[I 2024-01-18 20:31:27,848] Trial 14 finished with value: 0.8866850772370768 and parameters: {'learning_rate': 0.017966665694132977, 'max_depth': 8, 'subsample': 0.5961252027854559, 'colsample_bytree': 0.604073048291386, 'min_child_weight': 5}. Best is trial 13 with value: 0.8872647624000175.\n",
      "[I 2024-01-18 20:31:33,963] Trial 15 finished with value: 0.8868149184546961 and parameters: {'learning_rate': 0.04784574137428282, 'max_depth': 8, 'subsample': 0.5631648841262937, 'colsample_bytree': 0.3752096713698645, 'min_child_weight': 5}. Best is trial 13 with value: 0.8872647624000175.\n",
      "[I 2024-01-18 20:31:41,149] Trial 16 finished with value: 0.8864569501199355 and parameters: {'learning_rate': 0.022833730949686673, 'max_depth': 10, 'subsample': 0.9427180270680205, 'colsample_bytree': 0.44944717847693805, 'min_child_weight': 10}. Best is trial 13 with value: 0.8872647624000175.\n",
      "[I 2024-01-18 20:31:47,759] Trial 17 finished with value: 0.8865898660689171 and parameters: {'learning_rate': 0.015042938081389387, 'max_depth': 8, 'subsample': 0.4715989763362785, 'colsample_bytree': 0.6123207988472571, 'min_child_weight': 4}. Best is trial 13 with value: 0.8872647624000175.\n",
      "[I 2024-01-18 20:31:53,049] Trial 18 finished with value: 0.8836195368493519 and parameters: {'learning_rate': 0.06673906483434724, 'max_depth': 7, 'subsample': 0.9952788496785117, 'colsample_bytree': 0.22352295848510234, 'min_child_weight': 8}. Best is trial 13 with value: 0.8872647624000175.\n",
      "[I 2024-01-18 20:32:00,500] Trial 19 finished with value: 0.8869330011624448 and parameters: {'learning_rate': 0.03176375623707627, 'max_depth': 9, 'subsample': 0.6151507769800783, 'colsample_bytree': 0.6926671608183145, 'min_child_weight': 3}. Best is trial 13 with value: 0.8872647624000175.\n",
      "[I 2024-01-18 20:32:06,741] Trial 20 finished with value: 0.8850884624853894 and parameters: {'learning_rate': 0.0031753997756638424, 'max_depth': 7, 'subsample': 0.8366506774359876, 'colsample_bytree': 0.8941915507853853, 'min_child_weight': 7}. Best is trial 13 with value: 0.8872647624000175.\n",
      "[I 2024-01-18 20:32:13,807] Trial 21 finished with value: 0.8868699688851782 and parameters: {'learning_rate': 0.028080300639252424, 'max_depth': 9, 'subsample': 0.5623186577646591, 'colsample_bytree': 0.716720198428329, 'min_child_weight': 3}. Best is trial 13 with value: 0.8872647624000175.\n",
      "[I 2024-01-18 20:32:21,505] Trial 22 finished with value: 0.8866448976118464 and parameters: {'learning_rate': 0.06292076068669031, 'max_depth': 10, 'subsample': 0.618965233460133, 'colsample_bytree': 0.6043296142844499, 'min_child_weight': 1}. Best is trial 13 with value: 0.8872647624000175.\n",
      "[I 2024-01-18 20:32:29,785] Trial 23 finished with value: 0.8868703233631169 and parameters: {'learning_rate': 0.035372431679422014, 'max_depth': 9, 'subsample': 0.4875686312624204, 'colsample_bytree': 0.6993972237595998, 'min_child_weight': 3}. Best is trial 13 with value: 0.8872647624000175.\n",
      "[I 2024-01-18 20:32:35,627] Trial 24 finished with value: 0.8852393543832537 and parameters: {'learning_rate': 0.011269087577118832, 'max_depth': 8, 'subsample': 0.6487289991278133, 'colsample_bytree': 0.4430662101865167, 'min_child_weight': 6}. Best is trial 13 with value: 0.8872647624000175.\n",
      "[I 2024-01-18 20:32:42,125] Trial 25 finished with value: 0.8864738416775373 and parameters: {'learning_rate': 0.019651327284883183, 'max_depth': 9, 'subsample': 0.39588736214814607, 'colsample_bytree': 0.5720520565825743, 'min_child_weight': 12}. Best is trial 13 with value: 0.8872647624000175.\n",
      "[I 2024-01-18 20:32:49,627] Trial 26 finished with value: 0.8857578988518395 and parameters: {'learning_rate': 0.07361290919220276, 'max_depth': 10, 'subsample': 0.8521090281962742, 'colsample_bytree': 0.7709983064541364, 'min_child_weight': 3}. Best is trial 13 with value: 0.8872647624000175.\n",
      "[I 2024-01-18 20:32:55,341] Trial 27 finished with value: 0.8869472288189523 and parameters: {'learning_rate': 0.04344905717966983, 'max_depth': 7, 'subsample': 0.27190222390318414, 'colsample_bytree': 0.8894422698757457, 'min_child_weight': 10}. Best is trial 13 with value: 0.8872647624000175.\n",
      "[I 2024-01-18 20:33:01,061] Trial 28 finished with value: 0.8870625934062002 and parameters: {'learning_rate': 0.04927574968603241, 'max_depth': 7, 'subsample': 0.25984108691113467, 'colsample_bytree': 0.8840693335067861, 'min_child_weight': 11}. Best is trial 13 with value: 0.8872647624000175.\n",
      "[I 2024-01-18 20:33:05,987] Trial 29 finished with value: 0.8818321672128167 and parameters: {'learning_rate': 0.008324061687813225, 'max_depth': 5, 'subsample': 0.14977695498533788, 'colsample_bytree': 0.8967621243485827, 'min_child_weight': 11}. Best is trial 13 with value: 0.8872647624000175.\n",
      "[I 2024-01-18 20:33:12,498] Trial 30 finished with value: 0.8867598668112517 and parameters: {'learning_rate': 0.05761591150531901, 'max_depth': 8, 'subsample': 0.33029250610832284, 'colsample_bytree': 0.9996523363506249, 'min_child_weight': 17}. Best is trial 13 with value: 0.8872647624000175.\n",
      "[I 2024-01-18 20:33:18,875] Trial 31 finished with value: 0.8873851223165383 and parameters: {'learning_rate': 0.04112960413346224, 'max_depth': 7, 'subsample': 0.24764301321487597, 'colsample_bytree': 0.9087035918469082, 'min_child_weight': 13}. Best is trial 31 with value: 0.8873851223165383.\n",
      "[I 2024-01-18 20:33:24,824] Trial 32 finished with value: 0.8871818512804788 and parameters: {'learning_rate': 0.042103259032654695, 'max_depth': 6, 'subsample': 0.21220973019539813, 'colsample_bytree': 0.8572414544101941, 'min_child_weight': 13}. Best is trial 31 with value: 0.8873851223165383.\n",
      "[I 2024-01-18 20:33:30,818] Trial 33 finished with value: 0.8859812540547272 and parameters: {'learning_rate': 0.027891011284253232, 'max_depth': 6, 'subsample': 0.06171032104254634, 'colsample_bytree': 0.8460812672117037, 'min_child_weight': 13}. Best is trial 31 with value: 0.8873851223165383.\n",
      "[I 2024-01-18 20:33:37,074] Trial 34 finished with value: 0.8866381499555888 and parameters: {'learning_rate': 0.040041215700936515, 'max_depth': 5, 'subsample': 0.14740704980659103, 'colsample_bytree': 0.7593927978826552, 'min_child_weight': 17}. Best is trial 31 with value: 0.8873851223165383.\n",
      "[I 2024-01-18 20:33:43,573] Trial 35 finished with value: 0.8870184043394268 and parameters: {'learning_rate': 0.07919766693803276, 'max_depth': 6, 'subsample': 0.20968967389893386, 'colsample_bytree': 0.9375495213970663, 'min_child_weight': 13}. Best is trial 31 with value: 0.8873851223165383.\n",
      "[I 2024-01-18 20:33:51,573] Trial 36 finished with value: 0.886394782659452 and parameters: {'learning_rate': 0.013224132441735792, 'max_depth': 8, 'subsample': 0.33465751003137845, 'colsample_bytree': 0.6539101671979839, 'min_child_weight': 13}. Best is trial 31 with value: 0.8873851223165383.\n",
      "[I 2024-01-18 20:33:57,021] Trial 37 finished with value: 0.8866670652632194 and parameters: {'learning_rate': 0.05624474720178839, 'max_depth': 4, 'subsample': 0.1533918954094869, 'colsample_bytree': 0.9462968514775248, 'min_child_weight': 17}. Best is trial 31 with value: 0.8873851223165383.\n",
      "[I 2024-01-18 20:34:05,090] Trial 38 finished with value: 0.8871271339325436 and parameters: {'learning_rate': 0.036258284014079024, 'max_depth': 7, 'subsample': 0.8066429623998228, 'colsample_bytree': 0.8390444731090813, 'min_child_weight': 9}. Best is trial 31 with value: 0.8873851223165383.\n",
      "[I 2024-01-18 20:34:11,927] Trial 39 finished with value: 0.8858646668520768 and parameters: {'learning_rate': 0.01654748459781441, 'max_depth': 6, 'subsample': 0.5077396089489375, 'colsample_bytree': 0.8622402003361082, 'min_child_weight': 14}. Best is trial 31 with value: 0.8873851223165383.\n",
      "[I 2024-01-18 20:34:19,040] Trial 40 finished with value: 0.8872045763462284 and parameters: {'learning_rate': 0.03874460823042574, 'max_depth': 7, 'subsample': 0.38581638593840994, 'colsample_bytree': 0.8069782243984077, 'min_child_weight': 20}. Best is trial 31 with value: 0.8873851223165383.\n",
      "[I 2024-01-18 20:34:26,263] Trial 41 finished with value: 0.8870080017762747 and parameters: {'learning_rate': 0.03580347192892515, 'max_depth': 7, 'subsample': 0.39063235219371073, 'colsample_bytree': 0.793774037102206, 'min_child_weight': 20}. Best is trial 31 with value: 0.8873851223165383.\n",
      "[I 2024-01-18 20:34:33,829] Trial 42 finished with value: 0.886708460098482 and parameters: {'learning_rate': 0.02392923038464878, 'max_depth': 7, 'subsample': 0.2685793931507383, 'colsample_bytree': 0.9408609527061039, 'min_child_weight': 18}. Best is trial 31 with value: 0.8873851223165383.\n",
      "[I 2024-01-18 20:34:40,561] Trial 43 finished with value: 0.8868192791842447 and parameters: {'learning_rate': 0.0992364873128722, 'max_depth': 6, 'subsample': 0.20061838745581087, 'colsample_bytree': 0.7431502932102824, 'min_child_weight': 19}. Best is trial 31 with value: 0.8873851223165383.\n",
      "[I 2024-01-18 20:34:47,152] Trial 44 finished with value: 0.8869008858307663 and parameters: {'learning_rate': 0.04436446187514493, 'max_depth': 5, 'subsample': 0.44696610350835875, 'colsample_bytree': 0.8266361709858326, 'min_child_weight': 9}. Best is trial 31 with value: 0.8873851223165383.\n",
      "[I 2024-01-18 20:34:54,198] Trial 45 finished with value: 0.8855348837792817 and parameters: {'learning_rate': 0.0011181668267727228, 'max_depth': 7, 'subsample': 0.35444674762862943, 'colsample_bytree': 0.6524740618038429, 'min_child_weight': 16}. Best is trial 31 with value: 0.8873851223165383.\n",
      "[I 2024-01-18 20:35:01,904] Trial 46 finished with value: 0.886718399935491 and parameters: {'learning_rate': 0.03267948365467451, 'max_depth': 8, 'subsample': 0.28590928794327947, 'colsample_bytree': 0.7931291250489059, 'min_child_weight': 12}. Best is trial 31 with value: 0.8873851223165383.\n",
      "[I 2024-01-18 20:35:08,394] Trial 47 finished with value: 0.8876642695061113 and parameters: {'learning_rate': 0.07654186370277757, 'max_depth': 6, 'subsample': 0.778239580497082, 'colsample_bytree': 0.8486780423493341, 'min_child_weight': 14}. Best is trial 47 with value: 0.8876642695061113.\n",
      "[I 2024-01-18 20:35:13,921] Trial 48 finished with value: 0.8727818303596888 and parameters: {'learning_rate': 0.004348381214986539, 'max_depth': 4, 'subsample': 0.4168373195767489, 'colsample_bytree': 0.9802114920518051, 'min_child_weight': 14}. Best is trial 47 with value: 0.8876642695061113.\n",
      "[I 2024-01-18 20:35:19,654] Trial 49 finished with value: 0.8839917846797853 and parameters: {'learning_rate': 0.0789417244252885, 'max_depth': 5, 'subsample': 0.7096521301459424, 'colsample_bytree': 0.21511787532137222, 'min_child_weight': 16}. Best is trial 47 with value: 0.8876642695061113.\n",
      "[I 2024-01-18 20:35:26,439] Trial 50 finished with value: 0.8873861677796115 and parameters: {'learning_rate': 0.05545156695374457, 'max_depth': 6, 'subsample': 0.5410389578003794, 'colsample_bytree': 0.9266062960811353, 'min_child_weight': 14}. Best is trial 47 with value: 0.8876642695061113.\n",
      "[I 2024-01-18 20:35:33,285] Trial 51 finished with value: 0.8872719701973913 and parameters: {'learning_rate': 0.053611948809342544, 'max_depth': 6, 'subsample': 0.6552546889796955, 'colsample_bytree': 0.9114727725035203, 'min_child_weight': 14}. Best is trial 47 with value: 0.8876642695061113.\n",
      "[I 2024-01-18 20:35:39,763] Trial 52 finished with value: 0.887327000072104 and parameters: {'learning_rate': 0.05607140795813108, 'max_depth': 6, 'subsample': 0.671052290712885, 'colsample_bytree': 0.9239867158677945, 'min_child_weight': 14}. Best is trial 47 with value: 0.8876642695061113.\n",
      "[I 2024-01-18 20:35:46,005] Trial 53 finished with value: 0.8873582759253054 and parameters: {'learning_rate': 0.05588947088239277, 'max_depth': 6, 'subsample': 0.6747070689957152, 'colsample_bytree': 0.9382830717199588, 'min_child_weight': 14}. Best is trial 47 with value: 0.8876642695061113.\n",
      "[I 2024-01-18 20:35:51,380] Trial 54 finished with value: 0.8873419775550109 and parameters: {'learning_rate': 0.06756859859657453, 'max_depth': 5, 'subsample': 0.6690477161623716, 'colsample_bytree': 0.9205375561142798, 'min_child_weight': 14}. Best is trial 47 with value: 0.8876642695061113.\n",
      "[I 2024-01-18 20:35:56,770] Trial 55 finished with value: 0.8873996817654424 and parameters: {'learning_rate': 0.06804611888041716, 'max_depth': 5, 'subsample': 0.7132063812397857, 'colsample_bytree': 0.9771169402279251, 'min_child_weight': 15}. Best is trial 47 with value: 0.8876642695061113.\n",
      "[I 2024-01-18 20:36:01,624] Trial 56 finished with value: 0.8872392472031992 and parameters: {'learning_rate': 0.07125379541494685, 'max_depth': 4, 'subsample': 0.7400036313132197, 'colsample_bytree': 0.9735720234428293, 'min_child_weight': 15}. Best is trial 47 with value: 0.8876642695061113.\n",
      "[I 2024-01-18 20:36:07,024] Trial 57 finished with value: 0.8876244731060217 and parameters: {'learning_rate': 0.08522003441488495, 'max_depth': 5, 'subsample': 0.7840280809429998, 'colsample_bytree': 0.9682630928900665, 'min_child_weight': 12}. Best is trial 47 with value: 0.8876642695061113.\n",
      "[I 2024-01-18 20:36:11,457] Trial 58 finished with value: 0.8866191885569026 and parameters: {'learning_rate': 0.08333627146750988, 'max_depth': 3, 'subsample': 0.7883474069186023, 'colsample_bytree': 0.9948551221885448, 'min_child_weight': 12}. Best is trial 47 with value: 0.8876642695061113.\n",
      "[I 2024-01-18 20:36:16,900] Trial 59 finished with value: 0.8874819487044313 and parameters: {'learning_rate': 0.09415207886221551, 'max_depth': 5, 'subsample': 0.8980883992982558, 'colsample_bytree': 0.9621496460899943, 'min_child_weight': 16}. Best is trial 47 with value: 0.8876642695061113.\n",
      "[I 2024-01-18 20:36:21,725] Trial 60 finished with value: 0.8874532865089132 and parameters: {'learning_rate': 0.09152382255744744, 'max_depth': 4, 'subsample': 0.8865433202709505, 'colsample_bytree': 0.9633540722246631, 'min_child_weight': 16}. Best is trial 47 with value: 0.8876642695061113.\n",
      "[I 2024-01-18 20:36:26,651] Trial 61 finished with value: 0.8873459504659156 and parameters: {'learning_rate': 0.0967085238247707, 'max_depth': 4, 'subsample': 0.8843807989742962, 'colsample_bytree': 0.9591037240468189, 'min_child_weight': 16}. Best is trial 47 with value: 0.8876642695061113.\n",
      "[I 2024-01-18 20:36:31,842] Trial 62 finished with value: 0.8876157545454312 and parameters: {'learning_rate': 0.08531022470610022, 'max_depth': 5, 'subsample': 0.9604761842806293, 'colsample_bytree': 0.8755791554843121, 'min_child_weight': 16}. Best is trial 47 with value: 0.8876642695061113.\n",
      "[I 2024-01-18 20:36:37,195] Trial 63 finished with value: 0.8876307731241244 and parameters: {'learning_rate': 0.08880764168157719, 'max_depth': 5, 'subsample': 0.9371905385594792, 'colsample_bytree': 0.8753682773186513, 'min_child_weight': 15}. Best is trial 47 with value: 0.8876642695061113.\n",
      "[I 2024-01-18 20:36:42,531] Trial 64 finished with value: 0.887518237277947 and parameters: {'learning_rate': 0.08465441025740587, 'max_depth': 5, 'subsample': 0.9599460568368862, 'colsample_bytree': 0.8732877947030439, 'min_child_weight': 18}. Best is trial 47 with value: 0.8876642695061113.\n",
      "[I 2024-01-18 20:36:47,024] Trial 65 finished with value: 0.8866711865887029 and parameters: {'learning_rate': 0.08867126189858802, 'max_depth': 3, 'subsample': 0.9978883060899143, 'colsample_bytree': 0.8754530877821893, 'min_child_weight': 18}. Best is trial 47 with value: 0.8876642695061113.\n",
      "[I 2024-01-18 20:36:52,054] Trial 66 finished with value: 0.887393816167358 and parameters: {'learning_rate': 0.09664915960294501, 'max_depth': 4, 'subsample': 0.9416691385141264, 'colsample_bytree': 0.8588631543098345, 'min_child_weight': 18}. Best is trial 47 with value: 0.8876642695061113.\n",
      "[I 2024-01-18 20:36:57,594] Trial 67 finished with value: 0.8876177014583185 and parameters: {'learning_rate': 0.08027851444129218, 'max_depth': 5, 'subsample': 0.884090766028046, 'colsample_bytree': 0.8837813809788688, 'min_child_weight': 16}. Best is trial 47 with value: 0.8876642695061113.\n",
      "[I 2024-01-18 20:37:03,032] Trial 68 finished with value: 0.8875202556484805 and parameters: {'learning_rate': 0.07850003162690423, 'max_depth': 5, 'subsample': 0.9601212602538196, 'colsample_bytree': 0.8176911943046142, 'min_child_weight': 19}. Best is trial 47 with value: 0.8876642695061113.\n",
      "[I 2024-01-18 20:37:08,784] Trial 69 finished with value: 0.8873422201175034 and parameters: {'learning_rate': 0.06381633655991238, 'max_depth': 5, 'subsample': 0.9612476853330619, 'colsample_bytree': 0.7725671645989346, 'min_child_weight': 19}. Best is trial 47 with value: 0.8876642695061113.\n",
      "[I 2024-01-18 20:37:14,207] Trial 70 finished with value: 0.8875750674546588 and parameters: {'learning_rate': 0.07932414303151222, 'max_depth': 5, 'subsample': 0.8493547592234707, 'colsample_bytree': 0.8266254450095454, 'min_child_weight': 17}. Best is trial 47 with value: 0.8876642695061113.\n",
      "[I 2024-01-18 20:37:19,345] Trial 71 finished with value: 0.8875836282419589 and parameters: {'learning_rate': 0.08185678870322004, 'max_depth': 5, 'subsample': 0.8501931897687697, 'colsample_bytree': 0.8283063116764185, 'min_child_weight': 17}. Best is trial 47 with value: 0.8876642695061113.\n",
      "[I 2024-01-18 20:37:24,462] Trial 72 finished with value: 0.8874114037541263 and parameters: {'learning_rate': 0.07510275873941881, 'max_depth': 5, 'subsample': 0.8520296939709785, 'colsample_bytree': 0.8216934906604297, 'min_child_weight': 17}. Best is trial 47 with value: 0.8876642695061113.\n",
      "[I 2024-01-18 20:37:29,592] Trial 73 finished with value: 0.8875069799718984 and parameters: {'learning_rate': 0.07493936047494014, 'max_depth': 5, 'subsample': 0.809804126698369, 'colsample_bytree': 0.775855378328621, 'min_child_weight': 15}. Best is trial 47 with value: 0.8876642695061113.\n",
      "[I 2024-01-18 20:37:34,435] Trial 74 finished with value: 0.8869084186673308 and parameters: {'learning_rate': 0.06279322798318389, 'max_depth': 4, 'subsample': 0.9183033810975517, 'colsample_bytree': 0.7349292830619695, 'min_child_weight': 17}. Best is trial 47 with value: 0.8876642695061113.\n",
      "[I 2024-01-18 20:37:39,820] Trial 75 finished with value: 0.8875519535164056 and parameters: {'learning_rate': 0.08015484100005131, 'max_depth': 5, 'subsample': 0.7757266010305349, 'colsample_bytree': 0.8328488290364692, 'min_child_weight': 19}. Best is trial 47 with value: 0.8876642695061113.\n",
      "[I 2024-01-18 20:37:45,105] Trial 76 finished with value: 0.8757870415593694 and parameters: {'learning_rate': 0.0015649778038341836, 'max_depth': 5, 'subsample': 0.7724059220024238, 'colsample_bytree': 0.897143204750841, 'min_child_weight': 17}. Best is trial 47 with value: 0.8876642695061113.\n",
      "[I 2024-01-18 20:37:49,823] Trial 77 finished with value: 0.8870057952989465 and parameters: {'learning_rate': 0.0634512104060552, 'max_depth': 4, 'subsample': 0.8302738881123758, 'colsample_bytree': 0.6866179685174723, 'min_child_weight': 18}. Best is trial 47 with value: 0.8876642695061113.\n",
      "[I 2024-01-18 20:37:53,895] Trial 78 finished with value: 0.8810325310251622 and parameters: {'learning_rate': 0.04797450584953525, 'max_depth': 2, 'subsample': 0.8753697606243488, 'colsample_bytree': 0.8431095679098793, 'min_child_weight': 15}. Best is trial 47 with value: 0.8876642695061113.\n",
      "[I 2024-01-18 20:37:59,251] Trial 79 finished with value: 0.8875343646757179 and parameters: {'learning_rate': 0.08241421384263341, 'max_depth': 5, 'subsample': 0.7438625594891044, 'colsample_bytree': 0.5403158328249728, 'min_child_weight': 16}. Best is trial 47 with value: 0.8876642695061113.\n",
      "[I 2024-01-18 20:38:03,887] Trial 80 finished with value: 0.8853625474453153 and parameters: {'learning_rate': 0.04952976983611058, 'max_depth': 3, 'subsample': 0.7803562088751999, 'colsample_bytree': 0.8004139643680214, 'min_child_weight': 20}. Best is trial 47 with value: 0.8876642695061113.\n",
      "[I 2024-01-18 20:38:09,734] Trial 81 finished with value: 0.8874636504163804 and parameters: {'learning_rate': 0.0865003390612171, 'max_depth': 5, 'subsample': 0.8282682908012035, 'colsample_bytree': 0.37064583301065046, 'min_child_weight': 16}. Best is trial 47 with value: 0.8876642695061113.\n",
      "[I 2024-01-18 20:38:15,594] Trial 82 finished with value: 0.8874161285503156 and parameters: {'learning_rate': 0.07043705216371896, 'max_depth': 5, 'subsample': 0.7401675291029279, 'colsample_bytree': 0.888977026330513, 'min_child_weight': 17}. Best is trial 47 with value: 0.8876642695061113.\n",
      "[I 2024-01-18 20:38:21,952] Trial 83 finished with value: 0.8878230941525738 and parameters: {'learning_rate': 0.08302321075792173, 'max_depth': 6, 'subsample': 0.9178118324416605, 'colsample_bytree': 0.49556396438908396, 'min_child_weight': 16}. Best is trial 83 with value: 0.8878230941525738.\n",
      "[I 2024-01-18 20:38:29,243] Trial 84 finished with value: 0.887522777642459 and parameters: {'learning_rate': 0.06074720602071648, 'max_depth': 6, 'subsample': 0.9201366396575816, 'colsample_bytree': 0.4795255810357013, 'min_child_weight': 15}. Best is trial 83 with value: 0.8878230941525738.\n",
      "[I 2024-01-18 20:38:35,730] Trial 85 finished with value: 0.8876312521023848 and parameters: {'learning_rate': 0.07251571711448125, 'max_depth': 6, 'subsample': 0.8517395754000394, 'colsample_bytree': 0.4378182268820028, 'min_child_weight': 19}. Best is trial 83 with value: 0.8878230941525738.\n",
      "[I 2024-01-18 20:38:42,740] Trial 86 finished with value: 0.8878720522287928 and parameters: {'learning_rate': 0.09964753039423867, 'max_depth': 6, 'subsample': 0.9118201281141237, 'colsample_bytree': 0.41536036442118945, 'min_child_weight': 12}. Best is trial 86 with value: 0.8878720522287928.\n",
      "[I 2024-01-18 20:38:50,089] Trial 87 finished with value: 0.8875163664492896 and parameters: {'learning_rate': 0.07111080534547762, 'max_depth': 6, 'subsample': 0.9168643814389016, 'colsample_bytree': 0.4115709128541102, 'min_child_weight': 11}. Best is trial 86 with value: 0.8878720522287928.\n",
      "[I 2024-01-18 20:38:55,987] Trial 88 finished with value: 0.8865982959839063 and parameters: {'learning_rate': 0.09858460141471932, 'max_depth': 6, 'subsample': 0.8637224263397829, 'colsample_bytree': 0.31835724990200165, 'min_child_weight': 13}. Best is trial 86 with value: 0.8878720522287928.\n",
      "[I 2024-01-18 20:39:02,210] Trial 89 finished with value: 0.8853138288661027 and parameters: {'learning_rate': 0.00883307106463675, 'max_depth': 6, 'subsample': 0.9786972764277778, 'colsample_bytree': 0.4884039022590136, 'min_child_weight': 12}. Best is trial 86 with value: 0.8878720522287928.\n",
      "[I 2024-01-18 20:39:08,739] Trial 90 finished with value: 0.8879057538770502 and parameters: {'learning_rate': 0.0995227238765534, 'max_depth': 6, 'subsample': 0.8974781754027489, 'colsample_bytree': 0.41327513349222667, 'min_child_weight': 11}. Best is trial 90 with value: 0.8879057538770502.\n",
      "[I 2024-01-18 20:39:15,483] Trial 91 finished with value: 0.8878709655139808 and parameters: {'learning_rate': 0.09940835910383082, 'max_depth': 6, 'subsample': 0.9051461084803883, 'colsample_bytree': 0.42124263456590894, 'min_child_weight': 11}. Best is trial 90 with value: 0.8879057538770502.\n",
      "[I 2024-01-18 20:39:21,445] Trial 92 finished with value: 0.8876105052990799 and parameters: {'learning_rate': 0.08872659977313642, 'max_depth': 6, 'subsample': 0.9368352682520953, 'colsample_bytree': 0.4282329540056901, 'min_child_weight': 10}. Best is trial 90 with value: 0.8879057538770502.\n",
      "[I 2024-01-18 20:39:27,500] Trial 93 finished with value: 0.8878462341317057 and parameters: {'learning_rate': 0.09893954820575226, 'max_depth': 6, 'subsample': 0.9005184191599038, 'colsample_bytree': 0.39630008002975375, 'min_child_weight': 11}. Best is trial 90 with value: 0.8879057538770502.\n",
      "[I 2024-01-18 20:39:33,654] Trial 94 finished with value: 0.8872212000345372 and parameters: {'learning_rate': 0.06820310426795585, 'max_depth': 6, 'subsample': 0.816573581987059, 'colsample_bytree': 0.39603148042219, 'min_child_weight': 11}. Best is trial 90 with value: 0.8879057538770502.\n",
      "[I 2024-01-18 20:39:39,968] Trial 95 finished with value: 0.887885176819129 and parameters: {'learning_rate': 0.09544976781146368, 'max_depth': 7, 'subsample': 0.9017662707691348, 'colsample_bytree': 0.4600705925551841, 'min_child_weight': 12}. Best is trial 90 with value: 0.8879057538770502.\n",
      "[I 2024-01-18 20:39:46,298] Trial 96 finished with value: 0.8877672590379964 and parameters: {'learning_rate': 0.09734220213582577, 'max_depth': 7, 'subsample': 0.9089034651806117, 'colsample_bytree': 0.35679379484525825, 'min_child_weight': 9}. Best is trial 90 with value: 0.8879057538770502.\n",
      "[I 2024-01-18 20:39:52,963] Trial 97 finished with value: 0.8878353704712452 and parameters: {'learning_rate': 0.09999181743634358, 'max_depth': 7, 'subsample': 0.8993560795725113, 'colsample_bytree': 0.3499384457225807, 'min_child_weight': 9}. Best is trial 90 with value: 0.8879057538770502.\n",
      "[I 2024-01-18 20:39:59,692] Trial 98 finished with value: 0.8876854548472048 and parameters: {'learning_rate': 0.09970595552338653, 'max_depth': 7, 'subsample': 0.9106675763794607, 'colsample_bytree': 0.3554315548656403, 'min_child_weight': 7}. Best is trial 90 with value: 0.8879057538770502.\n",
      "[I 2024-01-18 20:40:06,011] Trial 99 finished with value: 0.885001267686176 and parameters: {'learning_rate': 0.09925244370371236, 'max_depth': 7, 'subsample': 0.9006554820384632, 'colsample_bytree': 0.2584314583126988, 'min_child_weight': 9}. Best is trial 90 with value: 0.8879057538770502.\n"
     ]
    }
   ],
   "source": [
    "study1 = create_study(study_name='optimization', direction='maximize', pruner=optuna.pruners.MedianPruner())\n",
    "\n",
    "study1.optimize(lambda trial: objective(trial, selected_pipeline, 'xgb'), n_trials=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:40:35,664] A new study created in memory with name: optimization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001042 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000825 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000827 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000898 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000878 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000877 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000891 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000889 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000889 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001116 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:40:43,392] Trial 0 finished with value: 0.887474586003919 and parameters: {'learning_rate': 0.06583214147746945, 'max_depth': 8, 'subsample': 0.2265368491029887, 'colsample_bytree': 0.5677145103125882, 'min_child_weight': 16}. Best is trial 0 with value: 0.887474586003919.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000600 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000595 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000595 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000589 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000609 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000593 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000653 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000615 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000595 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:40:46,955] Trial 1 finished with value: 0.8657032045661239 and parameters: {'learning_rate': 0.007501683499919533, 'max_depth': 2, 'subsample': 0.33050503838879847, 'colsample_bytree': 0.11589954736887258, 'min_child_weight': 14}. Best is trial 0 with value: 0.887474586003919.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000605 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000780 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000792 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000772 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000903 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000768 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000863 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000830 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000762 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000789 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000761 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:40:54,904] Trial 2 finished with value: 0.883541532352174 and parameters: {'learning_rate': 0.009782671520752446, 'max_depth': 8, 'subsample': 0.9751861596552789, 'colsample_bytree': 0.37334631749486497, 'min_child_weight': 6}. Best is trial 0 with value: 0.887474586003919.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002995 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000836 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000928 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000988 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000880 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000995 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001104 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000952 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001019 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000876 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:41:02,640] Trial 3 finished with value: 0.8841190461538272 and parameters: {'learning_rate': 0.007339257744205445, 'max_depth': 10, 'subsample': 0.809473600598107, 'colsample_bytree': 0.6693884120664638, 'min_child_weight': 8}. Best is trial 0 with value: 0.887474586003919.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000773 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000981 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000789 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000962 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000759 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000911 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000758 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000752 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000759 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000788 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:41:08,083] Trial 4 finished with value: 0.8789526757824004 and parameters: {'learning_rate': 0.0018567127934036567, 'max_depth': 4, 'subsample': 0.06665254006536923, 'colsample_bytree': 0.405860213063715, 'min_child_weight': 6}. Best is trial 0 with value: 0.887474586003919.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000731 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000627 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000613 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000624 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000635 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000616 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000618 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000631 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000623 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:41:11,979] Trial 5 finished with value: 0.860445988453812 and parameters: {'learning_rate': 0.05263651613995823, 'max_depth': 4, 'subsample': 0.8586319600751107, 'colsample_bytree': 0.09440178242085766, 'min_child_weight': 1}. Best is trial 0 with value: 0.887474586003919.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000626 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000837 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000991 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000872 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000978 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000939 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000878 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000889 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000874 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000868 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:41:15,512] Trial 6 finished with value: 0.8625776891984316 and parameters: {'learning_rate': 0.010614981645045474, 'max_depth': 2, 'subsample': 0.4204450099886841, 'colsample_bytree': 0.671724419310123, 'min_child_weight': 20}. Best is trial 0 with value: 0.887474586003919.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000898 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000787 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000775 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000778 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000785 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000767 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000762 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000808 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000759 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000766 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000832 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:41:22,736] Trial 7 finished with value: 0.883411020043574 and parameters: {'learning_rate': 0.003807851027060417, 'max_depth': 6, 'subsample': 0.7630988167613285, 'colsample_bytree': 0.4935849691337091, 'min_child_weight': 8}. Best is trial 0 with value: 0.887474586003919.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000834 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000836 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000829 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000816 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000875 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000867 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000862 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000877 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000872 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000889 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:41:30,272] Trial 8 finished with value: 0.8866508055612121 and parameters: {'learning_rate': 0.034977344589329634, 'max_depth': 9, 'subsample': 0.9328523892509223, 'colsample_bytree': 0.6001189991909757, 'min_child_weight': 17}. Best is trial 0 with value: 0.887474586003919.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000819 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000835 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000820 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000825 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000886 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000968 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000875 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000870 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000878 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000902 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:41:34,790] Trial 9 finished with value: 0.8715246976235296 and parameters: {'learning_rate': 0.004097810666664721, 'max_depth': 3, 'subsample': 0.9131425001721818, 'colsample_bytree': 0.5603438517502332, 'min_child_weight': 16}. Best is trial 0 with value: 0.887474586003919.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001049 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000851 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001054 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001170 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001021 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001034 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000991 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001022 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001037 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001004 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:41:42,360] Trial 10 finished with value: 0.8876548546186269 and parameters: {'learning_rate': 0.09884489851538067, 'max_depth': 7, 'subsample': 0.07694101033510714, 'colsample_bytree': 0.9658066021806486, 'min_child_weight': 12}. Best is trial 10 with value: 0.8876548546186269.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001205 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000958 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001253 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001179 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001114 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001127 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001223 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001209 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001179 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001369 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:41:49,877] Trial 11 finished with value: 0.8876722800843448 and parameters: {'learning_rate': 0.09593117364042116, 'max_depth': 7, 'subsample': 0.12605156233362413, 'colsample_bytree': 0.9966919303809116, 'min_child_weight': 12}. Best is trial 11 with value: 0.8876722800843448.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001131 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001061 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001090 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001287 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001160 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001082 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001141 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001211 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001073 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001180 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:41:57,664] Trial 12 finished with value: 0.8875979878907985 and parameters: {'learning_rate': 0.09617439797966353, 'max_depth': 6, 'subsample': 0.09811476369217953, 'colsample_bytree': 0.970547014762772, 'min_child_weight': 12}. Best is trial 11 with value: 0.8876722800843448.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001354 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001055 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001276 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001127 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001113 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001115 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001215 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001087 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001179 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001151 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:42:05,933] Trial 13 finished with value: 0.885880110068328 and parameters: {'learning_rate': 0.023792921113806948, 'max_depth': 7, 'subsample': 0.6184472786674883, 'colsample_bytree': 0.9972489657610252, 'min_child_weight': 11}. Best is trial 11 with value: 0.8876722800843448.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001066 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001271 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001098 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001171 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001062 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001014 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000994 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001022 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001114 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001383 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:42:12,850] Trial 14 finished with value: 0.8849520918265474 and parameters: {'learning_rate': 0.021995889124918074, 'max_depth': 5, 'subsample': 0.21321888573653988, 'colsample_bytree': 0.8525603011302473, 'min_child_weight': 12}. Best is trial 11 with value: 0.8876722800843448.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001043 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000948 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001039 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000851 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000970 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001034 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001365 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001386 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000981 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001086 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:42:20,454] Trial 15 finished with value: 0.887927758797741 and parameters: {'learning_rate': 0.0998967279943594, 'max_depth': 7, 'subsample': 0.5231345460679563, 'colsample_bytree': 0.7963988397680257, 'min_child_weight': 9}. Best is trial 15 with value: 0.887927758797741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000850 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001114 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001313 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000990 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001169 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001051 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001035 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001001 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001047 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001023 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:42:28,048] Trial 16 finished with value: 0.8868928833510239 and parameters: {'learning_rate': 0.04098622684754784, 'max_depth': 8, 'subsample': 0.5709937216574551, 'colsample_bytree': 0.8141890119371811, 'min_child_weight': 2}. Best is trial 15 with value: 0.887927758797741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001067 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001113 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001195 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001106 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001241 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001165 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001000 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001148 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001648 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001304 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:42:36,796] Trial 17 finished with value: 0.8851148167164572 and parameters: {'learning_rate': 0.018144485045733075, 'max_depth': 10, 'subsample': 0.4504151770683743, 'colsample_bytree': 0.8432412843001219, 'min_child_weight': 9}. Best is trial 15 with value: 0.887927758797741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001302 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001057 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001160 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001815 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001174 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001084 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001069 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001107 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001004 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001140 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:42:45,819] Trial 18 finished with value: 0.8875489966786597 and parameters: {'learning_rate': 0.0649572863837167, 'max_depth': 7, 'subsample': 0.7040144699818839, 'colsample_bytree': 0.7498518366327611, 'min_child_weight': 4}. Best is trial 15 with value: 0.887927758797741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001262 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001126 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001147 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001254 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001150 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001021 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001059 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001184 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001087 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001068 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:42:53,378] Trial 19 finished with value: 0.8745372395267271 and parameters: {'learning_rate': 0.001292874079755396, 'max_depth': 5, 'subsample': 0.3415597866617134, 'colsample_bytree': 0.8873376657663161, 'min_child_weight': 14}. Best is trial 15 with value: 0.887927758797741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000921 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001291 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001287 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001237 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001168 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004879 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001086 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001255 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001105 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001068 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:43:02,559] Trial 20 finished with value: 0.8863700381992399 and parameters: {'learning_rate': 0.029458481686709143, 'max_depth': 9, 'subsample': 0.6755825769686474, 'colsample_bytree': 0.7533016322696637, 'min_child_weight': 10}. Best is trial 15 with value: 0.887927758797741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001068 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001043 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000951 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001050 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001052 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000932 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001018 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001081 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001064 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001149 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:43:09,886] Trial 21 finished with value: 0.8875220763565981 and parameters: {'learning_rate': 0.09972025801064743, 'max_depth': 7, 'subsample': 0.17247673653096857, 'colsample_bytree': 0.9373744396766709, 'min_child_weight': 13}. Best is trial 15 with value: 0.887927758797741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001091 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001076 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001153 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001077 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001046 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001051 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001112 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001062 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000939 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001014 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:43:16,946] Trial 22 finished with value: 0.8874768040989038 and parameters: {'learning_rate': 0.09965076865842827, 'max_depth': 6, 'subsample': 0.3054986250202322, 'colsample_bytree': 0.9179271622440663, 'min_child_weight': 6}. Best is trial 15 with value: 0.887927758797741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001071 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001163 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001151 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001091 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001062 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001054 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001009 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001016 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001011 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001139 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:43:25,031] Trial 23 finished with value: 0.8872352198759785 and parameters: {'learning_rate': 0.05339148133350922, 'max_depth': 7, 'subsample': 0.14033337174679195, 'colsample_bytree': 0.7709049383322532, 'min_child_weight': 10}. Best is trial 15 with value: 0.887927758797741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001130 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001295 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001360 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001204 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001323 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001202 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001420 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001395 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001637 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001251 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:43:34,304] Trial 24 finished with value: 0.8874778121696631 and parameters: {'learning_rate': 0.06567414993472484, 'max_depth': 9, 'subsample': 0.05159937274589571, 'colsample_bytree': 0.9787762491358539, 'min_child_weight': 8}. Best is trial 15 with value: 0.887927758797741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000902 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000921 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001175 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001037 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000931 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000928 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000909 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000877 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000935 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001133 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:43:41,875] Trial 25 finished with value: 0.886975141221152 and parameters: {'learning_rate': 0.044983089136244825, 'max_depth': 5, 'subsample': 0.4921615788280834, 'colsample_bytree': 0.6871731010448221, 'min_child_weight': 14}. Best is trial 15 with value: 0.887927758797741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001132 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001086 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001084 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001142 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001013 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001025 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001052 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001099 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001053 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001035 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:43:49,657] Trial 26 finished with value: 0.887371913400131 and parameters: {'learning_rate': 0.07397874556575508, 'max_depth': 8, 'subsample': 0.18486505570925776, 'colsample_bytree': 0.871061832871421, 'min_child_weight': 11}. Best is trial 15 with value: 0.887927758797741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000760 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000759 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000791 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000768 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000771 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000772 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000771 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000767 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000777 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000761 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:43:57,080] Trial 27 finished with value: 0.8802115783956883 and parameters: {'learning_rate': 0.03200773744774122, 'max_depth': 6, 'subsample': 0.2702665757049142, 'colsample_bytree': 0.26760022532217004, 'min_child_weight': 18}. Best is trial 15 with value: 0.887927758797741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001080 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001091 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001149 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001048 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001074 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001049 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001044 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001125 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001028 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001059 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:44:04,775] Trial 28 finished with value: 0.8847174527096335 and parameters: {'learning_rate': 0.01567856903119456, 'max_depth': 7, 'subsample': 0.39527316333194573, 'colsample_bytree': 0.9278137319146267, 'min_child_weight': 13}. Best is trial 15 with value: 0.887927758797741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001043 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001079 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001127 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001142 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001027 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001010 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000998 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001055 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001004 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001053 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:44:12,376] Trial 29 finished with value: 0.8874854843708488 and parameters: {'learning_rate': 0.07081035240116926, 'max_depth': 8, 'subsample': 0.25107711195700233, 'colsample_bytree': 0.8042547624817022, 'min_child_weight': 16}. Best is trial 15 with value: 0.887927758797741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001158 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001217 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001073 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001185 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001264 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001131 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001134 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001132 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001153 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001126 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:44:19,457] Trial 30 finished with value: 0.8874368173551281 and parameters: {'learning_rate': 0.07815561088217947, 'max_depth': 6, 'subsample': 0.12483723146483278, 'colsample_bytree': 0.9995050836493553, 'min_child_weight': 9}. Best is trial 15 with value: 0.887927758797741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001078 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001036 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001055 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001047 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001069 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001037 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001113 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001036 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000988 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001037 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:44:26,347] Trial 31 finished with value: 0.8875476379830856 and parameters: {'learning_rate': 0.09631471414753959, 'max_depth': 6, 'subsample': 0.10323854764769744, 'colsample_bytree': 0.9448662971119046, 'min_child_weight': 12}. Best is trial 15 with value: 0.887927758797741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001062 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001057 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001079 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001030 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001000 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001028 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001012 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001118 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000982 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001034 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:44:33,803] Trial 32 finished with value: 0.8872295005122914 and parameters: {'learning_rate': 0.05300299246713188, 'max_depth': 7, 'subsample': 0.0505947500051207, 'colsample_bytree': 0.8953420038025537, 'min_child_weight': 15}. Best is trial 15 with value: 0.887927758797741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001079 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001095 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001067 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001073 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001105 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001239 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001100 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001207 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001143 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001199 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:44:40,163] Trial 33 finished with value: 0.8874642880201551 and parameters: {'learning_rate': 0.0846716251906807, 'max_depth': 5, 'subsample': 0.20386793084155574, 'colsample_bytree': 0.994368726892506, 'min_child_weight': 12}. Best is trial 15 with value: 0.887927758797741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001125 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001052 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001110 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000847 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001285 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001155 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001008 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001040 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001050 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001188 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:44:47,277] Trial 34 finished with value: 0.887261290214051 and parameters: {'learning_rate': 0.058010641987904885, 'max_depth': 6, 'subsample': 0.1389307714497885, 'colsample_bytree': 0.9342870750448363, 'min_child_weight': 7}. Best is trial 15 with value: 0.887927758797741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000649 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000656 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000637 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000640 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000644 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000615 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000664 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001806 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000652 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000636 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:44:53,970] Trial 35 finished with value: 0.8799604057623842 and parameters: {'learning_rate': 0.04070938876408981, 'max_depth': 8, 'subsample': 0.3619801628160205, 'colsample_bytree': 0.1802033925069249, 'min_child_weight': 10}. Best is trial 15 with value: 0.887927758797741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001062 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001105 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000884 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001065 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001026 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001031 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001369 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001018 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001003 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001022 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:45:01,498] Trial 36 finished with value: 0.8876351055774785 and parameters: {'learning_rate': 0.07778865414384063, 'max_depth': 8, 'subsample': 0.27980364204598923, 'colsample_bytree': 0.8185804284957474, 'min_child_weight': 13}. Best is trial 15 with value: 0.887927758797741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001303 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001167 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001106 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001147 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001019 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001046 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001047 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001149 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001052 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001110 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:45:09,950] Trial 37 finished with value: 0.8870609013461046 and parameters: {'learning_rate': 0.0463872079192345, 'max_depth': 9, 'subsample': 0.2665949498655876, 'colsample_bytree': 0.7444954237739524, 'min_child_weight': 15}. Best is trial 15 with value: 0.887927758797741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001116 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001104 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000959 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001092 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001051 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000996 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001262 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001034 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001069 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001022 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:45:18,252] Trial 38 finished with value: 0.8830405189352952 and parameters: {'learning_rate': 0.0053158104908614945, 'max_depth': 8, 'subsample': 0.5007730774466304, 'colsample_bytree': 0.8111340248995389, 'min_child_weight': 4}. Best is trial 15 with value: 0.887927758797741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001191 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000882 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000981 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000842 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000910 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000886 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000842 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000906 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000901 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001168 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:45:26,829] Trial 39 finished with value: 0.8833922512301909 and parameters: {'learning_rate': 0.0025881032503053307, 'max_depth': 10, 'subsample': 0.307714491829979, 'colsample_bytree': 0.6295844110406817, 'min_child_weight': 18}. Best is trial 15 with value: 0.887927758797741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000900 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000887 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000837 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000805 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000994 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001065 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000788 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000804 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000842 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000853 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:45:36,107] Trial 40 finished with value: 0.8874208836311757 and parameters: {'learning_rate': 0.07744189719529329, 'max_depth': 7, 'subsample': 0.5815931837640538, 'colsample_bytree': 0.4948366179496946, 'min_child_weight': 13}. Best is trial 15 with value: 0.887927758797741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001469 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000969 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000966 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001446 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000910 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000892 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000916 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001230 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000937 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000935 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:45:44,592] Trial 41 finished with value: 0.8876035041699715 and parameters: {'learning_rate': 0.08264890308546695, 'max_depth': 8, 'subsample': 0.0952489300038446, 'colsample_bytree': 0.6980295128925285, 'min_child_weight': 11}. Best is trial 15 with value: 0.887927758797741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001144 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007453 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001098 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001090 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001084 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001118 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001036 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001416 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001073 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001032 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:45:53,074] Trial 42 finished with value: 0.8875666463958681 and parameters: {'learning_rate': 0.06355877221112313, 'max_depth': 9, 'subsample': 0.1657634732226466, 'colsample_bytree': 0.7046653943031836, 'min_child_weight': 9}. Best is trial 15 with value: 0.887927758797741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000919 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000915 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001247 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001170 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000986 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001122 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000927 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000916 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000958 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000912 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:46:01,402] Trial 43 finished with value: 0.887640631048384 and parameters: {'learning_rate': 0.08193438728419075, 'max_depth': 8, 'subsample': 0.09209132681137636, 'colsample_bytree': 0.6335921734755526, 'min_child_weight': 11}. Best is trial 15 with value: 0.887927758797741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000788 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000789 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000776 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000795 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000785 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000856 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000812 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000793 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000833 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000800 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:46:09,868] Trial 44 finished with value: 0.8871279487815887 and parameters: {'learning_rate': 0.060489398018841005, 'max_depth': 8, 'subsample': 0.22573574965010237, 'colsample_bytree': 0.4268541658861821, 'min_child_weight': 14}. Best is trial 15 with value: 0.887927758797741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000855 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000847 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000849 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001122 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001036 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001423 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000941 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000948 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000944 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001111 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:46:18,480] Trial 45 finished with value: 0.8867962672324765 and parameters: {'learning_rate': 0.03768921095869212, 'max_depth': 7, 'subsample': 0.08477907799311696, 'colsample_bytree': 0.6055708274632126, 'min_child_weight': 7}. Best is trial 15 with value: 0.887927758797741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001503 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001067 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001066 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001048 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001058 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001173 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001050 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000986 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001106 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001188 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:46:26,788] Trial 46 finished with value: 0.8838227399091244 and parameters: {'learning_rate': 0.011192225342396474, 'max_depth': 9, 'subsample': 0.17545707869093577, 'colsample_bytree': 0.8481729095751834, 'min_child_weight': 11}. Best is trial 15 with value: 0.887927758797741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000850 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000976 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001424 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000837 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000935 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000933 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000911 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001104 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000905 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001013 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:46:35,312] Trial 47 finished with value: 0.8860868717746364 and parameters: {'learning_rate': 0.02772511508530974, 'max_depth': 8, 'subsample': 0.13834813279663633, 'colsample_bytree': 0.5497378366690598, 'min_child_weight': 13}. Best is trial 15 with value: 0.887927758797741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000927 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001063 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001030 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001183 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000951 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000900 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000951 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000912 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000894 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001115 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:46:43,626] Trial 48 finished with value: 0.8872782811305779 and parameters: {'learning_rate': 0.049284092023097706, 'max_depth': 7, 'subsample': 0.787566912191322, 'colsample_bytree': 0.6501374239874065, 'min_child_weight': 9}. Best is trial 15 with value: 0.887927758797741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001049 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002152 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001253 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000920 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001126 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001146 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001060 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001029 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001053 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001217 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:46:51,795] Trial 49 finished with value: 0.8877010384015973 and parameters: {'learning_rate': 0.08422606869677023, 'max_depth': 8, 'subsample': 0.42726057533382866, 'colsample_bytree': 0.787470650062883, 'min_child_weight': 15}. Best is trial 15 with value: 0.887927758797741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001249 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001218 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001135 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000901 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001438 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001145 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001022 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001350 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001092 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001053 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:46:57,311] Trial 50 finished with value: 0.8872764363853318 and parameters: {'learning_rate': 0.08553249880796601, 'max_depth': 4, 'subsample': 0.45407819779190944, 'colsample_bytree': 0.8864179786100166, 'min_child_weight': 15}. Best is trial 15 with value: 0.887927758797741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001097 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001014 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001073 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000967 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000998 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001034 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000988 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000987 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001046 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000978 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:47:04,951] Trial 51 finished with value: 0.8876523360209423 and parameters: {'learning_rate': 0.07360639738718744, 'max_depth': 8, 'subsample': 0.5634021332308559, 'colsample_bytree': 0.8036815641508096, 'min_child_weight': 12}. Best is trial 15 with value: 0.887927758797741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001033 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001034 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001122 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001060 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001111 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001124 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001104 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002209 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000988 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001053 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:47:12,780] Trial 52 finished with value: 0.8875399935933885 and parameters: {'learning_rate': 0.06523272259787609, 'max_depth': 7, 'subsample': 0.6479130438635892, 'colsample_bytree': 0.7372820069109463, 'min_child_weight': 20}. Best is trial 15 with value: 0.887927758797741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001336 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001164 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001734 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000906 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001159 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001067 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001137 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001070 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001089 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001033 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:47:21,068] Trial 53 finished with value: 0.8877529250976528 and parameters: {'learning_rate': 0.09733473247536038, 'max_depth': 8, 'subsample': 0.559268326809014, 'colsample_bytree': 0.8005688552094827, 'min_child_weight': 12}. Best is trial 15 with value: 0.887927758797741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001218 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001407 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000999 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001138 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001027 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001231 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001097 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001117 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001049 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001079 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:47:29,403] Trial 54 finished with value: 0.8875801451456347 and parameters: {'learning_rate': 0.09927266684503232, 'max_depth': 9, 'subsample': 0.5643170020656573, 'colsample_bytree': 0.9585547248245315, 'min_child_weight': 12}. Best is trial 15 with value: 0.887927758797741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001147 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001099 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001133 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001076 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001260 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001091 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001024 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001170 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001373 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001054 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:47:37,803] Trial 55 finished with value: 0.8872128055600956 and parameters: {'learning_rate': 0.05477677014798366, 'max_depth': 7, 'subsample': 0.7271467803994959, 'colsample_bytree': 0.7900335460155743, 'min_child_weight': 14}. Best is trial 15 with value: 0.887927758797741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001104 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001309 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001074 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002015 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001084 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001024 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001010 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001016 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001105 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:47:41,985] Trial 56 finished with value: 0.8830460417156302 and parameters: {'learning_rate': 0.06947603782186959, 'max_depth': 2, 'subsample': 0.539559584610108, 'colsample_bytree': 0.8396444552629152, 'min_child_weight': 17}. Best is trial 15 with value: 0.887927758797741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001031 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001066 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001160 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001087 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001082 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001103 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001068 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001017 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001042 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001104 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001151 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:47:49,753] Trial 57 finished with value: 0.8876033067674762 and parameters: {'learning_rate': 0.09735481221709691, 'max_depth': 6, 'subsample': 0.6214625392359955, 'colsample_bytree': 0.899427855069249, 'min_child_weight': 8}. Best is trial 15 with value: 0.887927758797741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005561 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001077 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001134 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001067 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001012 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001594 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001097 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001030 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000999 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001062 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:47:58,011] Trial 58 finished with value: 0.8839540060516244 and parameters: {'learning_rate': 0.007632945279111368, 'max_depth': 7, 'subsample': 0.45811133989969693, 'colsample_bytree': 0.7081546014950681, 'min_child_weight': 10}. Best is trial 15 with value: 0.887927758797741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000855 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001092 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001128 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001034 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001019 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001086 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001018 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001009 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001019 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001148 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:48:05,825] Trial 59 finished with value: 0.8870355847946741 and parameters: {'learning_rate': 0.04419695891162608, 'max_depth': 10, 'subsample': 0.4085602905928531, 'colsample_bytree': 0.7806398479717473, 'min_child_weight': 12}. Best is trial 15 with value: 0.887927758797741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001063 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001057 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001035 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001014 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001384 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001075 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001139 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001181 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001082 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001042 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:48:13,563] Trial 60 finished with value: 0.8877145077236426 and parameters: {'learning_rate': 0.08785830259607888, 'max_depth': 8, 'subsample': 0.5231600950709918, 'colsample_bytree': 0.8497194753056152, 'min_child_weight': 10}. Best is trial 15 with value: 0.887927758797741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001225 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001251 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000889 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001270 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001088 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001579 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001082 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001104 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001085 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001067 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:48:22,564] Trial 61 finished with value: 0.8877815698769254 and parameters: {'learning_rate': 0.08739593128879594, 'max_depth': 8, 'subsample': 0.5133345555994102, 'colsample_bytree': 0.8774972681980595, 'min_child_weight': 10}. Best is trial 15 with value: 0.887927758797741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001027 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001152 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001175 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001092 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001095 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001129 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001035 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001108 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001245 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001312 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:48:30,098] Trial 62 finished with value: 0.8877789610427358 and parameters: {'learning_rate': 0.08966638481408055, 'max_depth': 7, 'subsample': 0.48829469073442494, 'colsample_bytree': 0.8729105476271897, 'min_child_weight': 9}. Best is trial 15 with value: 0.887927758797741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001093 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001014 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001172 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001718 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001138 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001534 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001195 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001195 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001336 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001110 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:48:39,400] Trial 63 finished with value: 0.8876964978732564 and parameters: {'learning_rate': 0.08672588419447012, 'max_depth': 8, 'subsample': 0.5203909366727695, 'colsample_bytree': 0.8628238494649125, 'min_child_weight': 7}. Best is trial 15 with value: 0.887927758797741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001100 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001191 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001095 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001076 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001025 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001040 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001070 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001059 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001067 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001034 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:48:47,027] Trial 64 finished with value: 0.8876786702790922 and parameters: {'learning_rate': 0.08600762680213288, 'max_depth': 9, 'subsample': 0.5092240977431873, 'colsample_bytree': 0.8489432065519562, 'min_child_weight': 5}. Best is trial 15 with value: 0.887927758797741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001109 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001323 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001055 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001068 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001108 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001051 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001011 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001029 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001011 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001047 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:48:54,651] Trial 65 finished with value: 0.8874127150071291 and parameters: {'learning_rate': 0.05945138883915356, 'max_depth': 8, 'subsample': 0.4738626921296142, 'colsample_bytree': 0.8634176252276065, 'min_child_weight': 7}. Best is trial 15 with value: 0.887927758797741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001343 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001177 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000885 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000947 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001032 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001122 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001142 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001041 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001033 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001055 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:49:02,311] Trial 66 finished with value: 0.8874489525527463 and parameters: {'learning_rate': 0.06927251515902352, 'max_depth': 9, 'subsample': 0.5331599640346933, 'colsample_bytree': 0.9172249878608407, 'min_child_weight': 8}. Best is trial 15 with value: 0.887927758797741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001016 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000882 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001074 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001127 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001038 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001041 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001117 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001104 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001012 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001141 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:49:10,167] Trial 67 finished with value: 0.8873340418165397 and parameters: {'learning_rate': 0.05135919108317393, 'max_depth': 8, 'subsample': 0.3754200782191752, 'colsample_bytree': 0.724381581180664, 'min_child_weight': 9}. Best is trial 15 with value: 0.887927758797741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001040 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001061 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000890 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001124 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001028 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001182 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001054 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001428 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001158 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000982 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:49:18,027] Trial 68 finished with value: 0.8877386570778618 and parameters: {'learning_rate': 0.08661065874587849, 'max_depth': 8, 'subsample': 0.4301230332220787, 'colsample_bytree': 0.7688959013522074, 'min_child_weight': 6}. Best is trial 15 with value: 0.887927758797741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001414 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001098 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000901 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001072 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001363 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001045 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001050 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001001 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001042 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001114 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:49:26,503] Trial 69 finished with value: 0.8865374773640737 and parameters: {'learning_rate': 0.034471311016140364, 'max_depth': 7, 'subsample': 0.4280278365887243, 'colsample_bytree': 0.7815514071121731, 'min_child_weight': 2}. Best is trial 15 with value: 0.887927758797741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001106 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002262 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000923 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000876 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000946 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001123 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000893 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000922 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000916 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001081 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:49:35,213] Trial 70 finished with value: 0.8875858220209917 and parameters: {'learning_rate': 0.06112449990368563, 'max_depth': 9, 'subsample': 0.4269603802184425, 'colsample_bytree': 0.6731441097714006, 'min_child_weight': 5}. Best is trial 15 with value: 0.887927758797741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001030 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000941 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001315 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001120 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001060 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001277 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001220 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001141 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001046 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001051 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:49:43,503] Trial 71 finished with value: 0.8877306328703448 and parameters: {'learning_rate': 0.08731092886068564, 'max_depth': 8, 'subsample': 0.5893484956827665, 'colsample_bytree': 0.7606751728417995, 'min_child_weight': 8}. Best is trial 15 with value: 0.887927758797741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001141 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003844 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001027 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000911 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001119 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001089 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001171 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001281 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001010 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001030 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:49:51,424] Trial 72 finished with value: 0.8876864471998654 and parameters: {'learning_rate': 0.08824749559847768, 'max_depth': 8, 'subsample': 0.608289704999811, 'colsample_bytree': 0.7634101849570835, 'min_child_weight': 10}. Best is trial 15 with value: 0.887927758797741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001035 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001131 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000853 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001018 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001098 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001009 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001088 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005596 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001091 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001041 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:49:59,483] Trial 73 finished with value: 0.8875887201088388 and parameters: {'learning_rate': 0.07211984847858198, 'max_depth': 8, 'subsample': 0.48644462504214553, 'colsample_bytree': 0.8358764919355139, 'min_child_weight': 6}. Best is trial 15 with value: 0.887927758797741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001167 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001140 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000975 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001482 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001237 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001312 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001058 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001081 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001068 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001247 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:50:07,449] Trial 74 finished with value: 0.8878266643616319 and parameters: {'learning_rate': 0.08946658644916194, 'max_depth': 8, 'subsample': 0.6550118617467978, 'colsample_bytree': 0.754366402232365, 'min_child_weight': 8}. Best is trial 15 with value: 0.887927758797741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001354 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001172 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001202 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001272 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001807 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001276 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001497 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001136 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001031 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001580 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:50:16,050] Trial 75 finished with value: 0.8876796469120789 and parameters: {'learning_rate': 0.09854499798543329, 'max_depth': 9, 'subsample': 0.6739394821495426, 'colsample_bytree': 0.8235761988304185, 'min_child_weight': 8}. Best is trial 15 with value: 0.887927758797741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001009 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001076 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001195 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000970 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001621 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001212 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001056 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001336 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001078 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001085 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:50:24,982] Trial 76 finished with value: 0.8874967265432259 and parameters: {'learning_rate': 0.06945498757260471, 'max_depth': 7, 'subsample': 0.5954531568368965, 'colsample_bytree': 0.7240524907598875, 'min_child_weight': 9}. Best is trial 15 with value: 0.887927758797741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001568 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001209 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001187 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001017 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001185 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001093 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001080 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001117 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001173 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001082 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:50:33,939] Trial 77 finished with value: 0.8872361313476953 and parameters: {'learning_rate': 0.05601006343371939, 'max_depth': 8, 'subsample': 0.5515630421802908, 'colsample_bytree': 0.7619597297746733, 'min_child_weight': 7}. Best is trial 15 with value: 0.887927758797741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001118 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000924 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000898 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001312 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001062 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001058 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001080 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001252 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001099 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001055 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:50:43,119] Trial 78 finished with value: 0.8818265822390307 and parameters: {'learning_rate': 0.0017123556344543394, 'max_depth': 7, 'subsample': 0.6592541081932946, 'colsample_bytree': 0.8846115753035131, 'min_child_weight': 8}. Best is trial 15 with value: 0.887927758797741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000747 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000770 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001012 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000774 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000765 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000724 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000719 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000727 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000695 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000706 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:50:51,691] Trial 79 finished with value: 0.8865641025913562 and parameters: {'learning_rate': 0.08952082200684208, 'max_depth': 8, 'subsample': 0.6339051524075038, 'colsample_bytree': 0.323000564297086, 'min_child_weight': 10}. Best is trial 15 with value: 0.887927758797741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000880 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001798 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001094 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001098 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001223 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001012 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001056 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001028 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001180 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001124 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:50:59,253] Trial 80 finished with value: 0.8876503191217162 and parameters: {'learning_rate': 0.07595990620921327, 'max_depth': 7, 'subsample': 0.5877160345617232, 'colsample_bytree': 0.9078174693182437, 'min_child_weight': 9}. Best is trial 15 with value: 0.887927758797741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001110 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001065 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001004 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000889 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001074 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001073 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001070 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001095 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001065 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001059 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:51:06,942] Trial 81 finished with value: 0.8877117195963244 and parameters: {'learning_rate': 0.08261172477060129, 'max_depth': 8, 'subsample': 0.6964874559376614, 'colsample_bytree': 0.8037104086139016, 'min_child_weight': 6}. Best is trial 15 with value: 0.887927758797741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001010 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001124 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001025 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001116 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000998 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001109 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001028 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001067 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001168 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001117 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:51:14,471] Trial 82 finished with value: 0.8876538556450835 and parameters: {'learning_rate': 0.07809340829368233, 'max_depth': 8, 'subsample': 0.7027219541216835, 'colsample_bytree': 0.8235587748032783, 'min_child_weight': 5}. Best is trial 15 with value: 0.887927758797741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001184 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000927 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001123 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000922 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001073 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001056 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001010 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001079 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001341 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001581 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:51:24,166] Trial 83 finished with value: 0.8874426375837039 and parameters: {'learning_rate': 0.06557072664948622, 'max_depth': 8, 'subsample': 0.7576134526457263, 'colsample_bytree': 0.7558386697030819, 'min_child_weight': 6}. Best is trial 15 with value: 0.887927758797741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001069 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001165 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008304 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001008 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001063 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001115 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001202 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001171 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001137 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001155 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:51:32,272] Trial 84 finished with value: 0.8876189343996516 and parameters: {'learning_rate': 0.08847605736157924, 'max_depth': 9, 'subsample': 0.7143599385629185, 'colsample_bytree': 0.8009546904552585, 'min_child_weight': 6}. Best is trial 15 with value: 0.887927758797741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000734 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000646 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000661 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000674 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000678 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000753 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000638 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000645 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000640 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000644 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:51:37,330] Trial 85 finished with value: 0.8669196566771249 and parameters: {'learning_rate': 0.09936478068765207, 'max_depth': 8, 'subsample': 0.47314846937436933, 'colsample_bytree': 0.05395359789512749, 'min_child_weight': 4}. Best is trial 15 with value: 0.887927758797741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000996 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001302 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001026 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000989 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000933 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001252 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000930 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001045 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000925 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000874 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:51:46,382] Trial 86 finished with value: 0.8877656163391578 and parameters: {'learning_rate': 0.07673539659746736, 'max_depth': 9, 'subsample': 0.5178751984614973, 'colsample_bytree': 0.6773873505503041, 'min_child_weight': 11}. Best is trial 15 with value: 0.887927758797741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000913 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001034 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001066 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001126 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000929 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000904 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000940 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000917 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000933 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000904 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:51:54,672] Trial 87 finished with value: 0.8874914588566999 and parameters: {'learning_rate': 0.058884104126309796, 'max_depth': 10, 'subsample': 0.5185488391869071, 'colsample_bytree': 0.6718996555573129, 'min_child_weight': 11}. Best is trial 15 with value: 0.887927758797741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001193 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001159 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001088 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001144 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001001 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000990 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001085 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001064 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001048 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001124 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:52:02,981] Trial 88 finished with value: 0.8849978149422595 and parameters: {'learning_rate': 0.014324070671576932, 'max_depth': 9, 'subsample': 0.5428881170684635, 'colsample_bytree': 0.722665271022461, 'min_child_weight': 10}. Best is trial 15 with value: 0.887927758797741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001106 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001163 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001000 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000905 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002754 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001020 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001297 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001115 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001124 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001066 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:52:11,165] Trial 89 finished with value: 0.8871756855530347 and parameters: {'learning_rate': 0.049123532536413064, 'max_depth': 9, 'subsample': 0.4981077880838463, 'colsample_bytree': 0.8671167549129272, 'min_child_weight': 8}. Best is trial 15 with value: 0.887927758797741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000875 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000895 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000836 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000846 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000892 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000892 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000896 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000911 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000907 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000895 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:52:18,727] Trial 90 finished with value: 0.8869872451511271 and parameters: {'learning_rate': 0.04328304291989107, 'max_depth': 6, 'subsample': 0.9882396997717169, 'colsample_bytree': 0.6096291456280272, 'min_child_weight': 11}. Best is trial 15 with value: 0.887927758797741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001060 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001100 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001083 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001052 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001167 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000859 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001263 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001090 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001107 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001057 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:52:26,314] Trial 91 finished with value: 0.887669264381705 and parameters: {'learning_rate': 0.07818713253904443, 'max_depth': 8, 'subsample': 0.572788301832434, 'colsample_bytree': 0.7382988425891639, 'min_child_weight': 9}. Best is trial 15 with value: 0.887927758797741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001140 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001049 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001200 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000954 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001222 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001029 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001109 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001101 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001002 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001278 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:52:33,813] Trial 92 finished with value: 0.887602997536175 and parameters: {'learning_rate': 0.09022046667421377, 'max_depth': 8, 'subsample': 0.5945142458869654, 'colsample_bytree': 0.8281939217912786, 'min_child_weight': 10}. Best is trial 15 with value: 0.887927758797741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001172 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001073 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001063 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001145 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001041 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001170 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001291 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001033 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001052 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001037 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:52:41,326] Trial 93 finished with value: 0.8874830363628334 and parameters: {'learning_rate': 0.07711677017467972, 'max_depth': 7, 'subsample': 0.6416106255075084, 'colsample_bytree': 0.7761730668405842, 'min_child_weight': 8}. Best is trial 15 with value: 0.887927758797741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001146 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001034 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001150 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001065 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001047 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001050 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001067 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001078 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001048 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001050 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:52:45,785] Trial 94 finished with value: 0.8862572107238892 and parameters: {'learning_rate': 0.06974741144509247, 'max_depth': 3, 'subsample': 0.6895024669209411, 'colsample_bytree': 0.8073270557344374, 'min_child_weight': 7}. Best is trial 15 with value: 0.887927758797741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000970 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000892 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000938 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000887 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000857 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000903 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000896 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000915 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000900 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000908 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:52:53,645] Trial 95 finished with value: 0.8875514958169666 and parameters: {'learning_rate': 0.06334461733935327, 'max_depth': 8, 'subsample': 0.6206891599502378, 'colsample_bytree': 0.6880802060908999, 'min_child_weight': 9}. Best is trial 15 with value: 0.887927758797741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001177 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001093 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001084 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001028 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001029 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001073 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001044 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001023 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001108 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001105 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:53:01,022] Trial 96 finished with value: 0.8875403918261786 and parameters: {'learning_rate': 0.09085268258595292, 'max_depth': 7, 'subsample': 0.45072992059605355, 'colsample_bytree': 0.8832250928901457, 'min_child_weight': 11}. Best is trial 15 with value: 0.887927758797741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001037 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001154 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000890 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001144 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001068 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001002 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000991 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001037 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001128 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001075 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:53:08,529] Trial 97 finished with value: 0.8874554141704837 and parameters: {'learning_rate': 0.07895000677636488, 'max_depth': 10, 'subsample': 0.7348009383188537, 'colsample_bytree': 0.9439701075825873, 'min_child_weight': 6}. Best is trial 15 with value: 0.887927758797741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000950 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000850 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000840 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000885 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000896 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000892 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000901 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000984 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000901 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000902 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:53:16,719] Trial 98 finished with value: 0.8879058303503733 and parameters: {'learning_rate': 0.09321898565611006, 'max_depth': 9, 'subsample': 0.8493278784681701, 'colsample_bytree': 0.5779213308645639, 'min_child_weight': 10}. Best is trial 15 with value: 0.887927758797741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000908 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000836 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000934 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000843 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000863 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000911 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001022 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000954 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000900 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000903 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 20:53:25,015] Trial 99 finished with value: 0.8877494130461898 and parameters: {'learning_rate': 0.0996444224351401, 'max_depth': 9, 'subsample': 0.8621890855969296, 'colsample_bytree': 0.5017115293714774, 'min_child_weight': 10}. Best is trial 15 with value: 0.887927758797741.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    }
   ],
   "source": [
    "study2 = create_study(study_name='optimization', direction='maximize', pruner=optuna.pruners.MedianPruner())\n",
    "\n",
    "study2.optimize(lambda trial: objective(trial, selected_pipeline, 'lgbm'), n_trials=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 21:07:09,905] A new study created in memory with name: optimization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 21:07:24,745] Trial 0 finished with value: 0.8871639116621745 and parameters: {'learning_rate': 0.22604922371985178, 'max_iter': 718, 'max_leaf_nodes': 60, 'min_samples_leaf': 35, 'l2_regularization': 0.8049859123567723}. Best is trial 0 with value: 0.8871639116621745.\n",
      "[I 2024-01-18 21:07:35,215] Trial 1 finished with value: 0.8867050157287301 and parameters: {'learning_rate': 0.2792869347166801, 'max_iter': 264, 'max_leaf_nodes': 37, 'min_samples_leaf': 53, 'l2_regularization': 0.003213541411808046}. Best is trial 0 with value: 0.8871639116621745.\n",
      "[I 2024-01-18 21:07:54,846] Trial 2 finished with value: 0.8870595661819956 and parameters: {'learning_rate': 0.12436208870973971, 'max_iter': 435, 'max_leaf_nodes': 32, 'min_samples_leaf': 39, 'l2_regularization': 0.0005039658544666192}. Best is trial 0 with value: 0.8871639116621745.\n",
      "[I 2024-01-18 21:08:13,102] Trial 3 finished with value: 0.887335690432113 and parameters: {'learning_rate': 0.1599974693591596, 'max_iter': 737, 'max_leaf_nodes': 50, 'min_samples_leaf': 52, 'l2_regularization': 3.2054810876665795e-07}. Best is trial 3 with value: 0.887335690432113.\n",
      "[I 2024-01-18 21:08:23,455] Trial 4 finished with value: 0.8871461500232188 and parameters: {'learning_rate': 0.2472830383334009, 'max_iter': 207, 'max_leaf_nodes': 24, 'min_samples_leaf': 57, 'l2_regularization': 7.881516470281061e-08}. Best is trial 3 with value: 0.887335690432113.\n",
      "[I 2024-01-18 21:08:40,033] Trial 5 finished with value: 0.887438402006641 and parameters: {'learning_rate': 0.1430997244638051, 'max_iter': 259, 'max_leaf_nodes': 29, 'min_samples_leaf': 45, 'l2_regularization': 2.3084207362123765e-07}. Best is trial 5 with value: 0.887438402006641.\n",
      "[I 2024-01-18 21:09:06,224] Trial 6 finished with value: 0.8875580748966946 and parameters: {'learning_rate': 0.0885576086492077, 'max_iter': 207, 'max_leaf_nodes': 36, 'min_samples_leaf': 22, 'l2_regularization': 2.265195960481543e-09}. Best is trial 6 with value: 0.8875580748966946.\n",
      "[I 2024-01-18 21:09:34,817] Trial 7 finished with value: 0.8871537685012097 and parameters: {'learning_rate': 0.06657482938027418, 'max_iter': 135, 'max_leaf_nodes': 35, 'min_samples_leaf': 36, 'l2_regularization': 0.020292453566678127}. Best is trial 6 with value: 0.8875580748966946.\n",
      "[I 2024-01-18 21:09:47,986] Trial 8 finished with value: 0.8871724347389551 and parameters: {'learning_rate': 0.1951651306468438, 'max_iter': 240, 'max_leaf_nodes': 33, 'min_samples_leaf': 32, 'l2_regularization': 3.347934563166091e-06}. Best is trial 6 with value: 0.8875580748966946.\n",
      "[I 2024-01-18 21:10:00,098] Trial 9 finished with value: 0.8869551607830635 and parameters: {'learning_rate': 0.2077409431439427, 'max_iter': 565, 'max_leaf_nodes': 23, 'min_samples_leaf': 54, 'l2_regularization': 0.003464636798149274}. Best is trial 6 with value: 0.8875580748966946.\n",
      "[I 2024-01-18 21:11:21,906] Trial 10 finished with value: 0.8875612651697872 and parameters: {'learning_rate': 0.024576234211994646, 'max_iter': 913, 'max_leaf_nodes': 46, 'min_samples_leaf': 20, 'l2_regularization': 1.6737756778269863e-10}. Best is trial 10 with value: 0.8875612651697872.\n",
      "[I 2024-01-18 21:13:48,043] Trial 11 finished with value: 0.8874570976784086 and parameters: {'learning_rate': 0.012751759488049158, 'max_iter': 906, 'max_leaf_nodes': 46, 'min_samples_leaf': 20, 'l2_regularization': 2.9269062253927524e-10}. Best is trial 10 with value: 0.8875612651697872.\n",
      "[I 2024-01-18 21:14:29,545] Trial 12 finished with value: 0.887353433780545 and parameters: {'learning_rate': 0.06355390930800209, 'max_iter': 921, 'max_leaf_nodes': 54, 'min_samples_leaf': 20, 'l2_regularization': 2.224030408064707e-10}. Best is trial 10 with value: 0.8875612651697872.\n",
      "[I 2024-01-18 21:16:09,995] Trial 13 finished with value: 0.8875291143629052 and parameters: {'learning_rate': 0.017387292291123208, 'max_iter': 417, 'max_leaf_nodes': 42, 'min_samples_leaf': 27, 'l2_regularization': 7.27266697281921e-09}. Best is trial 10 with value: 0.8875612651697872.\n",
      "[I 2024-01-18 21:16:40,387] Trial 14 finished with value: 0.887622931897456 and parameters: {'learning_rate': 0.07839953320488427, 'max_iter': 997, 'max_leaf_nodes': 43, 'min_samples_leaf': 27, 'l2_regularization': 3.1343167788400232e-09}. Best is trial 14 with value: 0.887622931897456.\n",
      "[I 2024-01-18 21:17:24,242] Trial 15 finished with value: 0.8874247123634175 and parameters: {'learning_rate': 0.050097118311849, 'max_iter': 991, 'max_leaf_nodes': 43, 'min_samples_leaf': 27, 'l2_regularization': 1.4204277307448498e-08}. Best is trial 14 with value: 0.887622931897456.\n",
      "[I 2024-01-18 21:17:54,305] Trial 16 finished with value: 0.8872169678348649 and parameters: {'learning_rate': 0.09726904745625245, 'max_iter': 753, 'max_leaf_nodes': 64, 'min_samples_leaf': 27, 'l2_regularization': 1.5321298150971804e-10}. Best is trial 14 with value: 0.887622931897456.\n",
      "[I 2024-01-18 21:18:49,217] Trial 17 finished with value: 0.8876128414893163 and parameters: {'learning_rate': 0.04032935844127129, 'max_iter': 841, 'max_leaf_nodes': 54, 'min_samples_leaf': 44, 'l2_regularization': 3.1370564012774106e-05}. Best is trial 14 with value: 0.887622931897456.\n",
      "[I 2024-01-18 21:19:16,344] Trial 18 finished with value: 0.8871931847810266 and parameters: {'learning_rate': 0.10315626003834218, 'max_iter': 834, 'max_leaf_nodes': 55, 'min_samples_leaf': 44, 'l2_regularization': 1.4153057605719714e-05}. Best is trial 14 with value: 0.887622931897456.\n",
      "[I 2024-01-18 21:19:36,052] Trial 19 finished with value: 0.8867984262451793 and parameters: {'learning_rate': 0.16806785020909032, 'max_iter': 642, 'max_leaf_nodes': 54, 'min_samples_leaf': 47, 'l2_regularization': 2.0176942982334055e-05}. Best is trial 14 with value: 0.887622931897456.\n",
      "[I 2024-01-18 21:20:22,737] Trial 20 finished with value: 0.8871166041343916 and parameters: {'learning_rate': 0.05184301846709924, 'max_iter': 828, 'max_leaf_nodes': 59, 'min_samples_leaf': 40, 'l2_regularization': 0.0002738675768394229}. Best is trial 14 with value: 0.887622931897456.\n",
      "[I 2024-01-18 21:21:44,623] Trial 21 finished with value: 0.8877540408542435 and parameters: {'learning_rate': 0.02914386061886033, 'max_iter': 998, 'max_leaf_nodes': 48, 'min_samples_leaf': 25, 'l2_regularization': 2.45430963795829e-09}. Best is trial 21 with value: 0.8877540408542435.\n",
      "[I 2024-01-18 21:22:44,768] Trial 22 finished with value: 0.8873477841212223 and parameters: {'learning_rate': 0.033946377135504406, 'max_iter': 998, 'max_leaf_nodes': 50, 'min_samples_leaf': 30, 'l2_regularization': 1.261121047819577e-06}. Best is trial 21 with value: 0.8877540408542435.\n",
      "[I 2024-01-18 21:23:15,492] Trial 23 finished with value: 0.887400750561496 and parameters: {'learning_rate': 0.07947394788852245, 'max_iter': 837, 'max_leaf_nodes': 49, 'min_samples_leaf': 24, 'l2_regularization': 2.3633224469044004e-08}. Best is trial 21 with value: 0.8877540408542435.\n",
      "[I 2024-01-18 21:23:39,490] Trial 24 finished with value: 0.8871048610496185 and parameters: {'learning_rate': 0.11284678816552744, 'max_iter': 991, 'max_leaf_nodes': 45, 'min_samples_leaf': 49, 'l2_regularization': 7.308044917740817e-05}. Best is trial 21 with value: 0.8877540408542435.\n",
      "[I 2024-01-18 21:24:26,741] Trial 25 finished with value: 0.8874415548464625 and parameters: {'learning_rate': 0.042976073028333464, 'max_iter': 907, 'max_leaf_nodes': 39, 'min_samples_leaf': 32, 'l2_regularization': 1.6389530037782818e-09}. Best is trial 21 with value: 0.8877540408542435.\n",
      "[I 2024-01-18 21:24:59,666] Trial 26 finished with value: 0.8873327910966475 and parameters: {'learning_rate': 0.0733815400090345, 'max_iter': 787, 'max_leaf_nodes': 58, 'min_samples_leaf': 43, 'l2_regularization': 1.216543653818088e-06}. Best is trial 21 with value: 0.8877540408542435.\n",
      "[I 2024-01-18 21:25:17,680] Trial 27 finished with value: 0.8872862169840442 and parameters: {'learning_rate': 0.12828247206455162, 'max_iter': 636, 'max_leaf_nodes': 40, 'min_samples_leaf': 37, 'l2_regularization': 1.4709272489857438e-09}. Best is trial 21 with value: 0.8877540408542435.\n",
      "[I 2024-01-18 21:26:21,910] Trial 28 finished with value: 0.8871771803839152 and parameters: {'learning_rate': 0.03530821003420152, 'max_iter': 940, 'max_leaf_nodes': 51, 'min_samples_leaf': 24, 'l2_regularization': 5.736002472894667e-08}. Best is trial 21 with value: 0.8877540408542435.\n",
      "[I 2024-01-18 21:29:39,674] Trial 29 finished with value: 0.887201050881295 and parameters: {'learning_rate': 0.010312326608772307, 'max_iter': 655, 'max_leaf_nodes': 63, 'min_samples_leaf': 33, 'l2_regularization': 0.033968385426150924}. Best is trial 21 with value: 0.8877540408542435.\n",
      "[I 2024-01-18 21:30:27,880] Trial 30 finished with value: 0.8875493140698879 and parameters: {'learning_rate': 0.051894570482083666, 'max_iter': 869, 'max_leaf_nodes': 57, 'min_samples_leaf': 29, 'l2_regularization': 7.920122785463135e-05}. Best is trial 21 with value: 0.8877540408542435.\n",
      "[I 2024-01-18 21:31:38,218] Trial 31 finished with value: 0.887600340109463 and parameters: {'learning_rate': 0.029707362187940095, 'max_iter': 952, 'max_leaf_nodes': 47, 'min_samples_leaf': 23, 'l2_regularization': 1.1515369398381807e-09}. Best is trial 21 with value: 0.8877540408542435.\n",
      "[I 2024-01-18 21:31:48,192] Trial 32 finished with value: 0.8861668273985639 and parameters: {'learning_rate': 0.2999016988794169, 'max_iter': 972, 'max_leaf_nodes': 47, 'min_samples_leaf': 25, 'l2_regularization': 5.349186905808872e-09}. Best is trial 21 with value: 0.8877540408542435.\n",
      "[I 2024-01-18 21:32:57,104] Trial 33 finished with value: 0.8876027870147853 and parameters: {'learning_rate': 0.03189412394818693, 'max_iter': 878, 'max_leaf_nodes': 43, 'min_samples_leaf': 23, 'l2_regularization': 1.8633772985709735e-07}. Best is trial 21 with value: 0.8877540408542435.\n",
      "[I 2024-01-18 21:33:30,015] Trial 34 finished with value: 0.8873883813645828 and parameters: {'learning_rate': 0.058210850660746424, 'max_iter': 785, 'max_leaf_nodes': 39, 'min_samples_leaf': 41, 'l2_regularization': 2.1256010642314236e-07}. Best is trial 21 with value: 0.8877540408542435.\n",
      "[I 2024-01-18 21:33:58,933] Trial 35 finished with value: 0.8871372569142295 and parameters: {'learning_rate': 0.0875575283971652, 'max_iter': 698, 'max_leaf_nodes': 52, 'min_samples_leaf': 50, 'l2_regularization': 9.790520319168645e-07}. Best is trial 21 with value: 0.8877540408542435.\n",
      "[I 2024-01-18 21:34:58,371] Trial 36 finished with value: 0.8872941175072322 and parameters: {'learning_rate': 0.03347281233869834, 'max_iter': 485, 'max_leaf_nodes': 43, 'min_samples_leaf': 60, 'l2_regularization': 3.585824179734282e-08}. Best is trial 21 with value: 0.8877540408542435.\n",
      "[I 2024-01-18 21:35:10,570] Trial 37 finished with value: 0.886905398516113 and parameters: {'learning_rate': 0.2556699741053986, 'max_iter': 871, 'max_leaf_nodes': 27, 'min_samples_leaf': 34, 'l2_regularization': 0.478865790359785}. Best is trial 21 with value: 0.8877540408542435.\n",
      "[I 2024-01-18 21:35:28,616] Trial 38 finished with value: 0.8869092908296234 and parameters: {'learning_rate': 0.1410741484193479, 'max_iter': 365, 'max_leaf_nodes': 61, 'min_samples_leaf': 38, 'l2_regularization': 1.395287588330011e-07}. Best is trial 21 with value: 0.8877540408542435.\n",
      "[I 2024-01-18 21:35:57,419] Trial 39 finished with value: 0.8873742794339614 and parameters: {'learning_rate': 0.07387713626740386, 'max_iter': 870, 'max_leaf_nodes': 48, 'min_samples_leaf': 28, 'l2_regularization': 3.5450671171913263e-06}. Best is trial 21 with value: 0.8877540408542435.\n",
      "[I 2024-01-18 21:36:41,329] Trial 40 finished with value: 0.8872146733185411 and parameters: {'learning_rate': 0.043237406394485506, 'max_iter': 787, 'max_leaf_nodes': 41, 'min_samples_leaf': 30, 'l2_regularization': 5.649248609185066e-09}. Best is trial 21 with value: 0.8877540408542435.\n",
      "[I 2024-01-18 21:37:53,767] Trial 41 finished with value: 0.8875920444410248 and parameters: {'learning_rate': 0.02811371282818096, 'max_iter': 951, 'max_leaf_nodes': 45, 'min_samples_leaf': 23, 'l2_regularization': 7.144412971515668e-10}. Best is trial 21 with value: 0.8877540408542435.\n",
      "[I 2024-01-18 21:39:30,005] Trial 42 finished with value: 0.8872984383949136 and parameters: {'learning_rate': 0.02486946933376525, 'max_iter': 929, 'max_leaf_nodes': 52, 'min_samples_leaf': 22, 'l2_regularization': 5.188581354636536e-10}. Best is trial 21 with value: 0.8877540408542435.\n",
      "[I 2024-01-18 21:40:05,650] Trial 43 finished with value: 0.8873753554904595 and parameters: {'learning_rate': 0.06263209649792825, 'max_iter': 963, 'max_leaf_nodes': 44, 'min_samples_leaf': 25, 'l2_regularization': 1.8696085832563305e-09}. Best is trial 21 with value: 0.8877540408542435.\n",
      "[I 2024-01-18 21:42:44,293] Trial 44 finished with value: 0.8872322044222322 and parameters: {'learning_rate': 0.01031677484637259, 'max_iter': 873, 'max_leaf_nodes': 37, 'min_samples_leaf': 22, 'l2_regularization': 3.79396992079238e-07}. Best is trial 21 with value: 0.8877540408542435.\n",
      "[I 2024-01-18 21:43:31,929] Trial 45 finished with value: 0.8873286447179831 and parameters: {'learning_rate': 0.040467075978593184, 'max_iter': 693, 'max_leaf_nodes': 49, 'min_samples_leaf': 26, 'l2_regularization': 1.1942968344199142e-08}. Best is trial 21 with value: 0.8877540408542435.\n",
      "[I 2024-01-18 21:44:22,004] Trial 46 finished with value: 0.887574553900677 and parameters: {'learning_rate': 0.02642460620088856, 'max_iter': 899, 'max_leaf_nodes': 20, 'min_samples_leaf': 22, 'l2_regularization': 7.230583589549068e-08}. Best is trial 21 with value: 0.8877540408542435.\n",
      "[I 2024-01-18 21:44:44,005] Trial 47 finished with value: 0.8874206102106119 and parameters: {'learning_rate': 0.08767420860430497, 'max_iter': 954, 'max_leaf_nodes': 33, 'min_samples_leaf': 31, 'l2_regularization': 8.171210713048149e-10}. Best is trial 21 with value: 0.8877540408542435.\n",
      "[I 2024-01-18 21:44:59,927] Trial 48 finished with value: 0.8871120968837454 and parameters: {'learning_rate': 0.17923199820113542, 'max_iter': 585, 'max_leaf_nodes': 56, 'min_samples_leaf': 20, 'l2_regularization': 3.3641184985061007e-09}. Best is trial 21 with value: 0.8877540408542435.\n",
      "[I 2024-01-18 21:45:40,149] Trial 49 finished with value: 0.887399953771409 and parameters: {'learning_rate': 0.05692087948869977, 'max_iter': 749, 'max_leaf_nodes': 47, 'min_samples_leaf': 35, 'l2_regularization': 0.0007194649515594212}. Best is trial 21 with value: 0.8877540408542435.\n",
      "[I 2024-01-18 21:45:59,336] Trial 50 finished with value: 0.8874963872036845 and parameters: {'learning_rate': 0.11265932029294731, 'max_iter': 121, 'max_leaf_nodes': 37, 'min_samples_leaf': 41, 'l2_regularization': 4.619755521743043e-06}. Best is trial 21 with value: 0.8877540408542435.\n",
      "[I 2024-01-18 21:47:22,874] Trial 51 finished with value: 0.8874784262073168 and parameters: {'learning_rate': 0.023807728773646116, 'max_iter': 947, 'max_leaf_nodes': 44, 'min_samples_leaf': 23, 'l2_regularization': 4.191399214218382e-10}. Best is trial 21 with value: 0.8877540408542435.\n",
      "[I 2024-01-18 21:48:51,506] Trial 52 finished with value: 0.8872870360926342 and parameters: {'learning_rate': 0.021303473484738544, 'max_iter': 999, 'max_leaf_nodes': 46, 'min_samples_leaf': 26, 'l2_regularization': 8.369944201257411e-10}. Best is trial 21 with value: 0.8877540408542435.\n",
      "[I 2024-01-18 21:49:31,933] Trial 53 finished with value: 0.8876002187864718 and parameters: {'learning_rate': 0.04460248359172249, 'max_iter': 825, 'max_leaf_nodes': 42, 'min_samples_leaf': 23, 'l2_regularization': 1.4634829780307823e-08}. Best is trial 21 with value: 0.8877540408542435.\n",
      "[I 2024-01-18 21:50:07,296] Trial 54 finished with value: 0.8873281419341598 and parameters: {'learning_rate': 0.06708907798848684, 'max_iter': 817, 'max_leaf_nodes': 42, 'min_samples_leaf': 28, 'l2_regularization': 1.0957946626073493e-08}. Best is trial 21 with value: 0.8877540408542435.\n",
      "[I 2024-01-18 21:50:47,442] Trial 55 finished with value: 0.8874695343664243 and parameters: {'learning_rate': 0.05297539076082938, 'max_iter': 907, 'max_leaf_nodes': 41, 'min_samples_leaf': 47, 'l2_regularization': 2.0831721358084053e-08}. Best is trial 21 with value: 0.8877540408542435.\n",
      "[I 2024-01-18 21:51:43,681] Trial 56 finished with value: 0.8873819786274921 and parameters: {'learning_rate': 0.04047341859071807, 'max_iter': 842, 'max_leaf_nodes': 53, 'min_samples_leaf': 21, 'l2_regularization': 3.127487012624579e-05}. Best is trial 21 with value: 0.8877540408542435.\n",
      "[I 2024-01-18 21:51:55,997] Trial 57 finished with value: 0.8868659970202564 and parameters: {'learning_rate': 0.22446393155725713, 'max_iter': 888, 'max_leaf_nodes': 35, 'min_samples_leaf': 24, 'l2_regularization': 3.6771410358757793e-09}. Best is trial 21 with value: 0.8877540408542435.\n",
      "[I 2024-01-18 21:52:31,746] Trial 58 finished with value: 0.8876930489997918 and parameters: {'learning_rate': 0.07510854648834306, 'max_iter': 926, 'max_leaf_nodes': 48, 'min_samples_leaf': 26, 'l2_regularization': 1.5588053818189383e-10}. Best is trial 21 with value: 0.8877540408542435.\n",
      "[I 2024-01-18 21:53:00,115] Trial 59 finished with value: 0.8873661709649593 and parameters: {'learning_rate': 0.09491620831964945, 'max_iter': 926, 'max_leaf_nodes': 49, 'min_samples_leaf': 26, 'l2_regularization': 1.3081271052347066e-10}. Best is trial 21 with value: 0.8877540408542435.\n",
      "[I 2024-01-18 21:53:33,975] Trial 60 finished with value: 0.8875775243992119 and parameters: {'learning_rate': 0.08287303629428514, 'max_iter': 974, 'max_leaf_nodes': 51, 'min_samples_leaf': 28, 'l2_regularization': 2.0665074263223147e-10}. Best is trial 21 with value: 0.8877540408542435.\n",
      "[I 2024-01-18 21:54:31,752] Trial 61 finished with value: 0.8873423958914666 and parameters: {'learning_rate': 0.045562004882436974, 'max_iter': 806, 'max_leaf_nodes': 44, 'min_samples_leaf': 21, 'l2_regularization': 1.0436722970269555e-10}. Best is trial 21 with value: 0.8877540408542435.\n",
      "[I 2024-01-18 21:55:02,862] Trial 62 finished with value: 0.8873223739732243 and parameters: {'learning_rate': 0.07332118552699321, 'max_iter': 852, 'max_leaf_nodes': 39, 'min_samples_leaf': 25, 'l2_regularization': 1.2555040178992404e-09}. Best is trial 21 with value: 0.8877540408542435.\n",
      "[I 2024-01-18 21:56:58,587] Trial 63 finished with value: 0.8874605824886327 and parameters: {'learning_rate': 0.01779402673549476, 'max_iter': 1000, 'max_leaf_nodes': 47, 'min_samples_leaf': 46, 'l2_regularization': 2.991048576578401e-10}. Best is trial 21 with value: 0.8877540408542435.\n",
      "[I 2024-01-18 21:57:38,943] Trial 64 finished with value: 0.8876237026214597 and parameters: {'learning_rate': 0.06363645541088464, 'max_iter': 913, 'max_leaf_nodes': 50, 'min_samples_leaf': 43, 'l2_regularization': 5.831820239969152e-07}. Best is trial 21 with value: 0.8877540408542435.\n",
      "[I 2024-01-18 21:58:14,327] Trial 65 finished with value: 0.8873280315057739 and parameters: {'learning_rate': 0.06685566582517646, 'max_iter': 925, 'max_leaf_nodes': 54, 'min_samples_leaf': 45, 'l2_regularization': 5.140696907607193e-07}. Best is trial 21 with value: 0.8877540408542435.\n",
      "[I 2024-01-18 21:58:44,367] Trial 66 finished with value: 0.8876852344693168 and parameters: {'learning_rate': 0.09977384674578951, 'max_iter': 973, 'max_leaf_nodes': 50, 'min_samples_leaf': 54, 'l2_regularization': 6.073190164134846e-06}. Best is trial 21 with value: 0.8877540408542435.\n",
      "[I 2024-01-18 21:59:13,073] Trial 67 finished with value: 0.8868991035105367 and parameters: {'learning_rate': 0.10397845757676624, 'max_iter': 973, 'max_leaf_nodes': 50, 'min_samples_leaf': 54, 'l2_regularization': 5.549023025328292e-06}. Best is trial 21 with value: 0.8877540408542435.\n",
      "[I 2024-01-18 21:59:40,164] Trial 68 finished with value: 0.8875265679679094 and parameters: {'learning_rate': 0.11012822429049833, 'max_iter': 896, 'max_leaf_nodes': 56, 'min_samples_leaf': 56, 'l2_regularization': 5.5965528043742084e-05}. Best is trial 21 with value: 0.8877540408542435.\n",
      "[I 2024-01-18 22:00:01,944] Trial 69 finished with value: 0.8872353956546676 and parameters: {'learning_rate': 0.1375166700851923, 'max_iter': 927, 'max_leaf_nodes': 52, 'min_samples_leaf': 50, 'l2_regularization': 0.000251650936387819}. Best is trial 21 with value: 0.8877540408542435.\n",
      "[I 2024-01-18 22:00:22,518] Trial 70 finished with value: 0.8871783229091621 and parameters: {'learning_rate': 0.12784241096541799, 'max_iter': 855, 'max_leaf_nodes': 48, 'min_samples_leaf': 59, 'l2_regularization': 1.9053044577222724e-06}. Best is trial 21 with value: 0.8877540408542435.\n",
      "[I 2024-01-18 22:01:27,700] Trial 71 finished with value: 0.8875085389243671 and parameters: {'learning_rate': 0.031059187091519967, 'max_iter': 967, 'max_leaf_nodes': 51, 'min_samples_leaf': 39, 'l2_regularization': 1.9542994083320343e-05}. Best is trial 21 with value: 0.8877540408542435.\n",
      "[W 2024-01-18 22:01:50,408] Trial 72 failed with parameters: {'learning_rate': 0.07905700005677294, 'max_iter': 941, 'max_leaf_nodes': 45, 'min_samples_leaf': 51, 'l2_regularization': 6.790781506631028e-06} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/krystianpietrzak/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/var/folders/x1/2j32gjvd4v16s6kf306sfdcw0000gn/T/ipykernel_45424/4108997271.py\", line 4, in <lambda>\n",
      "    study3.optimize(lambda trial: objective(trial, selected_pipeline, 'hist'), n_trials=100)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/x1/2j32gjvd4v16s6kf306sfdcw0000gn/T/ipykernel_45424/710525806.py\", line 19, in objective\n",
      "    scores = cross_val_score(pipeline, X, y, scoring=roc_auc_scorer, cv=skf)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/krystianpietrzak/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 562, in cross_val_score\n",
      "    cv_results = cross_validate(\n",
      "                 ^^^^^^^^^^^^^^^\n",
      "  File \"/Users/krystianpietrzak/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/sklearn/utils/_param_validation.py\", line 214, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/krystianpietrzak/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 309, in cross_validate\n",
      "    results = parallel(\n",
      "              ^^^^^^^^^\n",
      "  File \"/Users/krystianpietrzak/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/sklearn/utils/parallel.py\", line 65, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/krystianpietrzak/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/joblib/parallel.py\", line 1863, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "                                                ^^^^^^^^^^^^\n",
      "  File \"/Users/krystianpietrzak/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/joblib/parallel.py\", line 1792, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/krystianpietrzak/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/sklearn/utils/parallel.py\", line 127, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/krystianpietrzak/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/krystianpietrzak/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/sklearn/base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/krystianpietrzak/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/sklearn/pipeline.py\", line 427, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/Users/krystianpietrzak/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/sklearn/base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/krystianpietrzak/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py\", line 696, in fit\n",
      "    grower.grow()\n",
      "  File \"/Users/krystianpietrzak/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/sklearn/ensemble/_hist_gradient_boosting/grower.py\", line 366, in grow\n",
      "    self.split_next()\n",
      "  File \"/Users/krystianpietrzak/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/sklearn/ensemble/_hist_gradient_boosting/grower.py\", line 586, in split_next\n",
      "    smallest_child.histograms = self.histogram_builder.compute_histograms_brute(\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "[W 2024-01-18 22:01:50,429] Trial 72 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/krystianpietrzak/Documents/ML/Kaggle/Kaggle-S4E1/eda.ipynb Cell 49\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/krystianpietrzak/Documents/ML/Kaggle/Kaggle-S4E1/eda.ipynb#Y256sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39moptuna\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/krystianpietrzak/Documents/ML/Kaggle/Kaggle-S4E1/eda.ipynb#Y256sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m study3 \u001b[39m=\u001b[39m create_study(study_name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39moptimization\u001b[39m\u001b[39m'\u001b[39m, direction\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmaximize\u001b[39m\u001b[39m'\u001b[39m, pruner\u001b[39m=\u001b[39moptuna\u001b[39m.\u001b[39mpruners\u001b[39m.\u001b[39mMedianPruner())\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/krystianpietrzak/Documents/ML/Kaggle/Kaggle-S4E1/eda.ipynb#Y256sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m study3\u001b[39m.\u001b[39;49moptimize(\u001b[39mlambda\u001b[39;49;00m trial: objective(trial, selected_pipeline, \u001b[39m'\u001b[39;49m\u001b[39mhist\u001b[39;49m\u001b[39m'\u001b[39;49m), n_trials\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m)\n",
      "File \u001b[0;32m~/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/optuna/study/study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[1;32m    349\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    357\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    358\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \n\u001b[1;32m    361\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 451\u001b[0m     _optimize(\n\u001b[1;32m    452\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    453\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[1;32m    454\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[1;32m    455\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    456\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[1;32m    457\u001b[0m         catch\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(catch) \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(catch, Iterable) \u001b[39melse\u001b[39;49;00m (catch,),\n\u001b[1;32m    458\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    459\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[1;32m    460\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[1;32m    461\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/optuna/study/_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[1;32m     67\u001b[0m             study,\n\u001b[1;32m     68\u001b[0m             func,\n\u001b[1;32m     69\u001b[0m             n_trials,\n\u001b[1;32m     70\u001b[0m             timeout,\n\u001b[1;32m     71\u001b[0m             catch,\n\u001b[1;32m     72\u001b[0m             callbacks,\n\u001b[1;32m     73\u001b[0m             gc_after_trial,\n\u001b[1;32m     74\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     75\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     76\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m     77\u001b[0m         )\n\u001b[1;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/optuna/study/_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[1;32m    164\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/optuna/study/_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    246\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    247\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[1;32m    248\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    250\u001b[0m ):\n\u001b[0;32m--> 251\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[1;32m    252\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/optuna/study/_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[1;32m    199\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 200\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[1;32m    201\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    202\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "\u001b[1;32m/Users/krystianpietrzak/Documents/ML/Kaggle/Kaggle-S4E1/eda.ipynb Cell 49\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/krystianpietrzak/Documents/ML/Kaggle/Kaggle-S4E1/eda.ipynb#Y256sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39moptuna\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/krystianpietrzak/Documents/ML/Kaggle/Kaggle-S4E1/eda.ipynb#Y256sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m study3 \u001b[39m=\u001b[39m create_study(study_name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39moptimization\u001b[39m\u001b[39m'\u001b[39m, direction\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmaximize\u001b[39m\u001b[39m'\u001b[39m, pruner\u001b[39m=\u001b[39moptuna\u001b[39m.\u001b[39mpruners\u001b[39m.\u001b[39mMedianPruner())\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/krystianpietrzak/Documents/ML/Kaggle/Kaggle-S4E1/eda.ipynb#Y256sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m study3\u001b[39m.\u001b[39moptimize(\u001b[39mlambda\u001b[39;00m trial: objective(trial, selected_pipeline, \u001b[39m'\u001b[39;49m\u001b[39mhist\u001b[39;49m\u001b[39m'\u001b[39;49m), n_trials\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m)\n",
      "\u001b[1;32m/Users/krystianpietrzak/Documents/ML/Kaggle/Kaggle-S4E1/eda.ipynb Cell 49\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/krystianpietrzak/Documents/ML/Kaggle/Kaggle-S4E1/eda.ipynb#Y256sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m skf \u001b[39m=\u001b[39m StratifiedKFold(n_splits\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/krystianpietrzak/Documents/ML/Kaggle/Kaggle-S4E1/eda.ipynb#Y256sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m roc_auc_scorer \u001b[39m=\u001b[39m make_scorer(roc_auc_score, needs_proba\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/krystianpietrzak/Documents/ML/Kaggle/Kaggle-S4E1/eda.ipynb#Y256sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m scores \u001b[39m=\u001b[39m cross_val_score(pipeline, X, y, scoring\u001b[39m=\u001b[39;49mroc_auc_scorer, cv\u001b[39m=\u001b[39;49mskf)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/krystianpietrzak/Documents/ML/Kaggle/Kaggle-S4E1/eda.ipynb#Y256sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mmin([np\u001b[39m.\u001b[39mmean(scores), np\u001b[39m.\u001b[39mmedian([scores])])\n",
      "File \u001b[0;32m~/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:562\u001b[0m, in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[39m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[1;32m    560\u001b[0m scorer \u001b[39m=\u001b[39m check_scoring(estimator, scoring\u001b[39m=\u001b[39mscoring)\n\u001b[0;32m--> 562\u001b[0m cv_results \u001b[39m=\u001b[39m cross_validate(\n\u001b[1;32m    563\u001b[0m     estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[1;32m    564\u001b[0m     X\u001b[39m=\u001b[39;49mX,\n\u001b[1;32m    565\u001b[0m     y\u001b[39m=\u001b[39;49my,\n\u001b[1;32m    566\u001b[0m     groups\u001b[39m=\u001b[39;49mgroups,\n\u001b[1;32m    567\u001b[0m     scoring\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mscore\u001b[39;49m\u001b[39m\"\u001b[39;49m: scorer},\n\u001b[1;32m    568\u001b[0m     cv\u001b[39m=\u001b[39;49mcv,\n\u001b[1;32m    569\u001b[0m     n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[1;32m    570\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    571\u001b[0m     fit_params\u001b[39m=\u001b[39;49mfit_params,\n\u001b[1;32m    572\u001b[0m     pre_dispatch\u001b[39m=\u001b[39;49mpre_dispatch,\n\u001b[1;32m    573\u001b[0m     error_score\u001b[39m=\u001b[39;49merror_score,\n\u001b[1;32m    574\u001b[0m )\n\u001b[1;32m    575\u001b[0m \u001b[39mreturn\u001b[39;00m cv_results[\u001b[39m\"\u001b[39m\u001b[39mtest_score\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:214\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    209\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m    210\u001b[0m         skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m    211\u001b[0m             prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    212\u001b[0m         )\n\u001b[1;32m    213\u001b[0m     ):\n\u001b[0;32m--> 214\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    215\u001b[0m \u001b[39mexcept\u001b[39;00m InvalidParameterError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    216\u001b[0m     \u001b[39m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[39m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[39m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[39m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     msg \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\n\u001b[1;32m    221\u001b[0m         \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+ must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    223\u001b[0m         \u001b[39mstr\u001b[39m(e),\n\u001b[1;32m    224\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:309\u001b[0m, in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[39m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[1;32m    307\u001b[0m \u001b[39m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[1;32m    308\u001b[0m parallel \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39mn_jobs, verbose\u001b[39m=\u001b[39mverbose, pre_dispatch\u001b[39m=\u001b[39mpre_dispatch)\n\u001b[0;32m--> 309\u001b[0m results \u001b[39m=\u001b[39m parallel(\n\u001b[1;32m    310\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    311\u001b[0m         clone(estimator),\n\u001b[1;32m    312\u001b[0m         X,\n\u001b[1;32m    313\u001b[0m         y,\n\u001b[1;32m    314\u001b[0m         scorers,\n\u001b[1;32m    315\u001b[0m         train,\n\u001b[1;32m    316\u001b[0m         test,\n\u001b[1;32m    317\u001b[0m         verbose,\n\u001b[1;32m    318\u001b[0m         \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    319\u001b[0m         fit_params,\n\u001b[1;32m    320\u001b[0m         return_train_score\u001b[39m=\u001b[39;49mreturn_train_score,\n\u001b[1;32m    321\u001b[0m         return_times\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    322\u001b[0m         return_estimator\u001b[39m=\u001b[39;49mreturn_estimator,\n\u001b[1;32m    323\u001b[0m         error_score\u001b[39m=\u001b[39;49merror_score,\n\u001b[1;32m    324\u001b[0m     )\n\u001b[1;32m    325\u001b[0m     \u001b[39mfor\u001b[39;49;00m train, test \u001b[39min\u001b[39;49;00m indices\n\u001b[1;32m    326\u001b[0m )\n\u001b[1;32m    328\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[1;32m    330\u001b[0m \u001b[39m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[1;32m    331\u001b[0m \u001b[39m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \u001b[39m# the correct key.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/sklearn/utils/parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     60\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[1;32m     61\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[1;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     63\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[1;32m     64\u001b[0m )\n\u001b[0;32m---> 65\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[0;32m~/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/joblib/parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1861\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1862\u001b[0m     \u001b[39mnext\u001b[39m(output)\n\u001b[0;32m-> 1863\u001b[0m     \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39m(output)\n\u001b[1;32m   1865\u001b[0m \u001b[39m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1866\u001b[0m \u001b[39m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1867\u001b[0m \u001b[39m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1868\u001b[0m \u001b[39m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1869\u001b[0m \u001b[39m# callback.\u001b[39;00m\n\u001b[1;32m   1870\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n",
      "File \u001b[0;32m~/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/joblib/parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1790\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_batches \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1791\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m-> 1792\u001b[0m res \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1793\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_completed_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1794\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m~/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/sklearn/utils/parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[1;32m    126\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[0;32m--> 127\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:729\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    727\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[1;32m    728\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 729\u001b[0m         estimator\u001b[39m.\u001b[39;49mfit(X_train, y_train, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\n\u001b[1;32m    731\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m    732\u001b[0m     \u001b[39m# Note fit time as time until error\u001b[39;00m\n\u001b[1;32m    733\u001b[0m     fit_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/sklearn/pipeline.py:427\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    425\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpassthrough\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    426\u001b[0m         fit_params_last_step \u001b[39m=\u001b[39m fit_params_steps[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m0\u001b[39m]]\n\u001b[0;32m--> 427\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_final_estimator\u001b[39m.\u001b[39;49mfit(Xt, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params_last_step)\n\u001b[1;32m    429\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:696\u001b[0m, in \u001b[0;36mBaseHistGradientBoosting.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_trees_per_iteration_):\n\u001b[1;32m    679\u001b[0m     grower \u001b[39m=\u001b[39m TreeGrower(\n\u001b[1;32m    680\u001b[0m         X_binned\u001b[39m=\u001b[39mX_binned_train,\n\u001b[1;32m    681\u001b[0m         gradients\u001b[39m=\u001b[39mg_view[:, k],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    694\u001b[0m         n_threads\u001b[39m=\u001b[39mn_threads,\n\u001b[1;32m    695\u001b[0m     )\n\u001b[0;32m--> 696\u001b[0m     grower\u001b[39m.\u001b[39;49mgrow()\n\u001b[1;32m    698\u001b[0m     acc_apply_split_time \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m grower\u001b[39m.\u001b[39mtotal_apply_split_time\n\u001b[1;32m    699\u001b[0m     acc_find_split_time \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m grower\u001b[39m.\u001b[39mtotal_find_split_time\n",
      "File \u001b[0;32m~/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/sklearn/ensemble/_hist_gradient_boosting/grower.py:366\u001b[0m, in \u001b[0;36mTreeGrower.grow\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Grow the tree, from root to leaves.\"\"\"\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msplittable_nodes:\n\u001b[0;32m--> 366\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msplit_next()\n\u001b[1;32m    368\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_apply_shrinkage()\n",
      "File \u001b[0;32m~/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/sklearn/ensemble/_hist_gradient_boosting/grower.py:586\u001b[0m, in \u001b[0;36mTreeGrower.split_next\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[39m# We use the brute O(n_samples) method on the child that has the\u001b[39;00m\n\u001b[1;32m    582\u001b[0m \u001b[39m# smallest number of samples, and the subtraction trick O(n_bins)\u001b[39;00m\n\u001b[1;32m    583\u001b[0m \u001b[39m# on the other one.\u001b[39;00m\n\u001b[1;32m    584\u001b[0m \u001b[39m# Note that both left and right child have the same allowed_features.\u001b[39;00m\n\u001b[1;32m    585\u001b[0m tic \u001b[39m=\u001b[39m time()\n\u001b[0;32m--> 586\u001b[0m smallest_child\u001b[39m.\u001b[39mhistograms \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhistogram_builder\u001b[39m.\u001b[39;49mcompute_histograms_brute(\n\u001b[1;32m    587\u001b[0m     smallest_child\u001b[39m.\u001b[39;49msample_indices, smallest_child\u001b[39m.\u001b[39;49mallowed_features\n\u001b[1;32m    588\u001b[0m )\n\u001b[1;32m    589\u001b[0m largest_child\u001b[39m.\u001b[39mhistograms \u001b[39m=\u001b[39m (\n\u001b[1;32m    590\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhistogram_builder\u001b[39m.\u001b[39mcompute_histograms_subtraction(\n\u001b[1;32m    591\u001b[0m         node\u001b[39m.\u001b[39mhistograms,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    594\u001b[0m     )\n\u001b[1;32m    595\u001b[0m )\n\u001b[1;32m    596\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtotal_compute_hist_time \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m time() \u001b[39m-\u001b[39m tic\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "study3 = create_study(study_name='optimization', direction='maximize', pruner=optuna.pruners.MedianPruner())\n",
    "\n",
    "study3.optimize(lambda trial: objective(trial, selected_pipeline, 'hist'), n_trials=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CATBOOST\n",
    "best_params = study.best_params\n",
    "catboost_model = CatBoostClassifier(**best_params, silent=True)\n",
    "catboost_pipeline = make_pipeline(selected_pipeline, catboost_model)\n",
    "average_cv_result = study.best_trial.values[0]\n",
    "log_results(catboost_pipeline, best_params, average_cv_result, 'catboost')\n",
    "\n",
    "# XGBOOST\n",
    "best_params = study1.best_params\n",
    "xgboost_model = XGBClassifier(**best_params, silent=True)\n",
    "xgboost_pipeline = make_pipeline(selected_pipeline, xgboost_model)\n",
    "average_cv_result = study1.best_trial.values[0]\n",
    "log_results(xgboost_pipeline, best_params, average_cv_result, 'xgboost')\n",
    "\n",
    "# LGBM\n",
    "best_params = study2.best_params\n",
    "lgbm_model = XGBClassifier(**best_params, silent=True)\n",
    "lgbm_pipeline = make_pipeline(selected_pipeline, lgbm_model)\n",
    "average_cv_result = study2.best_trial.values[0]\n",
    "log_results(lgbm_pipeline, best_params, average_cv_result, 'lgbm')\n",
    "\n",
    "# HIST\n",
    "best_params = study3.best_params\n",
    "hist_model = HistGradientBoostingClassifier(**best_params, silent=True)\n",
    "hist_pipeline = make_pipeline(selected_pipeline, hist_model)\n",
    "average_cv_result = study3.best_trial.values[0]\n",
    "log_results(hist_pipeline, best_params, average_cv_result, 'hist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No-Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training has stopped (degenerate solution on iteration 876, probably too small l2-regularization, try to increase it)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16504\n",
      "16504\n",
      "Fold 1, AUC Score on Validation Set: 0.8869434646457282\n",
      "----------------------------------------------------------------------\n",
      "16504\n",
      "16504\n",
      "Fold 2, AUC Score on Validation Set: 0.8823462015325354\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/krystianpietrzak/Documents/ML/Kaggle/Kaggle-S4E1/eda.ipynb Cell 49\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/krystianpietrzak/Documents/ML/Kaggle/Kaggle-S4E1/eda.ipynb#X62sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m X_train, X_val \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39miloc[train_idx], X\u001b[39m.\u001b[39miloc[val_idx]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/krystianpietrzak/Documents/ML/Kaggle/Kaggle-S4E1/eda.ipynb#X62sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m y_train, y_val \u001b[39m=\u001b[39m y[train_idx], y[val_idx]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/krystianpietrzak/Documents/ML/Kaggle/Kaggle-S4E1/eda.ipynb#X62sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m catboost_pipeline\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/krystianpietrzak/Documents/ML/Kaggle/Kaggle-S4E1/eda.ipynb#X62sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# predictions on the validation set\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/krystianpietrzak/Documents/ML/Kaggle/Kaggle-S4E1/eda.ipynb#X62sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(X_val))\n",
      "File \u001b[0;32m~/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/sklearn/pipeline.py:427\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    425\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpassthrough\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    426\u001b[0m         fit_params_last_step \u001b[39m=\u001b[39m fit_params_steps[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m0\u001b[39m]]\n\u001b[0;32m--> 427\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_final_estimator\u001b[39m.\u001b[39;49mfit(Xt, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params_last_step)\n\u001b[1;32m    429\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/catboost/core.py:5100\u001b[0m, in \u001b[0;36mCatBoostClassifier.fit\u001b[0;34m(self, X, y, cat_features, text_features, embedding_features, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[1;32m   5097\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mloss_function\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m params:\n\u001b[1;32m   5098\u001b[0m     CatBoostClassifier\u001b[39m.\u001b[39m_check_is_compatible_loss(params[\u001b[39m'\u001b[39m\u001b[39mloss_function\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m-> 5100\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(X, y, cat_features, text_features, embedding_features, \u001b[39mNone\u001b[39;49;00m, sample_weight, \u001b[39mNone\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m, baseline, use_best_model,\n\u001b[1;32m   5101\u001b[0m           eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period,\n\u001b[1;32m   5102\u001b[0m           silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\n\u001b[1;32m   5103\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/catboost/core.py:2319\u001b[0m, in \u001b[0;36mCatBoost._fit\u001b[0;34m(self, X, y, cat_features, text_features, embedding_features, pairs, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[1;32m   2315\u001b[0m allow_clear_pool \u001b[39m=\u001b[39m train_params[\u001b[39m\"\u001b[39m\u001b[39mallow_clear_pool\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   2317\u001b[0m \u001b[39mwith\u001b[39;00m log_fixup(log_cout, log_cerr), \\\n\u001b[1;32m   2318\u001b[0m     plot_wrapper(plot, plot_file, \u001b[39m'\u001b[39m\u001b[39mTraining plots\u001b[39m\u001b[39m'\u001b[39m, [_get_train_dir(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_params())]):\n\u001b[0;32m-> 2319\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train(\n\u001b[1;32m   2320\u001b[0m         train_pool,\n\u001b[1;32m   2321\u001b[0m         train_params[\u001b[39m\"\u001b[39;49m\u001b[39meval_sets\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m   2322\u001b[0m         params,\n\u001b[1;32m   2323\u001b[0m         allow_clear_pool,\n\u001b[1;32m   2324\u001b[0m         train_params[\u001b[39m\"\u001b[39;49m\u001b[39minit_model\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n\u001b[1;32m   2325\u001b[0m     )\n\u001b[1;32m   2327\u001b[0m \u001b[39m# Have property feature_importance possibly set\u001b[39;00m\n\u001b[1;32m   2328\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_object\u001b[39m.\u001b[39m_get_loss_function_name()\n",
      "File \u001b[0;32m~/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/catboost/core.py:1723\u001b[0m, in \u001b[0;36m_CatBoostBase._train\u001b[0;34m(self, train_pool, test_pool, params, allow_clear_pool, init_model)\u001b[0m\n\u001b[1;32m   1722\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_train\u001b[39m(\u001b[39mself\u001b[39m, train_pool, test_pool, params, allow_clear_pool, init_model):\n\u001b[0;32m-> 1723\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_object\u001b[39m.\u001b[39;49m_train(train_pool, test_pool, params, allow_clear_pool, init_model\u001b[39m.\u001b[39;49m_object \u001b[39mif\u001b[39;49;00m init_model \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m   1724\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_trained_model_attributes()\n",
      "File \u001b[0;32m_catboost.pyx:4645\u001b[0m, in \u001b[0;36m_catboost._CatBoost._train\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_catboost.pyx:4694\u001b[0m, in \u001b[0;36m_catboost._CatBoost._train\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X = train_df[cat_cols+num_cols+['CustomerId']]\n",
    "y = train_df['Exited']\n",
    "\n",
    "# number of folds\n",
    "n_splits = 10\n",
    "\n",
    "#  StratifiedKFold\n",
    "stratkf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "#  cross-validation results\n",
    "cv_results = []\n",
    "\n",
    "# stratified k-fold cross-validation\n",
    "for fold, (train_idx, val_idx) in enumerate(stratkf.split(X, y)):\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "    catboost_pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # predictions on the validation set\n",
    "    print(len(X_val))\n",
    "    y_val_pred_prob = catboost_pipeline.predict_proba(X_val)[:,1]\n",
    "    print(len(y_val_pred_prob))\n",
    "    y_pred = catboost_pipeline.predict(X_val)\n",
    "        \n",
    "\n",
    "    # Evaluating the model\n",
    "    roc_auc = roc_auc_score(y_val, y_val_pred_prob)\n",
    "    print(f'Fold {fold + 1}, AUC Score on Validation Set: {roc_auc}')\n",
    "    print('-'*70)\n",
    "\n",
    "    # results\n",
    "    cv_results.append(roc_auc)\n",
    "\n",
    "# average cross-validation result\n",
    "average_cv_result = sum(cv_results) / n_splits\n",
    "print(f'\\nAverage AUC-score across {n_splits} folds: {average_cv_result}')\n",
    "log_results(catboost_pipeline, best_params, average_cv_result, 'catboost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------\n",
      "encoder\n",
      "catboost\n",
      "{'iterations': 1000, 'learning_rate': 0.0334, 'depth': 6, 'subsample': 0.49, 'random_strength': 0.00527, 'l2_leaf_reg': 19.32, 'model_size_reg': 12.051}\n",
      "0.8899819036038054\n",
      "----------------------------------------------------------------------\n",
      "age_binning, encoder\n",
      "catboost\n",
      "{'iterations': 1000, 'learning_rate': 0.0334, 'depth': 6, 'subsample': 0.49, 'random_strength': 0.00527, 'l2_leaf_reg': 19.32, 'model_size_reg': 12.051}\n",
      "0.8900045215754551\n",
      "----------------------------------------------------------------------\n",
      "age_binning, encoder, scaler, skb\n",
      "catboost\n",
      "{'iterations': 1000, 'learning_rate': 0.0334, 'depth': 6, 'subsample': 0.49, 'random_strength': 0.00527, 'l2_leaf_reg': 19.32, 'model_size_reg': 12.051}\n",
      "0.8747904837669018\n",
      "----------------------------------------------------------------------\n",
      "age_binning, encoder, skb\n",
      " catboost\n",
      "{'iterations': 1000, 'learning_rate': 0.0334, 'depth': 6, 'subsample': 0.49, 'random_strength': 0.00527, 'l2_leaf_reg': 19.32, 'model_size_reg': 12.051}\n",
      "0.8849117288222272\n",
      "----------------------------------------------------------------------\n",
      "age_binning, encoder, skb\n",
      "catboost\n",
      "{'learning_rate': 0.003457984973866827, 'depth': 3, 'subsample': 0.32252251494116085, 'random_strength': 0.23271662765535872, 'l2_leaf_reg': 0.0004027976597397561, 'model_size_reg': 1.4659021812840547e-08}\n",
      "0.8800670024741089\n",
      "----------------------------------------------------------------------\n",
      "arithmetic_combiner, age_binning, encoder, skb\n",
      "catboost\n",
      "{'iterations': 1000, 'learning_rate': 0.0334, 'depth': 6, 'subsample': 0.49, 'random_strength': 0.00527, 'l2_leaf_reg': 19.32, 'model_size_reg': 12.051}\n",
      "0.7775983649827414\n",
      "----------------------------------------------------------------------\n",
      "arithmetic_combiner, age_binning, encoder, scaler, skb\n",
      "catboost\n",
      "{'iterations': 1000, 'learning_rate': 0.0334, 'depth': 6, 'subsample': 0.49, 'random_strength': 0.00527, 'l2_leaf_reg': 19.32, 'model_size_reg': 12.051}\n",
      "0.884887448447204\n",
      "----------------------------------------------------------------------\n",
      "arithmetic_combiner, age_binning, encoder, scaler, skb\n",
      "catboost\n",
      "{'learning_rate': 0.01505677476702907, 'depth': 10, 'subsample': 0.2716781821240621, 'random_strength': 4.83060093374094e-05, 'l2_leaf_reg': 0.00020833337972882073, 'model_size_reg': 4.043233260462883}\n",
      "0.8838625770001548\n",
      "----------------------------------------------------------------------\n",
      "arithmetic_combiner, age_binning, encoder, scaler, skb\n",
      "xgboost\n",
      "{'learning_rate': 0.04417184361012935, 'max_depth': 7, 'subsample': 0.37237101176213605, 'colsample_bytree': 0.7792720453028117, 'min_child_weight': 14}\n",
      "0.8837995482597867\n",
      "----------------------------------------------------------------------\n",
      "arithmetic_combiner, age_binning, encoder, scaler, skb\n",
      "xgboost\n",
      "{'learning_rate': 0.04417184361012935, 'max_depth': 7, 'subsample': 0.37237101176213605, 'colsample_bytree': 0.7792720453028117, 'min_child_weight': 14}\n",
      "0.8837995482597867\n",
      "----------------------------------------------------------------------\n",
      "arithmetic_combiner, age_binning, encoder, scaler, skb\n",
      "catboost\n",
      "{'learning_rate': 0.01505677476702907, 'depth': 10, 'subsample': 0.2716781821240621, 'random_strength': 4.83060093374094e-05, 'l2_leaf_reg': 0.00020833337972882073, 'model_size_reg': 4.043233260462883}\n",
      "0.8855715710213271\n",
      "----------------------------------------------------------------------\n",
      "balance_zero_transformer, age_binning, encoder\n",
      "catboost\n",
      "{'learning_rate': 0.01505677476702907, 'depth': 10, 'subsample': 0.2716781821240621, 'random_strength': 4.83060093374094e-05, 'l2_leaf_reg': 0.00020833337972882073, 'model_size_reg': 4.043233260462883}\n",
      "0.8862681085182832\n",
      "----------------------------------------------------------------------\n",
      "agg_balance_sum, balance_zero_transformer, age_binning, encoder, skb\n",
      "catboost\n",
      "{'learning_rate': 0.08984614537243998, 'depth': 9, 'subsample': 0.8920454231792831, 'random_strength': 0.0025669741381674537, 'l2_leaf_reg': 1.3091379303049552e-08, 'model_size_reg': 39.39005861220538}\n",
      "0.887769109898462\n",
      "----------------------------------------------------------------------\n",
      "agg_balance_sum, balance_zero_transformer, age_binning, encoder, skb\n",
      "xgboost\n",
      "{'learning_rate': 0.0995227238765534, 'max_depth': 6, 'subsample': 0.8974781754027489, 'colsample_bytree': 0.41327513349222667, 'min_child_weight': 11}\n",
      "0.8879057538770502\n",
      "----------------------------------------------------------------------\n",
      "agg_balance_sum, balance_zero_transformer, age_binning, encoder, skb\n",
      "lgbm\n",
      "{'learning_rate': 0.0998967279943594, 'max_depth': 7, 'subsample': 0.5231345460679563, 'colsample_bytree': 0.7963988397680257, 'min_child_weight': 9}\n",
      "0.887927758797741\n",
      "----------------------------------------------------------------------\n",
      "agg_balance_sum, balance_zero_transformer, age_binning, encoder, skb\n",
      "catboost\n",
      "{'learning_rate': 0.08984614537243998, 'depth': 9, 'subsample': 0.8920454231792831, 'random_strength': 0.0025669741381674537, 'l2_leaf_reg': 1.3091379303049552e-08, 'model_size_reg': 39.39005861220538}\n",
      "0.887769109898462\n",
      "----------------------------------------------------------------------\n",
      "agg_balance_sum, balance_zero_transformer, age_binning, encoder, skb\n",
      "xgboost\n",
      "{'learning_rate': 0.0995227238765534, 'max_depth': 6, 'subsample': 0.8974781754027489, 'colsample_bytree': 0.41327513349222667, 'min_child_weight': 11}\n",
      "0.8879057538770502\n",
      "----------------------------------------------------------------------\n",
      "agg_balance_sum, balance_zero_transformer, age_binning, encoder, skb\n",
      "lgbm\n",
      "{'learning_rate': 0.0998967279943594, 'max_depth': 7, 'subsample': 0.5231345460679563, 'colsample_bytree': 0.7963988397680257, 'min_child_weight': 9}\n",
      "0.887927758797741\n",
      "----------------------------------------------------------------------\n",
      "agg_balance_sum, balance_zero_transformer, age_binning, encoder, skb\n",
      "catboost\n",
      "{'learning_rate': 0.08984614537243998, 'depth': 9, 'subsample': 0.8920454231792831, 'random_strength': 0.0025669741381674537, 'l2_leaf_reg': 1.3091379303049552e-08, 'model_size_reg': 39.39005861220538}\n",
      "0.887769109898462\n",
      "----------------------------------------------------------------------\n",
      "agg_balance_sum, balance_zero_transformer, age_binning, encoder, skb\n",
      "xgboost\n",
      "{'learning_rate': 0.0995227238765534, 'max_depth': 6, 'subsample': 0.8974781754027489, 'colsample_bytree': 0.41327513349222667, 'min_child_weight': 11}\n",
      "0.8879057538770502\n",
      "----------------------------------------------------------------------\n",
      "agg_balance_sum, balance_zero_transformer, age_binning, encoder, skb\n",
      "lgbm\n",
      "{'learning_rate': 0.0998967279943594, 'max_depth': 7, 'subsample': 0.5231345460679563, 'colsample_bytree': 0.7963988397680257, 'min_child_weight': 9}\n",
      "0.887927758797741\n"
     ]
    }
   ],
   "source": [
    "df_log = pd.read_csv('log.csv')\n",
    "plt.figure(figsize=(20, 6))\n",
    "ax = sns.barplot(data=df_log, x='Time', y='ROC AUC')\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('ROC AUC Over Time')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('ROC AUC')\n",
    "\n",
    "# Adding the text on the bars\n",
    "for p in ax.patches:\n",
    "    ax.annotate(format(p.get_height(), '.6f'), \n",
    "                (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                ha = 'center', va = 'center', \n",
    "                xytext = (0, 10), \n",
    "                textcoords = 'offset points')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "for param, pipeline, model, score  in zip(df_log['Params'], df_log['Pipeline'], df_log['Model'], df_log['ROC AUC']):\n",
    "    print('-'*70)\n",
    "    print(pipeline)\n",
    "    print(model)\n",
    "    print(param)\n",
    "    print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/krystianpietrzak/Documents/ML/Kaggle/Kaggle-S4E1/eda.ipynb Cell 49\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/krystianpietrzak/Documents/ML/Kaggle/Kaggle-S4E1/eda.ipynb#Y100sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m transformed_feature_names \u001b[39m=\u001b[39m catboost_pipeline\u001b[39m.\u001b[39mnamed_steps[\u001b[39m'\u001b[39m\u001b[39mpipeline\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mencoder\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mget_feature_names_out()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/krystianpietrzak/Documents/ML/Kaggle/Kaggle-S4E1/eda.ipynb#Y100sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# Match importances to feature names\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/krystianpietrzak/Documents/ML/Kaggle/Kaggle-S4E1/eda.ipynb#Y100sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m importances_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mDataFrame({\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/krystianpietrzak/Documents/ML/Kaggle/Kaggle-S4E1/eda.ipynb#Y100sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39m'\u001b[39;49m\u001b[39mFeature\u001b[39;49m\u001b[39m'\u001b[39;49m: transformed_feature_names,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/krystianpietrzak/Documents/ML/Kaggle/Kaggle-S4E1/eda.ipynb#Y100sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39m'\u001b[39;49m\u001b[39mImportance\u001b[39;49m\u001b[39m'\u001b[39;49m: feature_importances\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/krystianpietrzak/Documents/ML/Kaggle/Kaggle-S4E1/eda.ipynb#Y100sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m })\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/krystianpietrzak/Documents/ML/Kaggle/Kaggle-S4E1/eda.ipynb#Y100sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# Sort the dataframe by importance\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/krystianpietrzak/Documents/ML/Kaggle/Kaggle-S4E1/eda.ipynb#Y100sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m importances_df\u001b[39m.\u001b[39msort_values(by\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mImportance\u001b[39m\u001b[39m'\u001b[39m, ascending\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/pandas/core/frame.py:733\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    727\u001b[0m     mgr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_mgr(\n\u001b[1;32m    728\u001b[0m         data, axes\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m\"\u001b[39m: index, \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: columns}, dtype\u001b[39m=\u001b[39mdtype, copy\u001b[39m=\u001b[39mcopy\n\u001b[1;32m    729\u001b[0m     )\n\u001b[1;32m    731\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, \u001b[39mdict\u001b[39m):\n\u001b[1;32m    732\u001b[0m     \u001b[39m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[0;32m--> 733\u001b[0m     mgr \u001b[39m=\u001b[39m dict_to_mgr(data, index, columns, dtype\u001b[39m=\u001b[39;49mdtype, copy\u001b[39m=\u001b[39;49mcopy, typ\u001b[39m=\u001b[39;49mmanager)\n\u001b[1;32m    734\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ma\u001b[39m.\u001b[39mMaskedArray):\n\u001b[1;32m    735\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mma\u001b[39;00m \u001b[39mimport\u001b[39;00m mrecords\n",
      "File \u001b[0;32m~/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/pandas/core/internals/construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    500\u001b[0m         \u001b[39m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[1;32m    501\u001b[0m         arrays \u001b[39m=\u001b[39m [x\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(x, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39melse\u001b[39;00m x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m arrays]\n\u001b[0;32m--> 503\u001b[0m \u001b[39mreturn\u001b[39;00m arrays_to_mgr(arrays, columns, index, dtype\u001b[39m=\u001b[39;49mdtype, typ\u001b[39m=\u001b[39;49mtyp, consolidate\u001b[39m=\u001b[39;49mcopy)\n",
      "File \u001b[0;32m~/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/pandas/core/internals/construction.py:114\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[39mif\u001b[39;00m verify_integrity:\n\u001b[1;32m    112\u001b[0m     \u001b[39m# figure out the index, if necessary\u001b[39;00m\n\u001b[1;32m    113\u001b[0m     \u001b[39mif\u001b[39;00m index \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 114\u001b[0m         index \u001b[39m=\u001b[39m _extract_index(arrays)\n\u001b[1;32m    115\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m         index \u001b[39m=\u001b[39m ensure_index(index)\n",
      "File \u001b[0;32m~/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/pandas/core/internals/construction.py:677\u001b[0m, in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    675\u001b[0m lengths \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mset\u001b[39m(raw_lengths))\n\u001b[1;32m    676\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(lengths) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 677\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mAll arrays must be of the same length\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    679\u001b[0m \u001b[39mif\u001b[39;00m have_dicts:\n\u001b[1;32m    680\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    681\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    682\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "# Get feature importances\n",
    "feature_importances = catboost_pipeline.named_steps['catboostclassifier'].get_feature_importance()\n",
    "\n",
    "# Get feature names from ColumnTransformer\n",
    "transformed_feature_names = catboost_pipeline.named_steps['pipeline']['encoder'].get_feature_names_out()\n",
    "\n",
    "# Match importances to feature names\n",
    "importances_df = pd.DataFrame({\n",
    "    'Feature': transformed_feature_names,\n",
    "    'Importance': feature_importances\n",
    "})\n",
    "\n",
    "# Sort the dataframe by importance\n",
    "importances_df.sort_values(by='Importance', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning of weights for ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: Trial, xgboost_pipeline, lgbm_pipeline, hist_pipeline):\n",
    "    X = train_df[cat_cols+num_cols+['CustomerId']]\n",
    "    y = train_df['Exited']\n",
    "\n",
    "    # Suggest weights for the ensemble components\n",
    "    weight_xgb = trial.suggest_float('xgb_weight', 0.1, 1)\n",
    "    weight_lgbm = trial.suggest_float('lgbm_weight', 0.1, 1)\n",
    "    weight_hist = trial.suggest_float('hist_weight', 0.1, 1)\n",
    "\n",
    "    # Create the ensemble model with the suggested weights\n",
    "    ensemble_model = VotingClassifier(\n",
    "        estimators=[\n",
    "            ('xgb', xgboost_pipeline),\n",
    "            ('lgbm', lgbm_pipeline),\n",
    "            ('hist', hist_pipeline),\n",
    "        ],\n",
    "        voting='soft',\n",
    "        weights=[weight_xgb, weight_lgbm, weight_hist]\n",
    "    )\n",
    "\n",
    "    # Perform cross-validation\n",
    "    skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    roc_auc_scorer = make_scorer(roc_auc_score, needs_proba=True)\n",
    "    scores = cross_val_score(ensemble_model, X, y, scoring=roc_auc_scorer, cv=skf)\n",
    "\n",
    "    # Return the mean of the ROC AUC scores\n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 22:04:23,298] A new study created in memory with name: no-name-d0abcaac-983f-462f-8eed-e42365d789f7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001393 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001103 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001003 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001042 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001006 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001019 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001060 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001041 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001027 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001008 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 22:05:50,776] Trial 0 finished with value: 0.8890824638865074 and parameters: {'xgb_weight': 0.9476128707220737, 'lgbm_weight': 0.5626673694518467, 'hist_weight': 0.23987315205853274}. Best is trial 0 with value: 0.8890824638865074.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000850 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000985 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000917 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001103 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001024 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001031 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001017 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001128 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001016 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000987 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 22:07:10,059] Trial 1 finished with value: 0.8890281342528604 and parameters: {'xgb_weight': 0.3936217014753463, 'lgbm_weight': 0.9401331240740378, 'hist_weight': 0.39003852330410965}. Best is trial 0 with value: 0.8890824638865074.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001120 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001082 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001101 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000918 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001081 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000992 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001021 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001236 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001031 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001067 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 22:08:32,131] Trial 2 finished with value: 0.8890102859314114 and parameters: {'xgb_weight': 0.23764585411232053, 'lgbm_weight': 0.5140193842384857, 'hist_weight': 0.6252420621805005}. Best is trial 0 with value: 0.8890824638865074.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001123 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001192 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001120 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001354 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001118 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001244 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001120 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001110 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001148 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001051 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 22:09:59,181] Trial 3 finished with value: 0.8890316285223344 and parameters: {'xgb_weight': 0.9870038611323773, 'lgbm_weight': 0.7687139961466076, 'hist_weight': 0.924289191527554}. Best is trial 0 with value: 0.8890824638865074.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001086 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001208 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001262 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31428, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000972 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211594 -> initscore=-1.315346\n",
      "[LightGBM] [Info] Start training from score -1.315346\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001083 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001115 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001203 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001056 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001054 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001349 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148531, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315314\n",
      "[LightGBM] [Info] Start training from score -1.315314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-18 22:11:31,340] Trial 4 finished with value: 0.8889291604103967 and parameters: {'xgb_weight': 0.24601161753223788, 'lgbm_weight': 0.24800836599461784, 'hist_weight': 0.6353232763878843}. Best is trial 0 with value: 0.8890824638865074.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 31429, number of negative: 117101\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001255 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1112\n",
      "[LightGBM] [Info] Number of data points in the train set: 148530, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211600 -> initscore=-1.315306\n",
      "[LightGBM] [Info] Start training from score -1.315306\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2024-01-18 22:11:39,400] Trial 5 failed with parameters: {'xgb_weight': 0.24325370170272886, 'lgbm_weight': 0.29519821475632324, 'hist_weight': 0.5423559547231966} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/krystianpietrzak/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/var/folders/x1/2j32gjvd4v16s6kf306sfdcw0000gn/T/ipykernel_45424/2569802937.py\", line 2, in <lambda>\n",
      "    study_ensemble.optimize(lambda trial: objective(trial, xgboost_pipeline, lgbm_pipeline, hist_pipeline), n_trials=20)\n",
      "                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/x1/2j32gjvd4v16s6kf306sfdcw0000gn/T/ipykernel_45424/773533221.py\", line 24, in objective\n",
      "    scores = cross_val_score(ensemble_model, X, y, scoring=roc_auc_scorer, cv=skf)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/krystianpietrzak/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 562, in cross_val_score\n",
      "    cv_results = cross_validate(\n",
      "                 ^^^^^^^^^^^^^^^\n",
      "  File \"/Users/krystianpietrzak/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/sklearn/utils/_param_validation.py\", line 214, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/krystianpietrzak/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 309, in cross_validate\n",
      "    results = parallel(\n",
      "              ^^^^^^^^^\n",
      "  File \"/Users/krystianpietrzak/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/sklearn/utils/parallel.py\", line 65, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/krystianpietrzak/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/joblib/parallel.py\", line 1863, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "                                                ^^^^^^^^^^^^\n",
      "  File \"/Users/krystianpietrzak/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/joblib/parallel.py\", line 1792, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/krystianpietrzak/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/sklearn/utils/parallel.py\", line 127, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/krystianpietrzak/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/sklearn/model_selection/_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/krystianpietrzak/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/sklearn/base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/krystianpietrzak/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/sklearn/ensemble/_voting.py\", line 349, in fit\n",
      "    return super().fit(X, transformed_y, sample_weight)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/krystianpietrzak/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/sklearn/ensemble/_voting.py\", line 81, in fit\n",
      "    self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/krystianpietrzak/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/sklearn/utils/parallel.py\", line 65, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/krystianpietrzak/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/joblib/parallel.py\", line 1863, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "                                                ^^^^^^^^^^^^\n",
      "  File \"/Users/krystianpietrzak/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/joblib/parallel.py\", line 1792, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/krystianpietrzak/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/sklearn/utils/parallel.py\", line 127, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/krystianpietrzak/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/sklearn/ensemble/_base.py\", line 36, in _fit_single_estimator\n",
      "    estimator.fit(X, y)\n",
      "  File \"/Users/krystianpietrzak/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/sklearn/base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/krystianpietrzak/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/sklearn/pipeline.py\", line 427, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/Users/krystianpietrzak/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/sklearn/base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/krystianpietrzak/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py\", line 696, in fit\n",
      "    grower.grow()\n",
      "  File \"/Users/krystianpietrzak/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/sklearn/ensemble/_hist_gradient_boosting/grower.py\", line 366, in grow\n",
      "    self.split_next()\n",
      "  File \"/Users/krystianpietrzak/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/sklearn/ensemble/_hist_gradient_boosting/grower.py\", line 470, in split_next\n",
      "    ) = self.splitter.split_indices(node.split_info, node.sample_indices)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "[W 2024-01-18 22:11:39,404] Trial 5 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/krystianpietrzak/Documents/ML/Kaggle/Kaggle-S4E1/eda.ipynb Cell 57\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/krystianpietrzak/Documents/ML/Kaggle/Kaggle-S4E1/eda.ipynb#Y262sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m study_ensemble \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(direction\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmaximize\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/krystianpietrzak/Documents/ML/Kaggle/Kaggle-S4E1/eda.ipynb#Y262sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m study_ensemble\u001b[39m.\u001b[39;49moptimize(\u001b[39mlambda\u001b[39;49;00m trial: objective(trial, xgboost_pipeline, lgbm_pipeline, hist_pipeline), n_trials\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m)\n",
      "File \u001b[0;32m~/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/optuna/study/study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[1;32m    349\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    357\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    358\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \n\u001b[1;32m    361\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 451\u001b[0m     _optimize(\n\u001b[1;32m    452\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    453\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[1;32m    454\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[1;32m    455\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    456\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[1;32m    457\u001b[0m         catch\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(catch) \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(catch, Iterable) \u001b[39melse\u001b[39;49;00m (catch,),\n\u001b[1;32m    458\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    459\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[1;32m    460\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[1;32m    461\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/optuna/study/_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[1;32m     67\u001b[0m             study,\n\u001b[1;32m     68\u001b[0m             func,\n\u001b[1;32m     69\u001b[0m             n_trials,\n\u001b[1;32m     70\u001b[0m             timeout,\n\u001b[1;32m     71\u001b[0m             catch,\n\u001b[1;32m     72\u001b[0m             callbacks,\n\u001b[1;32m     73\u001b[0m             gc_after_trial,\n\u001b[1;32m     74\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     75\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     76\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m     77\u001b[0m         )\n\u001b[1;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/optuna/study/_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[1;32m    164\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/optuna/study/_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    246\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    247\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[1;32m    248\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    250\u001b[0m ):\n\u001b[0;32m--> 251\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[1;32m    252\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/optuna/study/_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[1;32m    199\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 200\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[1;32m    201\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    202\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "\u001b[1;32m/Users/krystianpietrzak/Documents/ML/Kaggle/Kaggle-S4E1/eda.ipynb Cell 57\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/krystianpietrzak/Documents/ML/Kaggle/Kaggle-S4E1/eda.ipynb#Y262sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m study_ensemble \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(direction\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmaximize\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/krystianpietrzak/Documents/ML/Kaggle/Kaggle-S4E1/eda.ipynb#Y262sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m study_ensemble\u001b[39m.\u001b[39moptimize(\u001b[39mlambda\u001b[39;00m trial: objective(trial, xgboost_pipeline, lgbm_pipeline, hist_pipeline), n_trials\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m)\n",
      "\u001b[1;32m/Users/krystianpietrzak/Documents/ML/Kaggle/Kaggle-S4E1/eda.ipynb Cell 57\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/krystianpietrzak/Documents/ML/Kaggle/Kaggle-S4E1/eda.ipynb#Y262sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m skf \u001b[39m=\u001b[39m StratifiedKFold(n_splits\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/krystianpietrzak/Documents/ML/Kaggle/Kaggle-S4E1/eda.ipynb#Y262sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m roc_auc_scorer \u001b[39m=\u001b[39m make_scorer(roc_auc_score, needs_proba\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/krystianpietrzak/Documents/ML/Kaggle/Kaggle-S4E1/eda.ipynb#Y262sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m scores \u001b[39m=\u001b[39m cross_val_score(ensemble_model, X, y, scoring\u001b[39m=\u001b[39;49mroc_auc_scorer, cv\u001b[39m=\u001b[39;49mskf)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/krystianpietrzak/Documents/ML/Kaggle/Kaggle-S4E1/eda.ipynb#Y262sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m# Return the mean of the ROC AUC scores\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/krystianpietrzak/Documents/ML/Kaggle/Kaggle-S4E1/eda.ipynb#Y262sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mmean(scores)\n",
      "File \u001b[0;32m~/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:562\u001b[0m, in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[39m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[1;32m    560\u001b[0m scorer \u001b[39m=\u001b[39m check_scoring(estimator, scoring\u001b[39m=\u001b[39mscoring)\n\u001b[0;32m--> 562\u001b[0m cv_results \u001b[39m=\u001b[39m cross_validate(\n\u001b[1;32m    563\u001b[0m     estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[1;32m    564\u001b[0m     X\u001b[39m=\u001b[39;49mX,\n\u001b[1;32m    565\u001b[0m     y\u001b[39m=\u001b[39;49my,\n\u001b[1;32m    566\u001b[0m     groups\u001b[39m=\u001b[39;49mgroups,\n\u001b[1;32m    567\u001b[0m     scoring\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mscore\u001b[39;49m\u001b[39m\"\u001b[39;49m: scorer},\n\u001b[1;32m    568\u001b[0m     cv\u001b[39m=\u001b[39;49mcv,\n\u001b[1;32m    569\u001b[0m     n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[1;32m    570\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    571\u001b[0m     fit_params\u001b[39m=\u001b[39;49mfit_params,\n\u001b[1;32m    572\u001b[0m     pre_dispatch\u001b[39m=\u001b[39;49mpre_dispatch,\n\u001b[1;32m    573\u001b[0m     error_score\u001b[39m=\u001b[39;49merror_score,\n\u001b[1;32m    574\u001b[0m )\n\u001b[1;32m    575\u001b[0m \u001b[39mreturn\u001b[39;00m cv_results[\u001b[39m\"\u001b[39m\u001b[39mtest_score\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:214\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    209\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m    210\u001b[0m         skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m    211\u001b[0m             prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    212\u001b[0m         )\n\u001b[1;32m    213\u001b[0m     ):\n\u001b[0;32m--> 214\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    215\u001b[0m \u001b[39mexcept\u001b[39;00m InvalidParameterError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    216\u001b[0m     \u001b[39m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[39m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[39m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[39m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     msg \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\n\u001b[1;32m    221\u001b[0m         \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+ must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    223\u001b[0m         \u001b[39mstr\u001b[39m(e),\n\u001b[1;32m    224\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:309\u001b[0m, in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[39m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[1;32m    307\u001b[0m \u001b[39m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[1;32m    308\u001b[0m parallel \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39mn_jobs, verbose\u001b[39m=\u001b[39mverbose, pre_dispatch\u001b[39m=\u001b[39mpre_dispatch)\n\u001b[0;32m--> 309\u001b[0m results \u001b[39m=\u001b[39m parallel(\n\u001b[1;32m    310\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    311\u001b[0m         clone(estimator),\n\u001b[1;32m    312\u001b[0m         X,\n\u001b[1;32m    313\u001b[0m         y,\n\u001b[1;32m    314\u001b[0m         scorers,\n\u001b[1;32m    315\u001b[0m         train,\n\u001b[1;32m    316\u001b[0m         test,\n\u001b[1;32m    317\u001b[0m         verbose,\n\u001b[1;32m    318\u001b[0m         \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    319\u001b[0m         fit_params,\n\u001b[1;32m    320\u001b[0m         return_train_score\u001b[39m=\u001b[39;49mreturn_train_score,\n\u001b[1;32m    321\u001b[0m         return_times\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    322\u001b[0m         return_estimator\u001b[39m=\u001b[39;49mreturn_estimator,\n\u001b[1;32m    323\u001b[0m         error_score\u001b[39m=\u001b[39;49merror_score,\n\u001b[1;32m    324\u001b[0m     )\n\u001b[1;32m    325\u001b[0m     \u001b[39mfor\u001b[39;49;00m train, test \u001b[39min\u001b[39;49;00m indices\n\u001b[1;32m    326\u001b[0m )\n\u001b[1;32m    328\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[1;32m    330\u001b[0m \u001b[39m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[1;32m    331\u001b[0m \u001b[39m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \u001b[39m# the correct key.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/sklearn/utils/parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     60\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[1;32m     61\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[1;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     63\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[1;32m     64\u001b[0m )\n\u001b[0;32m---> 65\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[0;32m~/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/joblib/parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1861\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1862\u001b[0m     \u001b[39mnext\u001b[39m(output)\n\u001b[0;32m-> 1863\u001b[0m     \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39m(output)\n\u001b[1;32m   1865\u001b[0m \u001b[39m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1866\u001b[0m \u001b[39m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1867\u001b[0m \u001b[39m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1868\u001b[0m \u001b[39m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1869\u001b[0m \u001b[39m# callback.\u001b[39;00m\n\u001b[1;32m   1870\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n",
      "File \u001b[0;32m~/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/joblib/parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1790\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_batches \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1791\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m-> 1792\u001b[0m res \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1793\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_completed_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1794\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m~/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/sklearn/utils/parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[1;32m    126\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[0;32m--> 127\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:729\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    727\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[1;32m    728\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 729\u001b[0m         estimator\u001b[39m.\u001b[39;49mfit(X_train, y_train, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\n\u001b[1;32m    731\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m    732\u001b[0m     \u001b[39m# Note fit time as time until error\u001b[39;00m\n\u001b[1;32m    733\u001b[0m     fit_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/sklearn/ensemble/_voting.py:349\u001b[0m, in \u001b[0;36mVotingClassifier.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mle_\u001b[39m.\u001b[39mclasses_\n\u001b[1;32m    347\u001b[0m transformed_y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mle_\u001b[39m.\u001b[39mtransform(y)\n\u001b[0;32m--> 349\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit(X, transformed_y, sample_weight)\n",
      "File \u001b[0;32m~/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/sklearn/ensemble/_voting.py:81\u001b[0m, in \u001b[0;36m_BaseVoting.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweights \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweights) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators):\n\u001b[1;32m     76\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     77\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNumber of `estimators` and weights must be equal; got\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     78\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweights)\u001b[39m}\u001b[39;00m\u001b[39m weights, \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators)\u001b[39m}\u001b[39;00m\u001b[39m estimators\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     79\u001b[0m     )\n\u001b[0;32m---> 81\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_ \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs)(\n\u001b[1;32m     82\u001b[0m     delayed(_fit_single_estimator)(\n\u001b[1;32m     83\u001b[0m         clone(clf),\n\u001b[1;32m     84\u001b[0m         X,\n\u001b[1;32m     85\u001b[0m         y,\n\u001b[1;32m     86\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m     87\u001b[0m         message_clsname\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mVoting\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     88\u001b[0m         message\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_log_message(names[idx], idx \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m, \u001b[39mlen\u001b[39;49m(clfs)),\n\u001b[1;32m     89\u001b[0m     )\n\u001b[1;32m     90\u001b[0m     \u001b[39mfor\u001b[39;49;00m idx, clf \u001b[39min\u001b[39;49;00m \u001b[39menumerate\u001b[39;49m(clfs)\n\u001b[1;32m     91\u001b[0m     \u001b[39mif\u001b[39;49;00m clf \u001b[39m!=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mdrop\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m     92\u001b[0m )\n\u001b[1;32m     94\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnamed_estimators_ \u001b[39m=\u001b[39m Bunch()\n\u001b[1;32m     96\u001b[0m \u001b[39m# Uses 'drop' as placeholder for dropped estimators\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/sklearn/utils/parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     60\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[1;32m     61\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[1;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     63\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[1;32m     64\u001b[0m )\n\u001b[0;32m---> 65\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[0;32m~/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/joblib/parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1861\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1862\u001b[0m     \u001b[39mnext\u001b[39m(output)\n\u001b[0;32m-> 1863\u001b[0m     \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39m(output)\n\u001b[1;32m   1865\u001b[0m \u001b[39m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1866\u001b[0m \u001b[39m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1867\u001b[0m \u001b[39m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1868\u001b[0m \u001b[39m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1869\u001b[0m \u001b[39m# callback.\u001b[39;00m\n\u001b[1;32m   1870\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n",
      "File \u001b[0;32m~/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/joblib/parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1790\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_batches \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1791\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m-> 1792\u001b[0m res \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1793\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_completed_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1794\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m~/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/sklearn/utils/parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[1;32m    126\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[0;32m--> 127\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/sklearn/ensemble/_base.py:36\u001b[0m, in \u001b[0;36m_fit_single_estimator\u001b[0;34m(estimator, X, y, sample_weight, message_clsname, message)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     \u001b[39mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[0;32m---> 36\u001b[0m         estimator\u001b[39m.\u001b[39;49mfit(X, y)\n\u001b[1;32m     37\u001b[0m \u001b[39mreturn\u001b[39;00m estimator\n",
      "File \u001b[0;32m~/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/sklearn/pipeline.py:427\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    425\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpassthrough\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    426\u001b[0m         fit_params_last_step \u001b[39m=\u001b[39m fit_params_steps[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m0\u001b[39m]]\n\u001b[0;32m--> 427\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_final_estimator\u001b[39m.\u001b[39;49mfit(Xt, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params_last_step)\n\u001b[1;32m    429\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py:696\u001b[0m, in \u001b[0;36mBaseHistGradientBoosting.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_trees_per_iteration_):\n\u001b[1;32m    679\u001b[0m     grower \u001b[39m=\u001b[39m TreeGrower(\n\u001b[1;32m    680\u001b[0m         X_binned\u001b[39m=\u001b[39mX_binned_train,\n\u001b[1;32m    681\u001b[0m         gradients\u001b[39m=\u001b[39mg_view[:, k],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    694\u001b[0m         n_threads\u001b[39m=\u001b[39mn_threads,\n\u001b[1;32m    695\u001b[0m     )\n\u001b[0;32m--> 696\u001b[0m     grower\u001b[39m.\u001b[39;49mgrow()\n\u001b[1;32m    698\u001b[0m     acc_apply_split_time \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m grower\u001b[39m.\u001b[39mtotal_apply_split_time\n\u001b[1;32m    699\u001b[0m     acc_find_split_time \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m grower\u001b[39m.\u001b[39mtotal_find_split_time\n",
      "File \u001b[0;32m~/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/sklearn/ensemble/_hist_gradient_boosting/grower.py:366\u001b[0m, in \u001b[0;36mTreeGrower.grow\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Grow the tree, from root to leaves.\"\"\"\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msplittable_nodes:\n\u001b[0;32m--> 366\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msplit_next()\n\u001b[1;32m    368\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_apply_shrinkage()\n",
      "File \u001b[0;32m~/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/sklearn/ensemble/_hist_gradient_boosting/grower.py:470\u001b[0m, in \u001b[0;36mTreeGrower.split_next\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    463\u001b[0m node \u001b[39m=\u001b[39m heappop(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msplittable_nodes)\n\u001b[1;32m    465\u001b[0m tic \u001b[39m=\u001b[39m time()\n\u001b[1;32m    466\u001b[0m (\n\u001b[1;32m    467\u001b[0m     sample_indices_left,\n\u001b[1;32m    468\u001b[0m     sample_indices_right,\n\u001b[1;32m    469\u001b[0m     right_child_pos,\n\u001b[0;32m--> 470\u001b[0m ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msplitter\u001b[39m.\u001b[39;49msplit_indices(node\u001b[39m.\u001b[39;49msplit_info, node\u001b[39m.\u001b[39;49msample_indices)\n\u001b[1;32m    471\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtotal_apply_split_time \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m time() \u001b[39m-\u001b[39m tic\n\u001b[1;32m    473\u001b[0m depth \u001b[39m=\u001b[39m node\u001b[39m.\u001b[39mdepth \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "study_ensemble = optuna.create_study(direction='maximize', pruner=optuna.pruners.MedianPruner())\n",
    "study_ensemble.optimize(lambda trial: objective(trial, xgboost_pipeline, lgbm_pipeline, hist_pipeline), n_trials=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_ensemble = {'xgb_weight': 0.9476128707220737, 'lgbm_weight': 0.5626673694518467, 'hist_weight': 0.23987315205853274}\n",
    "ensemble_model = VotingClassifier(estimators=[\n",
    "                                                ('xgb', xgboost_pipeline),\n",
    "                                                ('lgbm', lgbm_pipeline),\n",
    "                                                ('hist', hist_pipeline),\n",
    "                                             ], voting='soft', weights = [best_params_ensemble['xgb_weight'], best_params_ensemble['lgbm_weight'], best_params_ensemble['hist_weight']]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_pipeline.fit(train_df[cat_cols + num_cols + ['CustomerId']], train_df['Exited'])\n",
    "test_probabilities = xgboost_pipeline.predict_proba(test_df[cat_cols + num_cols + ['CustomerId']])[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [
    {
     "ename": "CatBoostError",
     "evalue": "/Users/zomb-ml-platform-msk/go-agent-21.2.0/pipelines/BuildMaster/catboost.git/catboost/private/libs/algo/tensor_search_helpers.cpp:99: This should be unreachable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCatBoostError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m/Users/krystianpietrzak/Documents/ML/Kaggle/Kaggle-S4E1/eda.ipynb Cell 60\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/krystianpietrzak/Documents/ML/Kaggle/Kaggle-S4E1/eda.ipynb#Y140sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m catboost_pipeline\u001b[39m.\u001b[39;49mfit(train_df[cat_cols \u001b[39m+\u001b[39;49m num_cols\u001b[39m+\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mCustomerId\u001b[39;49m\u001b[39m'\u001b[39;49m]], train_df[\u001b[39m'\u001b[39;49m\u001b[39mExited\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/krystianpietrzak/Documents/ML/Kaggle/Kaggle-S4E1/eda.ipynb#Y140sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m test_probabilities \u001b[39m=\u001b[39m catboost_pipeline\u001b[39m.\u001b[39mpredict_proba(test_df[cat_cols \u001b[39m+\u001b[39m num_cols])[:,\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/sklearn/pipeline.py:427\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    425\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpassthrough\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    426\u001b[0m         fit_params_last_step \u001b[39m=\u001b[39m fit_params_steps[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m0\u001b[39m]]\n\u001b[0;32m--> 427\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_final_estimator\u001b[39m.\u001b[39;49mfit(Xt, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params_last_step)\n\u001b[1;32m    429\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/catboost/core.py:5100\u001b[0m, in \u001b[0;36mCatBoostClassifier.fit\u001b[0;34m(self, X, y, cat_features, text_features, embedding_features, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[1;32m   5097\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mloss_function\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m params:\n\u001b[1;32m   5098\u001b[0m     CatBoostClassifier\u001b[39m.\u001b[39m_check_is_compatible_loss(params[\u001b[39m'\u001b[39m\u001b[39mloss_function\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m-> 5100\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(X, y, cat_features, text_features, embedding_features, \u001b[39mNone\u001b[39;49;00m, sample_weight, \u001b[39mNone\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m, baseline, use_best_model,\n\u001b[1;32m   5101\u001b[0m           eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period,\n\u001b[1;32m   5102\u001b[0m           silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\n\u001b[1;32m   5103\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/catboost/core.py:2319\u001b[0m, in \u001b[0;36mCatBoost._fit\u001b[0;34m(self, X, y, cat_features, text_features, embedding_features, pairs, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[1;32m   2315\u001b[0m allow_clear_pool \u001b[39m=\u001b[39m train_params[\u001b[39m\"\u001b[39m\u001b[39mallow_clear_pool\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   2317\u001b[0m \u001b[39mwith\u001b[39;00m log_fixup(log_cout, log_cerr), \\\n\u001b[1;32m   2318\u001b[0m     plot_wrapper(plot, plot_file, \u001b[39m'\u001b[39m\u001b[39mTraining plots\u001b[39m\u001b[39m'\u001b[39m, [_get_train_dir(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_params())]):\n\u001b[0;32m-> 2319\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train(\n\u001b[1;32m   2320\u001b[0m         train_pool,\n\u001b[1;32m   2321\u001b[0m         train_params[\u001b[39m\"\u001b[39;49m\u001b[39meval_sets\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m   2322\u001b[0m         params,\n\u001b[1;32m   2323\u001b[0m         allow_clear_pool,\n\u001b[1;32m   2324\u001b[0m         train_params[\u001b[39m\"\u001b[39;49m\u001b[39minit_model\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n\u001b[1;32m   2325\u001b[0m     )\n\u001b[1;32m   2327\u001b[0m \u001b[39m# Have property feature_importance possibly set\u001b[39;00m\n\u001b[1;32m   2328\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_object\u001b[39m.\u001b[39m_get_loss_function_name()\n",
      "File \u001b[0;32m~/Documents/ML/Kaggle/Kaggle-S4E1/.conda/lib/python3.11/site-packages/catboost/core.py:1723\u001b[0m, in \u001b[0;36m_CatBoostBase._train\u001b[0;34m(self, train_pool, test_pool, params, allow_clear_pool, init_model)\u001b[0m\n\u001b[1;32m   1722\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_train\u001b[39m(\u001b[39mself\u001b[39m, train_pool, test_pool, params, allow_clear_pool, init_model):\n\u001b[0;32m-> 1723\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_object\u001b[39m.\u001b[39;49m_train(train_pool, test_pool, params, allow_clear_pool, init_model\u001b[39m.\u001b[39;49m_object \u001b[39mif\u001b[39;49;00m init_model \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m   1724\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_trained_model_attributes()\n",
      "File \u001b[0;32m_catboost.pyx:4645\u001b[0m, in \u001b[0;36m_catboost._CatBoost._train\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_catboost.pyx:4694\u001b[0m, in \u001b[0;36m_catboost._CatBoost._train\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mCatBoostError\u001b[0m: /Users/zomb-ml-platform-msk/go-agent-21.2.0/pipelines/BuildMaster/catboost.git/catboost/private/libs/algo/tensor_search_helpers.cpp:99: This should be unreachable"
     ]
    }
   ],
   "source": [
    "catboost_pipeline.fit(train_df[cat_cols + num_cols+['CustomerId']], train_df['Exited'])\n",
    "test_probabilities = catboost_pipeline.predict_proba(test_df[cat_cols + num_cols])[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_pipeline.fit(train_df[cat_cols + num_cols+['CustomerId']], train_df['Exited'])\n",
    "test_probabilities = lgbm_pipeline.predict_proba(test_df[cat_cols + num_cols+['CustomerId']])[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 34921, number of negative: 130113\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001172 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1113\n",
      "[LightGBM] [Info] Number of data points in the train set: 165034, number of used features: 15\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211599 -> initscore=-1.315315\n",
      "[LightGBM] [Info] Start training from score -1.315315\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    }
   ],
   "source": [
    "ensemble_model.fit(train_df[cat_cols + num_cols+['CustomerId']], train_df['Exited'])\n",
    "test_probabilities = ensemble_model.predict_proba(test_df[cat_cols + num_cols+['CustomerId']])[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>165034</th>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165035</th>\n",
       "      <td>0.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165036</th>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165037</th>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165038</th>\n",
       "      <td>0.42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Exited\n",
       "id            \n",
       "165034    0.02\n",
       "165035    0.86\n",
       "165036    0.03\n",
       "165037    0.26\n",
       "165038    0.42"
      ]
     },
     "execution_count": 615,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub = pd.DataFrame()\n",
    "sub['id'] = test_df['id']\n",
    "sub['Exited'] = test_probabilities\n",
    "sub = sub.set_index('id')\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv('submission.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
